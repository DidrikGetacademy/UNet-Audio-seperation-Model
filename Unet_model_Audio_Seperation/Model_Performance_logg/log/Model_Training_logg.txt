
###TRAINING#####
2025-01-18 05:38:34,590 - INFO - [Train] Model path C:\Users\didri\Desktop\UNet Models\UNet_vocal_isolation_model\Model_weights\Pre_trained\best_model_epoch-19.pth does not exist. Starting from scratch.
2025-01-18 05:38:34,590 - INFO - [Memory Clearing memory before training] Timestamp: 2025-01-18 05:38:34
2025-01-18 05:38:34,590 - INFO - [Memory Clearing memory before training] GPU 0: NVIDIA GeForce RTX 3060, Total=12.88 GB, Allocated=0.11 GB, Cached=0.12 GB
2025-01-18 05:38:34,590 - INFO - [Memory Clearing memory before training] CPU Memory Usage: ~0.66 GB
2025-01-18 05:38:34,653 - INFO - [Memory Memory after clearing it...] Timestamp: 2025-01-18 05:38:34
2025-01-18 05:38:34,653 - INFO - [Memory Memory after clearing it...] GPU 0: NVIDIA GeForce RTX 3060, Total=12.88 GB, Allocated=0.11 GB, Cached=0.12 GB
2025-01-18 05:38:34,653 - INFO - [Memory Memory after clearing it...] CPU Memory Usage: ~0.66 GB
2025-01-18 05:38:34,653 - INFO - Training dataset: 25 batches
2025-01-18 05:38:34,653 - INFO - Validation dataset: 16 batches
2025-01-18 05:39:51,754 - INFO - Training started with configuration: Batch size=6, Effective batch size: 126 Learning rate=1e-07, Epochs=40
2025-01-18 05:39:51,754 - INFO - [Train] Epoch 1/40 started.
2025-01-18 05:40:38,531 - INFO - [Batch] Epoch: 1/40, Batch: 1/25, Combined Loss: 0.039779 (Summen av L1 og STFT), L1 Loss: 0.266706 (Amplitudetap), STFT Loss: 1.079074 (Spektraltap)
2025-01-18 05:41:37,135 - INFO - [Batch] Epoch: 1/40, Batch: 2/25, Combined Loss: 0.223581 (Summen av L1 og STFT), L1 Loss: 2.894635 (Amplitudetap), STFT Loss: 5.466876 (Spektraltap)
2025-01-18 05:42:27,943 - INFO - [Batch] Epoch: 1/40, Batch: 3/25, Combined Loss: 0.026051 (Summen av L1 og STFT), L1 Loss: 0.352311 (Amplitudetap), STFT Loss: 0.630541 (Spektraltap)
2025-01-18 05:43:07,203 - INFO - [Batch] Epoch: 1/40, Batch: 4/25, Combined Loss: 0.018727 (Summen av L1 og STFT), L1 Loss: 0.112806 (Amplitudetap), STFT Loss: 0.513467 (Spektraltap)
2025-01-18 05:43:39,897 - INFO - [Batch] Epoch: 1/40, Batch: 5/25, Combined Loss: 0.064968 (Summen av L1 og STFT), L1 Loss: 0.705723 (Amplitudetap), STFT Loss: 1.646585 (Spektraltap)
2025-01-18 05:44:29,491 - INFO - [Batch] Epoch: 1/40, Batch: 6/25, Combined Loss: 0.108461 (Summen av L1 og STFT), L1 Loss: 1.317651 (Amplitudetap), STFT Loss: 2.689108 (Spektraltap)
2025-01-18 05:45:05,746 - INFO - [Batch] Epoch: 1/40, Batch: 7/25, Combined Loss: 0.058858 (Summen av L1 og STFT), L1 Loss: 0.746612 (Amplitudetap), STFT Loss: 1.445749 (Spektraltap)
2025-01-18 05:45:43,187 - INFO - [Batch] Epoch: 1/40, Batch: 8/25, Combined Loss: 0.038058 (Summen av L1 og STFT), L1 Loss: 0.298508 (Amplitudetap), STFT Loss: 1.013814 (Spektraltap)
2025-01-18 05:46:41,729 - INFO - [Batch] Epoch: 1/40, Batch: 9/25, Combined Loss: 0.093611 (Summen av L1 og STFT), L1 Loss: 1.109156 (Amplitudetap), STFT Loss: 2.332981 (Spektraltap)
2025-01-18 05:47:22,416 - INFO - [Batch] Epoch: 1/40, Batch: 10/25, Combined Loss: 0.106431 (Summen av L1 og STFT), L1 Loss: 1.063290 (Amplitudetap), STFT Loss: 2.737226 (Spektraltap)
2025-01-18 05:47:58,334 - INFO - [Batch] Epoch: 1/40, Batch: 11/25, Combined Loss: 0.135786 (Summen av L1 og STFT), L1 Loss: 1.664461 (Amplitudetap), STFT Loss: 3.360247 (Spektraltap)
2025-01-18 05:48:48,116 - INFO - [Batch] Epoch: 1/40, Batch: 12/25, Combined Loss: 0.128758 (Summen av L1 og STFT), L1 Loss: 1.825617 (Amplitudetap), STFT Loss: 3.080325 (Spektraltap)
2025-01-18 05:49:44,084 - INFO - [Batch] Epoch: 1/40, Batch: 13/25, Combined Loss: 0.061463 (Summen av L1 og STFT), L1 Loss: 0.701927 (Amplitudetap), STFT Loss: 1.543062 (Spektraltap)
2025-01-18 05:50:29,594 - INFO - [Batch] Epoch: 1/40, Batch: 14/25, Combined Loss: 0.054757 (Summen av L1 og STFT), L1 Loss: 0.605292 (Amplitudetap), STFT Loss: 1.383302 (Spektraltap)
2025-01-18 05:51:02,294 - INFO - [Batch] Epoch: 1/40, Batch: 15/25, Combined Loss: 0.120184 (Summen av L1 og STFT), L1 Loss: 1.444799 (Amplitudetap), STFT Loss: 2.986306 (Spektraltap)
2025-01-18 05:51:39,751 - INFO - [Batch] Epoch: 1/40, Batch: 16/25, Combined Loss: 0.112966 (Summen av L1 og STFT), L1 Loss: 1.247294 (Amplitudetap), STFT Loss: 2.854433 (Spektraltap)
2025-01-18 05:52:39,192 - INFO - [Batch] Epoch: 1/40, Batch: 17/25, Combined Loss: 0.041157 (Summen av L1 og STFT), L1 Loss: 0.344876 (Amplitudetap), STFT Loss: 1.086900 (Spektraltap)
2025-01-18 05:53:25,092 - INFO - [Batch] Epoch: 1/40, Batch: 18/25, Combined Loss: 0.186258 (Summen av L1 og STFT), L1 Loss: 2.255365 (Amplitudetap), STFT Loss: 4.621157 (Spektraltap)
2025-01-18 05:54:09,858 - INFO - [Batch] Epoch: 1/40, Batch: 19/25, Combined Loss: 0.082484 (Summen av L1 og STFT), L1 Loss: 0.917158 (Amplitudetap), STFT Loss: 2.081455 (Spektraltap)
2025-01-18 05:54:50,795 - INFO - [Batch] Epoch: 1/40, Batch: 20/25, Combined Loss: 0.046493 (Summen av L1 og STFT), L1 Loss: 0.374773 (Amplitudetap), STFT Loss: 1.234185 (Spektraltap)
2025-01-18 05:55:31,760 - INFO - [Batch] Epoch: 1/40, Batch: 21/25, Combined Loss: 0.091908 (Summen av L1 og STFT), L1 Loss: 0.949918 (Amplitudetap), STFT Loss: 2.350117 (Spektraltap)
2025-01-18 05:55:31,838 - INFO - Epoch [1/40], Batch [21/25], Gradient Norm: 0.0485
2025-01-18 05:56:12,557 - INFO - [Batch] Epoch: 1/40, Batch: 22/25, Combined Loss: 0.063757 (Summen av L1 og STFT), L1 Loss: 0.941206 (Amplitudetap), STFT Loss: 1.509337 (Spektraltap)
2025-01-18 05:56:56,471 - INFO - [Batch] Epoch: 1/40, Batch: 23/25, Combined Loss: 0.066219 (Summen av L1 og STFT), L1 Loss: 0.730998 (Amplitudetap), STFT Loss: 1.673296 (Spektraltap)
2025-01-18 05:57:39,095 - INFO - [Batch] Epoch: 1/40, Batch: 24/25, Combined Loss: 0.074107 (Summen av L1 og STFT), L1 Loss: 0.861933 (Amplitudetap), STFT Loss: 1.853810 (Spektraltap)
2025-01-18 05:58:45,694 - INFO - [Batch] Epoch: 1/40, Batch: 25/25, Combined Loss: 0.063453 (Summen av L1 og STFT), L1 Loss: 0.785390 (Amplitudetap), STFT Loss: 1.566995 (Spektraltap)
2025-01-18 05:58:45,710 - INFO - Epoch [1/40], Batch [25/25], Gradient Norm: 0.0276
2025-01-18 05:58:45,710 - INFO - Current Learning Rate 1e-07
2025-01-18 05:58:45,710 - INFO - [Epoch Improvement] Epoch 1: No comparison (first epoch).
2025-01-18 05:58:45,710 - INFO - [Epoch Summary] Avg Training Loss: 0.084331
2025-01-18 05:58:45,828 - INFO - [Train] New best model saved at C:\Users\didri\Desktop\UNet Models\UNet_vocal_isolation_model\Model_weights\CheckPoints\best_model_epoch-0.pth with loss 0.084331
2025-01-18 06:07:03,676 - INFO - [Validation] Epoch 1/40, [Validation] Avg Validation Loss:  1.078020
2025-01-18 06:07:03,677 - INFO - [Memory After Epoch 1] Timestamp: 2025-01-18 06:07:03
2025-01-18 06:07:03,677 - INFO - [Memory After Epoch 1] GPU 0: NVIDIA GeForce RTX 3060, Total=12.88 GB, Allocated=0.51 GB, Cached=23.26 GB
2025-01-18 06:07:03,677 - INFO - [Memory After Epoch 1] CPU Memory Usage: ~7.70 GB
2025-01-18 06:07:03,677 - INFO - [Epoch Summary] Epoch: 1/40, Avg Combined Loss: 0.084331, Avg L1 Loss: 0.980736, Avg STFT Loss: 2.109614
2025-01-18 06:07:03,677 - INFO - Previous epoch loss: 0.08433098211884499
2025-01-18 06:07:03,677 - INFO - [Train] Epoch 2/40 started.
2025-01-18 06:07:49,100 - INFO - [Batch] Epoch: 2/40, Batch: 1/25, Combined Loss: 0.070580 (Summen av L1 og STFT), L1 Loss: 0.829442 (Amplitudetap), STFT Loss: 1.761939 (Spektraltap)
2025-01-18 06:08:39,493 - INFO - [Batch] Epoch: 2/40, Batch: 2/25, Combined Loss: 0.059587 (Summen av L1 og STFT), L1 Loss: 0.516256 (Amplitudetap), STFT Loss: 1.566366 (Spektraltap)
2025-01-18 06:09:34,796 - INFO - [Batch] Epoch: 2/40, Batch: 3/25, Combined Loss: 0.035211 (Summen av L1 og STFT), L1 Loss: 0.209602 (Amplitudetap), STFT Loss: 0.966499 (Spektraltap)
2025-01-18 06:10:23,764 - INFO - [Batch] Epoch: 2/40, Batch: 4/25, Combined Loss: 0.116208 (Summen av L1 og STFT), L1 Loss: 1.294316 (Amplitudetap), STFT Loss: 2.931536 (Spektraltap)
2025-01-18 06:11:15,328 - INFO - [Batch] Epoch: 2/40, Batch: 5/25, Combined Loss: 0.072891 (Summen av L1 og STFT), L1 Loss: 0.784656 (Amplitudetap), STFT Loss: 1.850463 (Spektraltap)
2025-01-18 06:12:02,976 - INFO - [Batch] Epoch: 2/40, Batch: 6/25, Combined Loss: 0.181443 (Summen av L1 og STFT), L1 Loss: 2.182515 (Amplitudetap), STFT Loss: 4.507934 (Spektraltap)
2025-01-18 06:12:51,425 - INFO - [Batch] Epoch: 2/40, Batch: 7/25, Combined Loss: 0.081876 (Summen av L1 og STFT), L1 Loss: 0.872911 (Amplitudetap), STFT Loss: 2.082188 (Spektraltap)
2025-01-18 06:13:37,875 - INFO - [Batch] Epoch: 2/40, Batch: 8/25, Combined Loss: 0.072394 (Summen av L1 og STFT), L1 Loss: 0.819404 (Amplitudetap), STFT Loss: 1.820643 (Spektraltap)
2025-01-18 06:14:30,932 - INFO - [Batch] Epoch: 2/40, Batch: 9/25, Combined Loss: 0.080149 (Summen av L1 og STFT), L1 Loss: 1.182403 (Amplitudetap), STFT Loss: 1.897717 (Spektraltap)
2025-01-18 06:15:21,911 - INFO - [Batch] Epoch: 2/40, Batch: 10/25, Combined Loss: 0.096432 (Summen av L1 og STFT), L1 Loss: 1.066694 (Amplitudetap), STFT Loss: 2.435816 (Spektraltap)
2025-01-18 06:16:16,400 - INFO - [Batch] Epoch: 2/40, Batch: 11/25, Combined Loss: 0.033628 (Summen av L1 og STFT), L1 Loss: 0.311248 (Amplitudetap), STFT Loss: 0.875461 (Spektraltap)
2025-01-18 06:17:07,065 - INFO - [Batch] Epoch: 2/40, Batch: 12/25, Combined Loss: 0.111079 (Summen av L1 og STFT), L1 Loss: 1.380537 (Amplitudetap), STFT Loss: 2.740697 (Spektraltap)
2025-01-18 06:17:55,604 - INFO - [Batch] Epoch: 2/40, Batch: 13/25, Combined Loss: 0.029609 (Summen av L1 og STFT), L1 Loss: 0.150707 (Amplitudetap), STFT Loss: 0.823686 (Spektraltap)
2025-01-18 06:18:41,670 - INFO - [Batch] Epoch: 2/40, Batch: 14/25, Combined Loss: 0.045445 (Summen av L1 og STFT), L1 Loss: 0.436314 (Amplitudetap), STFT Loss: 1.176348 (Spektraltap)
2025-01-18 06:19:29,904 - INFO - [Batch] Epoch: 2/40, Batch: 15/25, Combined Loss: 0.078632 (Summen av L1 og STFT), L1 Loss: 0.880590 (Amplitudetap), STFT Loss: 1.981563 (Spektraltap)
2025-01-18 06:20:23,438 - INFO - [Batch] Epoch: 2/40, Batch: 16/25, Combined Loss: 0.071033 (Summen av L1 og STFT), L1 Loss: 0.971542 (Amplitudetap), STFT Loss: 1.714602 (Spektraltap)
2025-01-18 06:21:13,096 - INFO - [Batch] Epoch: 2/40, Batch: 17/25, Combined Loss: 0.065330 (Summen av L1 og STFT), L1 Loss: 0.814732 (Amplitudetap), STFT Loss: 1.610723 (Spektraltap)
2025-01-18 06:21:58,565 - INFO - [Batch] Epoch: 2/40, Batch: 18/25, Combined Loss: 0.029615 (Summen av L1 og STFT), L1 Loss: 0.207081 (Amplitudetap), STFT Loss: 0.799713 (Spektraltap)
2025-01-18 06:22:55,720 - INFO - [Batch] Epoch: 2/40, Batch: 19/25, Combined Loss: 0.086747 (Summen av L1 og STFT), L1 Loss: 1.196987 (Amplitudetap), STFT Loss: 2.089425 (Spektraltap)
2025-01-18 06:23:48,161 - INFO - [Batch] Epoch: 2/40, Batch: 20/25, Combined Loss: 0.074808 (Summen av L1 og STFT), L1 Loss: 0.726912 (Amplitudetap), STFT Loss: 1.932693 (Spektraltap)
2025-01-18 06:24:46,747 - INFO - [Batch] Epoch: 2/40, Batch: 21/25, Combined Loss: 0.183095 (Summen av L1 og STFT), L1 Loss: 2.242207 (Amplitudetap), STFT Loss: 4.531906 (Spektraltap)
2025-01-18 06:24:46,747 - INFO - Epoch [2/40], Batch [21/25], Gradient Norm: 0.0922
2025-01-18 06:25:36,800 - INFO - [Batch] Epoch: 2/40, Batch: 22/25, Combined Loss: 0.192961 (Summen av L1 og STFT), L1 Loss: 2.502884 (Amplitudetap), STFT Loss: 4.716166 (Spektraltap)
2025-01-18 06:26:31,485 - INFO - [Batch] Epoch: 2/40, Batch: 23/25, Combined Loss: 0.074078 (Summen av L1 og STFT), L1 Loss: 0.811354 (Amplitudetap), STFT Loss: 1.874623 (Spektraltap)
2025-01-18 06:27:31,842 - INFO - [Batch] Epoch: 2/40, Batch: 24/25, Combined Loss: 0.102510 (Summen av L1 og STFT), L1 Loss: 1.273008 (Amplitudetap), STFT Loss: 2.529733 (Spektraltap)
2025-01-18 06:28:21,502 - INFO - [Batch] Epoch: 2/40, Batch: 25/25, Combined Loss: 0.063279 (Summen av L1 og STFT), L1 Loss: 0.858387 (Amplitudetap), STFT Loss: 1.530480 (Spektraltap)
2025-01-18 06:28:21,518 - INFO - Epoch [2/40], Batch [25/25], Gradient Norm: 0.0746
2025-01-18 06:28:21,549 - INFO - Current Learning Rate 1e-07
2025-01-18 06:28:21,549 - INFO - [Epoch Improvement] Epoch 2: Loss improved by 0.00% from previous epoch.
2025-01-18 06:28:21,549 - INFO - [Epoch Summary] Avg Training Loss: 0.084345
2025-01-18 06:28:21,549 - INFO - [Train] No improvement in loss for epoch 2 with loss: 0.08434486329555511. Best loss remains 0.084331. Trigger_times: 0
2025-01-18 06:36:45,262 - INFO - [Validation] Epoch 2/40, [Validation] Avg Validation Loss:  1.079102
2025-01-18 06:36:45,262 - INFO - [Epoch Summary] Epoch: 2/40, Avg Combined Loss: 0.084345, Avg L1 Loss: 0.980822, Avg STFT Loss: 2.109785
2025-01-18 06:36:45,262 - INFO - Previous epoch loss: 0.08434486329555511
2025-01-18 06:36:45,262 - INFO - [Train] Epoch 3/40 started.
2025-01-18 06:37:37,416 - INFO - [Batch] Epoch: 3/40, Batch: 1/25, Combined Loss: 0.125644 (Summen av L1 og STFT), L1 Loss: 1.468040 (Amplitudetap), STFT Loss: 3.140150 (Spektraltap)
2025-01-18 06:38:20,366 - INFO - [Batch] Epoch: 3/40, Batch: 2/25, Combined Loss: 0.105917 (Summen av L1 og STFT), L1 Loss: 1.326986 (Amplitudetap), STFT Loss: 2.608791 (Spektraltap)
2025-01-18 06:39:19,395 - INFO - [Batch] Epoch: 3/40, Batch: 3/25, Combined Loss: 0.101109 (Summen av L1 og STFT), L1 Loss: 1.152295 (Amplitudetap), STFT Loss: 2.539433 (Spektraltap)
2025-01-18 06:40:14,098 - INFO - [Batch] Epoch: 3/40, Batch: 4/25, Combined Loss: 0.074846 (Summen av L1 og STFT), L1 Loss: 0.887019 (Amplitudetap), STFT Loss: 1.865219 (Spektraltap)
2025-01-18 06:40:58,649 - INFO - [Batch] Epoch: 3/40, Batch: 5/25, Combined Loss: 0.088554 (Summen av L1 og STFT), L1 Loss: 0.954629 (Amplitudetap), STFT Loss: 2.247489 (Spektraltap)
2025-01-18 06:41:59,532 - INFO - [Batch] Epoch: 3/40, Batch: 6/25, Combined Loss: 0.062131 (Summen av L1 og STFT), L1 Loss: 0.812212 (Amplitudetap), STFT Loss: 1.515837 (Spektraltap)
2025-01-18 06:42:53,116 - INFO - [Batch] Epoch: 3/40, Batch: 7/25, Combined Loss: 0.056513 (Summen av L1 og STFT), L1 Loss: 0.686702 (Amplitudetap), STFT Loss: 1.401088 (Spektraltap)
2025-01-18 06:43:27,623 - INFO - [Batch] Epoch: 3/40, Batch: 8/25, Combined Loss: 0.030080 (Summen av L1 og STFT), L1 Loss: 0.277997 (Amplitudetap), STFT Loss: 0.783256 (Spektraltap)
2025-01-18 06:44:12,207 - INFO - [Batch] Epoch: 3/40, Batch: 9/25, Combined Loss: 0.069609 (Summen av L1 og STFT), L1 Loss: 0.728517 (Amplitudetap), STFT Loss: 1.776050 (Spektraltap)
2025-01-18 06:44:54,777 - INFO - [Batch] Epoch: 3/40, Batch: 10/25, Combined Loss: 0.152243 (Summen av L1 og STFT), L1 Loss: 2.097996 (Amplitudetap), STFT Loss: 3.668156 (Spektraltap)
2025-01-18 06:45:53,269 - INFO - [Batch] Epoch: 3/40, Batch: 11/25, Combined Loss: 0.076642 (Summen av L1 og STFT), L1 Loss: 0.829059 (Amplitudetap), STFT Loss: 1.943938 (Spektraltap)
2025-01-18 06:46:44,814 - INFO - [Batch] Epoch: 3/40, Batch: 12/25, Combined Loss: 0.044838 (Summen av L1 og STFT), L1 Loss: 0.349514 (Amplitudetap), STFT Loss: 1.195354 (Spektraltap)
2025-01-18 06:47:33,986 - INFO - [Batch] Epoch: 3/40, Batch: 13/25, Combined Loss: 0.140015 (Summen av L1 og STFT), L1 Loss: 1.679598 (Amplitudetap), STFT Loss: 3.480622 (Spektraltap)
2025-01-18 06:48:23,981 - INFO - [Batch] Epoch: 3/40, Batch: 14/25, Combined Loss: 0.066758 (Summen av L1 og STFT), L1 Loss: 0.622051 (Amplitudetap), STFT Loss: 1.736132 (Spektraltap)
2025-01-18 06:49:16,580 - INFO - [Batch] Epoch: 3/40, Batch: 15/25, Combined Loss: 0.078980 (Summen av L1 og STFT), L1 Loss: 0.886302 (Amplitudetap), STFT Loss: 1.989556 (Spektraltap)
2025-01-18 06:49:51,556 - INFO - [Batch] Epoch: 3/40, Batch: 16/25, Combined Loss: 0.076978 (Summen av L1 og STFT), L1 Loss: 0.790484 (Amplitudetap), STFT Loss: 1.970569 (Spektraltap)
2025-01-18 06:50:38,419 - INFO - [Batch] Epoch: 3/40, Batch: 17/25, Combined Loss: 0.090474 (Summen av L1 og STFT), L1 Loss: 0.944212 (Amplitudetap), STFT Loss: 2.309558 (Spektraltap)
2025-01-18 06:51:32,630 - INFO - [Batch] Epoch: 3/40, Batch: 18/25, Combined Loss: 0.111699 (Summen av L1 og STFT), L1 Loss: 1.229308 (Amplitudetap), STFT Loss: 2.824112 (Spektraltap)
2025-01-18 06:52:24,374 - INFO - [Batch] Epoch: 3/40, Batch: 19/25, Combined Loss: 0.117497 (Summen av L1 og STFT), L1 Loss: 1.637159 (Amplitudetap), STFT Loss: 2.823281 (Spektraltap)
2025-01-18 06:53:14,000 - INFO - [Batch] Epoch: 3/40, Batch: 20/25, Combined Loss: 0.156529 (Summen av L1 og STFT), L1 Loss: 1.993026 (Amplitudetap), STFT Loss: 3.841715 (Spektraltap)
2025-01-18 06:54:04,820 - INFO - [Batch] Epoch: 3/40, Batch: 21/25, Combined Loss: 0.039968 (Summen av L1 og STFT), L1 Loss: 0.262879 (Amplitudetap), STFT Loss: 1.086388 (Spektraltap)
2025-01-18 06:54:04,836 - INFO - Epoch [3/40], Batch [21/25], Gradient Norm: 0.0306
2025-01-18 06:55:01,798 - INFO - [Batch] Epoch: 3/40, Batch: 22/25, Combined Loss: 0.015302 (Summen av L1 og STFT), L1 Loss: 0.334396 (Amplitudetap), STFT Loss: 0.315746 (Spektraltap)
2025-01-18 06:55:56,427 - INFO - [Batch] Epoch: 3/40, Batch: 23/25, Combined Loss: 0.061741 (Summen av L1 og STFT), L1 Loss: 0.646117 (Amplitudetap), STFT Loss: 1.575319 (Spektraltap)
2025-01-18 06:56:44,920 - INFO - [Batch] Epoch: 3/40, Batch: 24/25, Combined Loss: 0.123565 (Summen av L1 og STFT), L1 Loss: 1.490098 (Amplitudetap), STFT Loss: 3.068335 (Spektraltap)
2025-01-18 06:57:46,070 - INFO - [Batch] Epoch: 3/40, Batch: 25/25, Combined Loss: 0.040555 (Summen av L1 og STFT), L1 Loss: 0.433051 (Amplitudetap), STFT Loss: 1.031055 (Spektraltap)
2025-01-18 06:57:46,085 - INFO - Epoch [3/40], Batch [25/25], Gradient Norm: 0.0440
2025-01-18 06:57:46,085 - INFO - Current Learning Rate 1e-07
2025-01-18 06:57:46,085 - INFO - [Epoch Improvement] Epoch 3: Loss improved by 0.00% from previous epoch.
2025-01-18 06:57:46,085 - INFO - [Epoch Summary] Avg Training Loss: 0.084327
2025-01-18 06:57:46,244 - INFO - [Train] New best model saved at C:\Users\didri\Desktop\UNet Models\UNet_vocal_isolation_model\Model_weights\CheckPoints\best_model_epoch-2.pth with loss 0.084327
2025-01-18 07:06:40,722 - INFO - [Validation] Epoch 3/40, [Validation] Avg Validation Loss:  1.079201
2025-01-18 07:06:40,722 - INFO - [Epoch Summary] Epoch: 3/40, Avg Combined Loss: 0.084327, Avg L1 Loss: 0.980810, Avg STFT Loss: 2.109686
2025-01-18 07:06:40,722 - INFO - Previous epoch loss: 0.08432742699980736
2025-01-18 07:06:40,722 - INFO - [Train] Epoch 4/40 started.
2025-01-18 07:07:33,886 - INFO - [Batch] Epoch: 4/40, Batch: 1/25, Combined Loss: 0.030377 (Summen av L1 og STFT), L1 Loss: 0.261538 (Amplitudetap), STFT Loss: 0.799213 (Spektraltap)
2025-01-18 07:08:12,589 - INFO - [Batch] Epoch: 4/40, Batch: 2/25, Combined Loss: 0.100214 (Summen av L1 og STFT), L1 Loss: 1.131782 (Amplitudetap), STFT Loss: 2.521367 (Spektraltap)
2025-01-18 07:09:06,144 - INFO - [Batch] Epoch: 4/40, Batch: 3/25, Combined Loss: 0.110500 (Summen av L1 og STFT), L1 Loss: 1.405451 (Amplitudetap), STFT Loss: 2.712658 (Spektraltap)
2025-01-18 07:10:00,266 - INFO - [Batch] Epoch: 4/40, Batch: 4/25, Combined Loss: 0.084104 (Summen av L1 og STFT), L1 Loss: 0.942394 (Amplitudetap), STFT Loss: 2.119243 (Spektraltap)
2025-01-18 07:10:59,810 - INFO - [Batch] Epoch: 4/40, Batch: 5/25, Combined Loss: 0.115313 (Summen av L1 og STFT), L1 Loss: 1.284837 (Amplitudetap), STFT Loss: 2.908738 (Spektraltap)
2025-01-18 07:11:54,696 - INFO - [Batch] Epoch: 4/40, Batch: 6/25, Combined Loss: 0.034692 (Summen av L1 og STFT), L1 Loss: 0.308009 (Amplitudetap), STFT Loss: 0.908767 (Spektraltap)
2025-01-18 07:12:53,484 - INFO - [Batch] Epoch: 4/40, Batch: 7/25, Combined Loss: 0.079566 (Summen av L1 og STFT), L1 Loss: 0.826605 (Amplitudetap), STFT Loss: 2.032730 (Spektraltap)
2025-01-18 07:13:43,298 - INFO - [Batch] Epoch: 4/40, Batch: 8/25, Combined Loss: 0.104491 (Summen av L1 og STFT), L1 Loss: 1.203458 (Amplitudetap), STFT Loss: 2.618976 (Spektraltap)
2025-01-18 07:14:36,760 - INFO - [Batch] Epoch: 4/40, Batch: 9/25, Combined Loss: 0.160312 (Summen av L1 og STFT), L1 Loss: 2.050505 (Amplitudetap), STFT Loss: 3.930567 (Spektraltap)
2025-01-18 07:15:22,206 - INFO - [Batch] Epoch: 4/40, Batch: 10/25, Combined Loss: 0.014323 (Summen av L1 og STFT), L1 Loss: 0.099463 (Amplitudetap), STFT Loss: 0.387061 (Spektraltap)
2025-01-18 07:16:06,110 - INFO - [Batch] Epoch: 4/40, Batch: 11/25, Combined Loss: 0.030685 (Summen av L1 og STFT), L1 Loss: 0.327668 (Amplitudetap), STFT Loss: 0.780134 (Spektraltap)
2025-01-18 07:16:52,466 - INFO - [Batch] Epoch: 4/40, Batch: 12/25, Combined Loss: 0.063407 (Summen av L1 og STFT), L1 Loss: 0.696873 (Amplitudetap), STFT Loss: 1.603556 (Spektraltap)
2025-01-18 07:17:41,704 - INFO - [Batch] Epoch: 4/40, Batch: 13/25, Combined Loss: 0.083531 (Summen av L1 og STFT), L1 Loss: 0.920216 (Amplitudetap), STFT Loss: 2.111543 (Spektraltap)
2025-01-18 07:18:37,338 - INFO - [Batch] Epoch: 4/40, Batch: 14/25, Combined Loss: 0.074209 (Summen av L1 og STFT), L1 Loss: 0.968201 (Amplitudetap), STFT Loss: 1.811337 (Spektraltap)
2025-01-18 07:19:37,386 - INFO - [Batch] Epoch: 4/40, Batch: 15/25, Combined Loss: 0.057432 (Summen av L1 og STFT), L1 Loss: 0.737843 (Amplitudetap), STFT Loss: 1.406752 (Spektraltap)
2025-01-18 07:20:20,768 - INFO - [Batch] Epoch: 4/40, Batch: 16/25, Combined Loss: 0.069449 (Summen av L1 og STFT), L1 Loss: 0.696137 (Amplitudetap), STFT Loss: 1.785111 (Spektraltap)
2025-01-18 07:21:01,005 - INFO - [Batch] Epoch: 4/40, Batch: 17/25, Combined Loss: 0.072076 (Summen av L1 og STFT), L1 Loss: 1.045628 (Amplitudetap), STFT Loss: 1.714166 (Spektraltap)
2025-01-18 07:21:43,466 - INFO - [Batch] Epoch: 4/40, Batch: 18/25, Combined Loss: 0.098399 (Summen av L1 og STFT), L1 Loss: 1.334524 (Amplitudetap), STFT Loss: 2.380034 (Spektraltap)
2025-01-18 07:22:30,204 - INFO - [Batch] Epoch: 4/40, Batch: 19/25, Combined Loss: 0.042264 (Summen av L1 og STFT), L1 Loss: 0.388317 (Amplitudetap), STFT Loss: 1.101510 (Spektraltap)
2025-01-18 07:23:28,537 - INFO - [Batch] Epoch: 4/40, Batch: 20/25, Combined Loss: 0.199100 (Summen av L1 og STFT), L1 Loss: 2.279478 (Amplitudetap), STFT Loss: 4.996090 (Spektraltap)
2025-01-18 07:24:16,200 - INFO - [Batch] Epoch: 4/40, Batch: 21/25, Combined Loss: 0.028112 (Summen av L1 og STFT), L1 Loss: 0.203008 (Amplitudetap), STFT Loss: 0.756358 (Spektraltap)
2025-01-18 07:24:16,200 - INFO - Epoch [4/40], Batch [21/25], Gradient Norm: 0.0267
2025-01-18 07:25:15,550 - INFO - [Batch] Epoch: 4/40, Batch: 22/25, Combined Loss: 0.074514 (Summen av L1 og STFT), L1 Loss: 0.835800 (Amplitudetap), STFT Loss: 1.877234 (Spektraltap)
2025-01-18 07:26:03,493 - INFO - [Batch] Epoch: 4/40, Batch: 23/25, Combined Loss: 0.147954 (Summen av L1 og STFT), L1 Loss: 1.744706 (Amplitudetap), STFT Loss: 3.690885 (Spektraltap)
2025-01-18 07:26:46,508 - INFO - [Batch] Epoch: 4/40, Batch: 24/25, Combined Loss: 0.133429 (Summen av L1 og STFT), L1 Loss: 1.630375 (Amplitudetap), STFT Loss: 3.304126 (Spektraltap)
2025-01-18 07:27:38,065 - INFO - [Batch] Epoch: 4/40, Batch: 25/25, Combined Loss: 0.099238 (Summen av L1 og STFT), L1 Loss: 1.199410 (Amplitudetap), STFT Loss: 2.463094 (Spektraltap)
2025-01-18 07:27:38,080 - INFO - Epoch [4/40], Batch [25/25], Gradient Norm: 0.0455
2025-01-18 07:27:38,096 - INFO - Current Learning Rate 1e-07
2025-01-18 07:27:38,096 - INFO - [Epoch Improvement] Epoch 4: Loss improved by 0.00% from previous epoch.
2025-01-18 07:27:38,096 - INFO - [Epoch Summary] Avg Training Loss: 0.084308
2025-01-18 07:27:38,217 - INFO - [Train] New best model saved at C:\Users\didri\Desktop\UNet Models\UNet_vocal_isolation_model\Model_weights\CheckPoints\best_model_epoch-3.pth with loss 0.084308
2025-01-18 07:36:18,449 - INFO - [Validation] Epoch 4/40, [Validation] Avg Validation Loss:  1.079131
2025-01-18 07:36:18,449 - INFO - [Epoch Summary] Epoch: 4/40, Avg Combined Loss: 0.084308, Avg L1 Loss: 0.980830, Avg STFT Loss: 2.109477
2025-01-18 07:36:18,449 - INFO - Previous epoch loss: 0.08430770047008991
2025-01-18 07:36:18,449 - INFO - [Train] Epoch 5/40 started.
2025-01-18 07:37:01,469 - INFO - [Batch] Epoch: 5/40, Batch: 1/25, Combined Loss: 0.189904 (Summen av L1 og STFT), L1 Loss: 2.502988 (Amplitudetap), STFT Loss: 4.624419 (Spektraltap)
2025-01-18 07:37:51,194 - INFO - [Batch] Epoch: 5/40, Batch: 2/25, Combined Loss: 0.139279 (Summen av L1 og STFT), L1 Loss: 1.735504 (Amplitudetap), STFT Loss: 3.434595 (Spektraltap)
2025-01-18 07:38:35,139 - INFO - [Batch] Epoch: 5/40, Batch: 3/25, Combined Loss: 0.164703 (Summen av L1 og STFT), L1 Loss: 1.931057 (Amplitudetap), STFT Loss: 4.113485 (Spektraltap)
2025-01-18 07:39:26,568 - INFO - [Batch] Epoch: 5/40, Batch: 4/25, Combined Loss: 0.036832 (Summen av L1 og STFT), L1 Loss: 0.388644 (Amplitudetap), STFT Loss: 0.938409 (Spektraltap)
2025-01-18 07:40:24,403 - INFO - [Batch] Epoch: 5/40, Batch: 5/25, Combined Loss: 0.072988 (Summen av L1 og STFT), L1 Loss: 0.805134 (Amplitudetap), STFT Loss: 1.844568 (Spektraltap)
2025-01-18 07:41:20,814 - INFO - [Batch] Epoch: 5/40, Batch: 6/25, Combined Loss: 0.082677 (Summen av L1 og STFT), L1 Loss: 0.911169 (Amplitudetap), STFT Loss: 2.089805 (Spektraltap)
2025-01-18 07:42:02,519 - INFO - [Batch] Epoch: 5/40, Batch: 7/25, Combined Loss: 0.037143 (Summen av L1 og STFT), L1 Loss: 0.324734 (Amplitudetap), STFT Loss: 0.975110 (Spektraltap)
2025-01-18 07:42:58,973 - INFO - [Batch] Epoch: 5/40, Batch: 8/25, Combined Loss: 0.019921 (Summen av L1 og STFT), L1 Loss: 0.219157 (Amplitudetap), STFT Loss: 0.503695 (Spektraltap)
2025-01-18 07:43:47,837 - INFO - [Batch] Epoch: 5/40, Batch: 9/25, Combined Loss: 0.034502 (Summen av L1 og STFT), L1 Loss: 0.371598 (Amplitudetap), STFT Loss: 0.875810 (Spektraltap)
2025-01-18 07:44:43,795 - INFO - [Batch] Epoch: 5/40, Batch: 10/25, Combined Loss: 0.091686 (Summen av L1 og STFT), L1 Loss: 1.069062 (Amplitudetap), STFT Loss: 2.292399 (Spektraltap)
2025-01-18 07:45:41,210 - INFO - [Batch] Epoch: 5/40, Batch: 11/25, Combined Loss: 0.093260 (Summen av L1 og STFT), L1 Loss: 1.180746 (Amplitudetap), STFT Loss: 2.291756 (Spektraltap)
2025-01-18 07:46:33,371 - INFO - [Batch] Epoch: 5/40, Batch: 12/25, Combined Loss: 0.061368 (Summen av L1 og STFT), L1 Loss: 0.645350 (Amplitudetap), STFT Loss: 1.564474 (Spektraltap)
2025-01-18 07:47:18,211 - INFO - [Batch] Epoch: 5/40, Batch: 13/25, Combined Loss: 0.068028 (Summen av L1 og STFT), L1 Loss: 0.696045 (Amplitudetap), STFT Loss: 1.742547 (Spektraltap)
2025-01-18 07:48:13,559 - INFO - [Batch] Epoch: 5/40, Batch: 14/25, Combined Loss: 0.131358 (Summen av L1 og STFT), L1 Loss: 1.611545 (Amplitudetap), STFT Loss: 3.250069 (Spektraltap)
2025-01-18 07:49:04,782 - INFO - [Batch] Epoch: 5/40, Batch: 15/25, Combined Loss: 0.053822 (Summen av L1 og STFT), L1 Loss: 0.600499 (Amplitudetap), STFT Loss: 1.357302 (Spektraltap)
2025-01-18 07:49:43,388 - INFO - [Batch] Epoch: 5/40, Batch: 16/25, Combined Loss: 0.047073 (Summen av L1 og STFT), L1 Loss: 0.590178 (Amplitudetap), STFT Loss: 1.159260 (Spektraltap)
2025-01-18 07:50:36,847 - INFO - [Batch] Epoch: 5/40, Batch: 17/25, Combined Loss: 0.162114 (Summen av L1 og STFT), L1 Loss: 1.905668 (Amplitudetap), STFT Loss: 4.046699 (Spektraltap)
2025-01-18 07:51:25,037 - INFO - [Batch] Epoch: 5/40, Batch: 18/25, Combined Loss: 0.104814 (Summen av L1 og STFT), L1 Loss: 1.161025 (Amplitudetap), STFT Loss: 2.646833 (Spektraltap)
2025-01-18 07:52:10,737 - INFO - [Batch] Epoch: 5/40, Batch: 19/25, Combined Loss: 0.068382 (Summen av L1 og STFT), L1 Loss: 0.809255 (Amplitudetap), STFT Loss: 1.704642 (Spektraltap)
2025-01-18 07:52:58,838 - INFO - [Batch] Epoch: 5/40, Batch: 20/25, Combined Loss: 0.127667 (Summen av L1 og STFT), L1 Loss: 1.744818 (Amplitudetap), STFT Loss: 3.082240 (Spektraltap)
2025-01-18 07:53:41,966 - INFO - [Batch] Epoch: 5/40, Batch: 21/25, Combined Loss: 0.088761 (Summen av L1 og STFT), L1 Loss: 0.884671 (Amplitudetap), STFT Loss: 2.283691 (Spektraltap)
2025-01-18 07:53:41,966 - INFO - Epoch [5/40], Batch [21/25], Gradient Norm: 0.0433
2025-01-18 07:54:40,195 - INFO - [Batch] Epoch: 5/40, Batch: 22/25, Combined Loss: 0.034385 (Summen av L1 og STFT), L1 Loss: 0.270553 (Amplitudetap), STFT Loss: 0.915594 (Spektraltap)
2025-01-18 07:55:33,678 - INFO - [Batch] Epoch: 5/40, Batch: 23/25, Combined Loss: 0.076284 (Summen av L1 og STFT), L1 Loss: 0.854436 (Amplitudetap), STFT Loss: 1.922323 (Spektraltap)
2025-01-18 07:56:20,796 - INFO - [Batch] Epoch: 5/40, Batch: 24/25, Combined Loss: 0.031360 (Summen av L1 og STFT), L1 Loss: 0.300556 (Amplitudetap), STFT Loss: 0.811993 (Spektraltap)
2025-01-18 07:57:02,011 - INFO - [Batch] Epoch: 5/40, Batch: 25/25, Combined Loss: 0.089415 (Summen av L1 og STFT), L1 Loss: 0.995265 (Amplitudetap), STFT Loss: 2.255920 (Spektraltap)
2025-01-18 07:57:02,011 - INFO - Epoch [5/40], Batch [25/25], Gradient Norm: 0.0446
2025-01-18 07:57:02,026 - INFO - Current Learning Rate 1e-07
2025-01-18 07:57:02,026 - INFO - [Epoch Improvement] Epoch 5: Loss improved by 0.00% from previous epoch.
2025-01-18 07:57:02,026 - INFO - [Epoch Summary] Avg Training Loss: 0.084309
2025-01-18 07:57:02,026 - INFO - [Train] No improvement in loss for epoch 5 with loss: 0.0843090358376503. Best loss remains 0.084308. Trigger_times: 0
2025-01-18 08:05:39,503 - INFO - [Validation] Epoch 5/40, [Validation] Avg Validation Loss:  1.079106
2025-01-18 08:05:39,503 - INFO - [Memory After Epoch 5] Timestamp: 2025-01-18 08:05:39
2025-01-18 08:05:39,503 - INFO - [Memory After Epoch 5] GPU 0: NVIDIA GeForce RTX 3060, Total=12.88 GB, Allocated=0.51 GB, Cached=23.26 GB
2025-01-18 08:05:39,503 - INFO - [Memory After Epoch 5] CPU Memory Usage: ~0.55 GB
2025-01-18 08:05:39,503 - INFO - [Epoch Summary] Epoch: 5/40, Avg Combined Loss: 0.084309, Avg L1 Loss: 0.980741, Avg STFT Loss: 2.109402
2025-01-18 08:05:39,503 - INFO - Previous epoch loss: 0.0843090358376503
2025-01-18 08:05:39,503 - INFO - [Train] Epoch 6/40 started.
2025-01-18 08:06:27,145 - INFO - [Batch] Epoch: 6/40, Batch: 1/25, Combined Loss: 0.031882 (Summen av L1 og STFT), L1 Loss: 0.232823 (Amplitudetap), STFT Loss: 0.856666 (Spektraltap)
2025-01-18 08:07:26,039 - INFO - [Batch] Epoch: 6/40, Batch: 2/25, Combined Loss: 0.081511 (Summen av L1 og STFT), L1 Loss: 0.857271 (Amplitudetap), STFT Loss: 2.077936 (Spektraltap)
2025-01-18 08:08:10,528 - INFO - [Batch] Epoch: 6/40, Batch: 3/25, Combined Loss: 0.036701 (Summen av L1 og STFT), L1 Loss: 0.269843 (Amplitudetap), STFT Loss: 0.985386 (Spektraltap)
2025-01-18 08:09:06,857 - INFO - [Batch] Epoch: 6/40, Batch: 4/25, Combined Loss: 0.121397 (Summen av L1 og STFT), L1 Loss: 1.527021 (Amplitudetap), STFT Loss: 2.987484 (Spektraltap)
2025-01-18 08:10:02,020 - INFO - [Batch] Epoch: 6/40, Batch: 5/25, Combined Loss: 0.126108 (Summen av L1 og STFT), L1 Loss: 1.856213 (Amplitudetap), STFT Loss: 2.987735 (Spektraltap)
2025-01-18 08:11:00,558 - INFO - [Batch] Epoch: 6/40, Batch: 6/25, Combined Loss: 0.144445 (Summen av L1 og STFT), L1 Loss: 1.608332 (Amplitudetap), STFT Loss: 3.644079 (Spektraltap)
2025-01-18 08:11:39,352 - INFO - [Batch] Epoch: 6/40, Batch: 7/25, Combined Loss: 0.056168 (Summen av L1 og STFT), L1 Loss: 0.624041 (Amplitudetap), STFT Loss: 1.417582 (Spektraltap)
2025-01-18 08:12:19,978 - INFO - [Batch] Epoch: 6/40, Batch: 8/25, Combined Loss: 0.096093 (Summen av L1 og STFT), L1 Loss: 1.188472 (Amplitudetap), STFT Loss: 2.373435 (Spektraltap)
2025-01-18 08:13:21,286 - INFO - [Batch] Epoch: 6/40, Batch: 9/25, Combined Loss: 0.085227 (Summen av L1 og STFT), L1 Loss: 0.957620 (Amplitudetap), STFT Loss: 2.146410 (Spektraltap)
2025-01-18 08:13:55,170 - INFO - [Batch] Epoch: 6/40, Batch: 10/25, Combined Loss: 0.156385 (Summen av L1 og STFT), L1 Loss: 2.051001 (Amplitudetap), STFT Loss: 3.812546 (Spektraltap)
2025-01-18 08:14:43,271 - INFO - [Batch] Epoch: 6/40, Batch: 11/25, Combined Loss: 0.118227 (Summen av L1 og STFT), L1 Loss: 1.437672 (Amplitudetap), STFT Loss: 2.930667 (Spektraltap)
2025-01-18 08:15:26,010 - INFO - [Batch] Epoch: 6/40, Batch: 12/25, Combined Loss: 0.030730 (Summen av L1 og STFT), L1 Loss: 0.242656 (Amplitudetap), STFT Loss: 0.817895 (Spektraltap)
2025-01-18 08:16:18,847 - INFO - [Batch] Epoch: 6/40, Batch: 13/25, Combined Loss: 0.084263 (Summen av L1 og STFT), L1 Loss: 0.879333 (Amplitudetap), STFT Loss: 2.151032 (Spektraltap)
2025-01-18 08:17:12,454 - INFO - [Batch] Epoch: 6/40, Batch: 14/25, Combined Loss: 0.069193 (Summen av L1 og STFT), L1 Loss: 0.791517 (Amplitudetap), STFT Loss: 1.736577 (Spektraltap)
2025-01-18 08:18:06,330 - INFO - [Batch] Epoch: 6/40, Batch: 15/25, Combined Loss: 0.057316 (Summen av L1 og STFT), L1 Loss: 0.625895 (Amplitudetap), STFT Loss: 1.451233 (Spektraltap)
2025-01-18 08:19:01,298 - INFO - [Batch] Epoch: 6/40, Batch: 16/25, Combined Loss: 0.024496 (Summen av L1 og STFT), L1 Loss: 0.188731 (Amplitudetap), STFT Loss: 0.654001 (Spektraltap)
2025-01-18 08:19:57,104 - INFO - [Batch] Epoch: 6/40, Batch: 17/25, Combined Loss: 0.125890 (Summen av L1 og STFT), L1 Loss: 1.398145 (Amplitudetap), STFT Loss: 3.177494 (Spektraltap)
2025-01-18 08:20:38,309 - INFO - [Batch] Epoch: 6/40, Batch: 18/25, Combined Loss: 0.037010 (Summen av L1 og STFT), L1 Loss: 0.390417 (Amplitudetap), STFT Loss: 0.942973 (Spektraltap)
2025-01-18 08:21:25,610 - INFO - [Batch] Epoch: 6/40, Batch: 19/25, Combined Loss: 0.029656 (Summen av L1 og STFT), L1 Loss: 0.303779 (Amplitudetap), STFT Loss: 0.759480 (Spektraltap)
2025-01-18 08:22:20,324 - INFO - [Batch] Epoch: 6/40, Batch: 20/25, Combined Loss: 0.156571 (Summen av L1 og STFT), L1 Loss: 1.802802 (Amplitudetap), STFT Loss: 3.924499 (Spektraltap)
2025-01-18 08:23:07,266 - INFO - [Batch] Epoch: 6/40, Batch: 21/25, Combined Loss: 0.120158 (Summen av L1 og STFT), L1 Loss: 1.454109 (Amplitudetap), STFT Loss: 2.981536 (Spektraltap)
2025-01-18 08:23:07,282 - INFO - Epoch [6/40], Batch [21/25], Gradient Norm: 0.0917
2025-01-18 08:24:00,626 - INFO - [Batch] Epoch: 6/40, Batch: 22/25, Combined Loss: 0.103433 (Summen av L1 og STFT), L1 Loss: 1.230876 (Amplitudetap), STFT Loss: 2.575482 (Spektraltap)
2025-01-18 08:24:52,941 - INFO - [Batch] Epoch: 6/40, Batch: 23/25, Combined Loss: 0.031102 (Summen av L1 og STFT), L1 Loss: 0.244775 (Amplitudetap), STFT Loss: 0.828168 (Spektraltap)
2025-01-18 08:25:35,054 - INFO - [Batch] Epoch: 6/40, Batch: 24/25, Combined Loss: 0.074368 (Summen av L1 og STFT), L1 Loss: 1.021905 (Amplitudetap), STFT Loss: 1.793085 (Spektraltap)
2025-01-18 08:26:32,902 - INFO - [Batch] Epoch: 6/40, Batch: 25/25, Combined Loss: 0.109050 (Summen av L1 og STFT), L1 Loss: 1.328831 (Amplitudetap), STFT Loss: 2.702009 (Spektraltap)
2025-01-18 08:26:32,902 - INFO - Epoch [6/40], Batch [25/25], Gradient Norm: 0.0542
2025-01-18 08:26:32,902 - INFO - Current Learning Rate 5e-08
2025-01-18 08:26:32,902 - INFO - [Epoch Improvement] Epoch 6: Loss improved by 0.00% from previous epoch.
2025-01-18 08:26:32,902 - INFO - [Epoch Summary] Avg Training Loss: 0.084295
2025-01-18 08:26:33,043 - INFO - [Train] New best model saved at C:\Users\didri\Desktop\UNet Models\UNet_vocal_isolation_model\Model_weights\CheckPoints\best_model_epoch-5.pth with loss 0.084295
2025-01-18 08:35:11,821 - INFO - [Validation] Epoch 6/40, [Validation] Avg Validation Loss:  1.079099
2025-01-18 08:35:11,821 - INFO - [Epoch Summary] Epoch: 6/40, Avg Combined Loss: 0.084295, Avg L1 Loss: 0.980711, Avg STFT Loss: 2.109271
2025-01-18 08:35:11,821 - INFO - Previous epoch loss: 0.08429522976279259
2025-01-18 08:35:11,821 - INFO - [Train] Epoch 7/40 started.
2025-01-18 08:36:06,170 - INFO - [Batch] Epoch: 7/40, Batch: 1/25, Combined Loss: 0.124619 (Summen av L1 og STFT), L1 Loss: 1.426399 (Amplitudetap), STFT Loss: 3.127261 (Spektraltap)
2025-01-18 08:36:44,857 - INFO - [Batch] Epoch: 7/40, Batch: 2/25, Combined Loss: 0.039194 (Summen av L1 og STFT), L1 Loss: 0.306634 (Amplitudetap), STFT Loss: 1.044402 (Spektraltap)
2025-01-18 08:37:29,675 - INFO - [Batch] Epoch: 7/40, Batch: 3/25, Combined Loss: 0.088122 (Summen av L1 og STFT), L1 Loss: 0.887256 (Amplitudetap), STFT Loss: 2.263421 (Spektraltap)
2025-01-18 08:38:03,699 - INFO - [Batch] Epoch: 7/40, Batch: 4/25, Combined Loss: 0.110571 (Summen av L1 og STFT), L1 Loss: 1.269493 (Amplitudetap), STFT Loss: 2.773052 (Spektraltap)
2025-01-18 08:38:53,334 - INFO - [Batch] Epoch: 7/40, Batch: 5/25, Combined Loss: 0.110910 (Summen av L1 og STFT), L1 Loss: 1.326486 (Amplitudetap), STFT Loss: 2.758816 (Spektraltap)
2025-01-18 08:39:45,047 - INFO - [Batch] Epoch: 7/40, Batch: 6/25, Combined Loss: 0.024562 (Summen av L1 og STFT), L1 Loss: 0.238607 (Amplitudetap), STFT Loss: 0.634598 (Spektraltap)
2025-01-18 08:40:33,891 - INFO - [Batch] Epoch: 7/40, Batch: 7/25, Combined Loss: 0.061624 (Summen av L1 og STFT), L1 Loss: 0.734732 (Amplitudetap), STFT Loss: 1.533842 (Spektraltap)
2025-01-18 08:41:23,517 - INFO - [Batch] Epoch: 7/40, Batch: 8/25, Combined Loss: 0.143882 (Summen av L1 og STFT), L1 Loss: 1.746987 (Amplitudetap), STFT Loss: 3.567737 (Spektraltap)
2025-01-18 08:42:18,053 - INFO - [Batch] Epoch: 7/40, Batch: 9/25, Combined Loss: 0.076559 (Summen av L1 og STFT), L1 Loss: 1.045719 (Amplitudetap), STFT Loss: 1.848607 (Spektraltap)
2025-01-18 08:43:01,451 - INFO - [Batch] Epoch: 7/40, Batch: 10/25, Combined Loss: 0.085074 (Summen av L1 og STFT), L1 Loss: 1.229694 (Amplitudetap), STFT Loss: 2.025203 (Spektraltap)
2025-01-18 08:43:44,400 - INFO - [Batch] Epoch: 7/40, Batch: 11/25, Combined Loss: 0.068293 (Summen av L1 og STFT), L1 Loss: 0.719368 (Amplitudetap), STFT Loss: 1.740488 (Spektraltap)
2025-01-18 08:44:31,723 - INFO - [Batch] Epoch: 7/40, Batch: 12/25, Combined Loss: 0.128065 (Summen av L1 og STFT), L1 Loss: 1.561860 (Amplitudetap), STFT Loss: 3.172579 (Spektraltap)
2025-01-18 08:45:15,962 - INFO - [Batch] Epoch: 7/40, Batch: 13/25, Combined Loss: 0.056974 (Summen av L1 og STFT), L1 Loss: 0.717842 (Amplitudetap), STFT Loss: 1.401571 (Spektraltap)
2025-01-18 08:46:12,556 - INFO - [Batch] Epoch: 7/40, Batch: 14/25, Combined Loss: 0.069088 (Summen av L1 og STFT), L1 Loss: 0.820950 (Amplitudetap), STFT Loss: 1.720812 (Spektraltap)
2025-01-18 08:47:08,098 - INFO - [Batch] Epoch: 7/40, Batch: 15/25, Combined Loss: 0.036758 (Summen av L1 og STFT), L1 Loss: 0.384260 (Amplitudetap), STFT Loss: 0.938062 (Spektraltap)
2025-01-18 08:47:54,446 - INFO - [Batch] Epoch: 7/40, Batch: 16/25, Combined Loss: 0.045067 (Summen av L1 og STFT), L1 Loss: 0.335130 (Amplitudetap), STFT Loss: 1.208383 (Spektraltap)
2025-01-18 08:48:43,135 - INFO - [Batch] Epoch: 7/40, Batch: 17/25, Combined Loss: 0.114304 (Summen av L1 og STFT), L1 Loss: 1.397444 (Amplitudetap), STFT Loss: 2.830217 (Spektraltap)
2025-01-18 08:49:34,517 - INFO - [Batch] Epoch: 7/40, Batch: 18/25, Combined Loss: 0.077694 (Summen av L1 og STFT), L1 Loss: 0.694388 (Amplitudetap), STFT Loss: 2.033232 (Spektraltap)
2025-01-18 08:50:30,346 - INFO - [Batch] Epoch: 7/40, Batch: 19/25, Combined Loss: 0.096325 (Summen av L1 og STFT), L1 Loss: 1.156769 (Amplitudetap), STFT Loss: 2.393979 (Spektraltap)
2025-01-18 08:51:28,888 - INFO - [Batch] Epoch: 7/40, Batch: 20/25, Combined Loss: 0.131818 (Summen av L1 og STFT), L1 Loss: 1.615096 (Amplitudetap), STFT Loss: 3.262352 (Spektraltap)
2025-01-18 08:52:28,636 - INFO - [Batch] Epoch: 7/40, Batch: 21/25, Combined Loss: 0.126594 (Summen av L1 og STFT), L1 Loss: 1.525839 (Amplitudetap), STFT Loss: 3.143885 (Spektraltap)
2025-01-18 08:52:28,636 - INFO - Epoch [7/40], Batch [21/25], Gradient Norm: 0.0800
2025-01-18 08:53:12,397 - INFO - [Batch] Epoch: 7/40, Batch: 22/25, Combined Loss: 0.072378 (Summen av L1 og STFT), L1 Loss: 1.036717 (Amplitudetap), STFT Loss: 1.727026 (Spektraltap)
2025-01-18 08:54:05,507 - INFO - [Batch] Epoch: 7/40, Batch: 23/25, Combined Loss: 0.046901 (Summen av L1 og STFT), L1 Loss: 0.391698 (Amplitudetap), STFT Loss: 1.239146 (Spektraltap)
2025-01-18 08:54:58,178 - INFO - [Batch] Epoch: 7/40, Batch: 24/25, Combined Loss: 0.087054 (Summen av L1 og STFT), L1 Loss: 0.933482 (Amplitudetap), STFT Loss: 2.211551 (Spektraltap)
2025-01-18 08:55:52,579 - INFO - [Batch] Epoch: 7/40, Batch: 25/25, Combined Loss: 0.085190 (Summen av L1 og STFT), L1 Loss: 1.016162 (Amplitudetap), STFT Loss: 2.120188 (Spektraltap)
2025-01-18 08:55:52,579 - INFO - Epoch [7/40], Batch [25/25], Gradient Norm: 0.0469
2025-01-18 08:55:52,595 - INFO - Current Learning Rate 5e-08
2025-01-18 08:55:52,595 - INFO - [Epoch Improvement] Epoch 7: Loss improved by 0.00% from previous epoch.
2025-01-18 08:55:52,595 - INFO - [Epoch Summary] Avg Training Loss: 0.084305
2025-01-18 08:55:52,595 - INFO - [Train] No improvement in loss for epoch 7 with loss: 0.08430474258959293. Best loss remains 0.084295. Trigger_times: 0
2025-01-18 09:04:27,440 - INFO - [Validation] Epoch 7/40, [Validation] Avg Validation Loss:  1.079055
2025-01-18 09:04:27,440 - INFO - [Epoch Summary] Epoch: 7/40, Avg Combined Loss: 0.084305, Avg L1 Loss: 0.980718, Avg STFT Loss: 2.109206
2025-01-18 09:04:27,440 - INFO - Previous epoch loss: 0.08430474258959293
2025-01-18 09:04:27,440 - INFO - [Train] Epoch 8/40 started.
2025-01-18 09:05:23,602 - INFO - [Batch] Epoch: 8/40, Batch: 1/25, Combined Loss: 0.140907 (Summen av L1 og STFT), L1 Loss: 1.807320 (Amplitudetap), STFT Loss: 3.452648 (Spektraltap)
2025-01-18 09:06:07,646 - INFO - [Batch] Epoch: 8/40, Batch: 2/25, Combined Loss: 0.086927 (Summen av L1 og STFT), L1 Loss: 0.991979 (Amplitudetap), STFT Loss: 2.182689 (Spektraltap)
2025-01-18 09:06:53,206 - INFO - [Batch] Epoch: 8/40, Batch: 3/25, Combined Loss: 0.077019 (Summen av L1 og STFT), L1 Loss: 0.725949 (Amplitudetap), STFT Loss: 1.999455 (Spektraltap)
2025-01-18 09:07:49,342 - INFO - [Batch] Epoch: 8/40, Batch: 4/25, Combined Loss: 0.170872 (Summen av L1 og STFT), L1 Loss: 2.076658 (Amplitudetap), STFT Loss: 4.236158 (Spektraltap)
2025-01-18 09:08:40,648 - INFO - [Batch] Epoch: 8/40, Batch: 5/25, Combined Loss: 0.063651 (Summen av L1 og STFT), L1 Loss: 0.582738 (Amplitudetap), STFT Loss: 1.659788 (Spektraltap)
2025-01-18 09:09:27,065 - INFO - [Batch] Epoch: 8/40, Batch: 6/25, Combined Loss: 0.124282 (Summen av L1 og STFT), L1 Loss: 1.465378 (Amplitudetap), STFT Loss: 3.100428 (Spektraltap)
2025-01-18 09:10:22,003 - INFO - [Batch] Epoch: 8/40, Batch: 7/25, Combined Loss: 0.020190 (Summen av L1 og STFT), L1 Loss: 0.200060 (Amplitudetap), STFT Loss: 0.519947 (Spektraltap)
2025-01-18 09:11:01,536 - INFO - [Batch] Epoch: 8/40, Batch: 8/25, Combined Loss: 0.091121 (Summen av L1 og STFT), L1 Loss: 0.927190 (Amplitudetap), STFT Loss: 2.336277 (Spektraltap)
2025-01-18 09:11:57,478 - INFO - [Batch] Epoch: 8/40, Batch: 9/25, Combined Loss: 0.128547 (Summen av L1 og STFT), L1 Loss: 1.489049 (Amplitudetap), STFT Loss: 3.218243 (Spektraltap)
2025-01-18 09:12:43,802 - INFO - [Batch] Epoch: 8/40, Batch: 10/25, Combined Loss: 0.056691 (Summen av L1 og STFT), L1 Loss: 0.492848 (Amplitudetap), STFT Loss: 1.489497 (Spektraltap)
2025-01-18 09:13:28,580 - INFO - [Batch] Epoch: 8/40, Batch: 11/25, Combined Loss: 0.076681 (Summen av L1 og STFT), L1 Loss: 0.847907 (Amplitudetap), STFT Loss: 1.937040 (Spektraltap)
2025-01-18 09:14:09,616 - INFO - [Batch] Epoch: 8/40, Batch: 12/25, Combined Loss: 0.031358 (Summen av L1 og STFT), L1 Loss: 0.261052 (Amplitudetap), STFT Loss: 0.828851 (Spektraltap)
2025-01-18 09:14:57,395 - INFO - [Batch] Epoch: 8/40, Batch: 13/25, Combined Loss: 0.020114 (Summen av L1 og STFT), L1 Loss: 0.127848 (Amplitudetap), STFT Loss: 0.548624 (Spektraltap)
2025-01-18 09:15:38,892 - INFO - [Batch] Epoch: 8/40, Batch: 14/25, Combined Loss: 0.140145 (Summen av L1 og STFT), L1 Loss: 1.685877 (Amplitudetap), STFT Loss: 3.481822 (Spektraltap)
2025-01-18 09:16:30,208 - INFO - [Batch] Epoch: 8/40, Batch: 15/25, Combined Loss: 0.099984 (Summen av L1 og STFT), L1 Loss: 1.226848 (Amplitudetap), STFT Loss: 2.473729 (Spektraltap)
2025-01-18 09:17:19,385 - INFO - [Batch] Epoch: 8/40, Batch: 16/25, Combined Loss: 0.037664 (Summen av L1 og STFT), L1 Loss: 0.392265 (Amplitudetap), STFT Loss: 0.961802 (Spektraltap)
2025-01-18 09:18:21,763 - INFO - [Batch] Epoch: 8/40, Batch: 17/25, Combined Loss: 0.129055 (Summen av L1 og STFT), L1 Loss: 1.787813 (Amplitudetap), STFT Loss: 3.105456 (Spektraltap)
2025-01-18 09:19:16,333 - INFO - [Batch] Epoch: 8/40, Batch: 18/25, Combined Loss: 0.110264 (Summen av L1 og STFT), L1 Loss: 1.519765 (Amplitudetap), STFT Loss: 2.656603 (Spektraltap)
2025-01-18 09:20:08,991 - INFO - [Batch] Epoch: 8/40, Batch: 19/25, Combined Loss: 0.028220 (Summen av L1 og STFT), L1 Loss: 0.185712 (Amplitudetap), STFT Loss: 0.767002 (Spektraltap)
2025-01-18 09:20:57,461 - INFO - [Batch] Epoch: 8/40, Batch: 20/25, Combined Loss: 0.062175 (Summen av L1 og STFT), L1 Loss: 0.702111 (Amplitudetap), STFT Loss: 1.564359 (Spektraltap)
2025-01-18 09:21:55,616 - INFO - [Batch] Epoch: 8/40, Batch: 21/25, Combined Loss: 0.119448 (Summen av L1 og STFT), L1 Loss: 1.459097 (Amplitudetap), STFT Loss: 2.958101 (Spektraltap)
2025-01-18 09:21:55,616 - INFO - Epoch [8/40], Batch [21/25], Gradient Norm: 0.0573
2025-01-18 09:22:48,481 - INFO - [Batch] Epoch: 8/40, Batch: 22/25, Combined Loss: 0.019764 (Summen av L1 og STFT), L1 Loss: 0.373754 (Amplitudetap), STFT Loss: 0.432744 (Spektraltap)
2025-01-18 09:23:51,484 - INFO - [Batch] Epoch: 8/40, Batch: 23/25, Combined Loss: 0.030268 (Summen av L1 og STFT), L1 Loss: 0.286707 (Amplitudetap), STFT Loss: 0.785155 (Spektraltap)
2025-01-18 09:24:37,789 - INFO - [Batch] Epoch: 8/40, Batch: 24/25, Combined Loss: 0.123336 (Summen av L1 og STFT), L1 Loss: 1.458208 (Amplitudetap), STFT Loss: 3.075126 (Spektraltap)
2025-01-18 09:25:30,523 - INFO - [Batch] Epoch: 8/40, Batch: 25/25, Combined Loss: 0.118669 (Summen av L1 og STFT), L1 Loss: 1.447334 (Amplitudetap), STFT Loss: 2.939784 (Spektraltap)
2025-01-18 09:25:30,523 - INFO - Epoch [8/40], Batch [25/25], Gradient Norm: 0.0867
2025-01-18 09:25:30,523 - INFO - Current Learning Rate 5e-08
2025-01-18 09:25:30,523 - INFO - [Epoch Improvement] Epoch 8: Loss improved by 0.00% from previous epoch.
2025-01-18 09:25:30,523 - INFO - [Epoch Summary] Avg Training Loss: 0.084294
2025-01-18 09:25:30,659 - INFO - [Train] New best model saved at C:\Users\didri\Desktop\UNet Models\UNet_vocal_isolation_model\Model_weights\CheckPoints\best_model_epoch-7.pth with loss 0.084294
2025-01-18 09:34:13,632 - INFO - [Validation] Epoch 8/40, [Validation] Avg Validation Loss:  1.079036
2025-01-18 09:34:13,633 - INFO - [Epoch Summary] Epoch: 8/40, Avg Combined Loss: 0.084294, Avg L1 Loss: 0.980736, Avg STFT Loss: 2.109112
2025-01-18 09:34:13,633 - INFO - Previous epoch loss: 0.0842940393090248
2025-01-18 09:34:13,633 - INFO - [Train] Epoch 9/40 started.
2025-01-18 09:35:12,573 - INFO - [Batch] Epoch: 9/40, Batch: 1/25, Combined Loss: 0.111355 (Summen av L1 og STFT), L1 Loss: 1.377030 (Amplitudetap), STFT Loss: 2.750492 (Spektraltap)
2025-01-18 09:36:03,745 - INFO - [Batch] Epoch: 9/40, Batch: 2/25, Combined Loss: 0.152924 (Summen av L1 og STFT), L1 Loss: 2.131906 (Amplitudetap), STFT Loss: 3.674039 (Spektraltap)
2025-01-18 09:36:49,456 - INFO - [Batch] Epoch: 9/40, Batch: 3/25, Combined Loss: 0.023886 (Summen av L1 og STFT), L1 Loss: 0.204631 (Amplitudetap), STFT Loss: 0.628890 (Spektraltap)
2025-01-18 09:37:42,756 - INFO - [Batch] Epoch: 9/40, Batch: 4/25, Combined Loss: 0.125809 (Summen av L1 og STFT), L1 Loss: 1.521839 (Amplitudetap), STFT Loss: 3.122056 (Spektraltap)
2025-01-18 09:38:30,166 - INFO - [Batch] Epoch: 9/40, Batch: 5/25, Combined Loss: 0.039610 (Summen av L1 og STFT), L1 Loss: 0.357624 (Amplitudetap), STFT Loss: 1.035025 (Spektraltap)
2025-01-18 09:39:27,554 - INFO - [Batch] Epoch: 9/40, Batch: 6/25, Combined Loss: 0.069591 (Summen av L1 og STFT), L1 Loss: 0.823126 (Amplitudetap), STFT Loss: 1.734958 (Spektraltap)
2025-01-18 09:40:07,033 - INFO - [Batch] Epoch: 9/40, Batch: 7/25, Combined Loss: 0.039055 (Summen av L1 og STFT), L1 Loss: 0.530757 (Amplitudetap), STFT Loss: 0.944172 (Spektraltap)
2025-01-18 09:40:45,289 - INFO - [Batch] Epoch: 9/40, Batch: 8/25, Combined Loss: 0.091133 (Summen av L1 og STFT), L1 Loss: 0.997603 (Amplitudetap), STFT Loss: 2.306450 (Spektraltap)
2025-01-18 09:41:39,974 - INFO - [Batch] Epoch: 9/40, Batch: 9/25, Combined Loss: 0.096812 (Summen av L1 og STFT), L1 Loss: 1.175436 (Amplitudetap), STFT Loss: 2.400609 (Spektraltap)
2025-01-18 09:42:28,485 - INFO - [Batch] Epoch: 9/40, Batch: 10/25, Combined Loss: 0.076958 (Summen av L1 og STFT), L1 Loss: 0.949016 (Amplitudetap), STFT Loss: 1.902025 (Spektraltap)
2025-01-18 09:43:26,447 - INFO - [Batch] Epoch: 9/40, Batch: 11/25, Combined Loss: 0.110320 (Summen av L1 og STFT), L1 Loss: 1.409182 (Amplitudetap), STFT Loss: 2.705662 (Spektraltap)
2025-01-18 09:44:00,771 - INFO - [Batch] Epoch: 9/40, Batch: 12/25, Combined Loss: 0.085308 (Summen av L1 og STFT), L1 Loss: 1.240219 (Amplitudetap), STFT Loss: 2.027704 (Spektraltap)
2025-01-18 09:44:53,646 - INFO - [Batch] Epoch: 9/40, Batch: 13/25, Combined Loss: 0.109670 (Summen av L1 og STFT), L1 Loss: 1.160476 (Amplitudetap), STFT Loss: 2.792746 (Spektraltap)
2025-01-18 09:45:42,719 - INFO - [Batch] Epoch: 9/40, Batch: 14/25, Combined Loss: 0.089736 (Summen av L1 og STFT), L1 Loss: 0.915592 (Amplitudetap), STFT Loss: 2.299676 (Spektraltap)
2025-01-18 09:46:32,201 - INFO - [Batch] Epoch: 9/40, Batch: 15/25, Combined Loss: 0.027626 (Summen av L1 og STFT), L1 Loss: 0.230894 (Amplitudetap), STFT Loss: 0.729823 (Spektraltap)
2025-01-18 09:47:20,089 - INFO - [Batch] Epoch: 9/40, Batch: 16/25, Combined Loss: 0.069417 (Summen av L1 og STFT), L1 Loss: 0.786442 (Amplitudetap), STFT Loss: 1.745478 (Spektraltap)
2025-01-18 09:48:00,349 - INFO - [Batch] Epoch: 9/40, Batch: 17/25, Combined Loss: 0.091863 (Summen av L1 og STFT), L1 Loss: 0.986597 (Amplitudetap), STFT Loss: 2.333057 (Spektraltap)
2025-01-18 09:48:44,990 - INFO - [Batch] Epoch: 9/40, Batch: 18/25, Combined Loss: 0.054071 (Summen av L1 og STFT), L1 Loss: 0.608244 (Amplitudetap), STFT Loss: 1.361444 (Spektraltap)
2025-01-18 09:49:39,588 - INFO - [Batch] Epoch: 9/40, Batch: 19/25, Combined Loss: 0.096569 (Summen av L1 og STFT), L1 Loss: 1.033775 (Amplitudetap), STFT Loss: 2.454009 (Spektraltap)
2025-01-18 09:50:39,313 - INFO - [Batch] Epoch: 9/40, Batch: 20/25, Combined Loss: 0.073316 (Summen av L1 og STFT), L1 Loss: 0.859841 (Amplitudetap), STFT Loss: 1.830980 (Spektraltap)
2025-01-18 09:51:26,398 - INFO - [Batch] Epoch: 9/40, Batch: 21/25, Combined Loss: 0.180519 (Summen av L1 og STFT), L1 Loss: 2.200348 (Amplitudetap), STFT Loss: 4.472572 (Spektraltap)
2025-01-18 09:51:26,398 - INFO - Epoch [9/40], Batch [21/25], Gradient Norm: 0.0889
2025-01-18 09:52:16,702 - INFO - [Batch] Epoch: 9/40, Batch: 22/25, Combined Loss: 0.089574 (Summen av L1 og STFT), L1 Loss: 0.860108 (Amplitudetap), STFT Loss: 2.318614 (Spektraltap)
2025-01-18 09:53:21,040 - INFO - [Batch] Epoch: 9/40, Batch: 23/25, Combined Loss: 0.080820 (Summen av L1 og STFT), L1 Loss: 0.907849 (Amplitudetap), STFT Loss: 2.035529 (Spektraltap)
2025-01-18 09:54:15,202 - INFO - [Batch] Epoch: 9/40, Batch: 24/25, Combined Loss: 0.027460 (Summen av L1 og STFT), L1 Loss: 0.205651 (Amplitudetap), STFT Loss: 0.735661 (Spektraltap)
2025-01-18 09:55:01,461 - INFO - [Batch] Epoch: 9/40, Batch: 25/25, Combined Loss: 0.094032 (Summen av L1 og STFT), L1 Loss: 1.048033 (Amplitudetap), STFT Loss: 2.371788 (Spektraltap)
2025-01-18 09:55:01,461 - INFO - Epoch [9/40], Batch [25/25], Gradient Norm: 0.0553
2025-01-18 09:55:01,477 - INFO - Current Learning Rate 5e-08
2025-01-18 09:55:01,477 - INFO - [Epoch Improvement] Epoch 9: Loss improved by 0.00% from previous epoch.
2025-01-18 09:55:01,477 - INFO - [Epoch Summary] Avg Training Loss: 0.084297
2025-01-18 09:55:01,477 - INFO - [Train] No improvement in loss for epoch 9 with loss: 0.084297306984663. Best loss remains 0.084294. Trigger_times: 0
2025-01-18 10:03:40,338 - INFO - [Validation] Epoch 9/40, [Validation] Avg Validation Loss:  1.078999
2025-01-18 10:03:40,338 - INFO - [Epoch Summary] Epoch: 9/40, Avg Combined Loss: 0.084297, Avg L1 Loss: 0.980753, Avg STFT Loss: 2.109048
2025-01-18 10:03:40,338 - INFO - Previous epoch loss: 0.084297306984663
2025-01-18 10:03:40,338 - INFO - [Train] Epoch 10/40 started.
2025-01-18 10:04:19,134 - INFO - [Batch] Epoch: 10/40, Batch: 1/25, Combined Loss: 0.042357 (Summen av L1 og STFT), L1 Loss: 0.376679 (Amplitudetap), STFT Loss: 1.109271 (Spektraltap)
2025-01-18 10:05:09,406 - INFO - [Batch] Epoch: 10/40, Batch: 2/25, Combined Loss: 0.023582 (Summen av L1 og STFT), L1 Loss: 0.128364 (Amplitudetap), STFT Loss: 0.652437 (Spektraltap)
2025-01-18 10:05:49,722 - INFO - [Batch] Epoch: 10/40, Batch: 3/25, Combined Loss: 0.127315 (Summen av L1 og STFT), L1 Loss: 1.649726 (Amplitudetap), STFT Loss: 3.112437 (Spektraltap)
2025-01-18 10:06:46,661 - INFO - [Batch] Epoch: 10/40, Batch: 4/25, Combined Loss: 0.123853 (Summen av L1 og STFT), L1 Loss: 1.745972 (Amplitudetap), STFT Loss: 2.967314 (Spektraltap)
2025-01-18 10:07:35,164 - INFO - [Batch] Epoch: 10/40, Batch: 5/25, Combined Loss: 0.045184 (Summen av L1 og STFT), L1 Loss: 0.353303 (Amplitudetap), STFT Loss: 1.204105 (Spektraltap)
2025-01-18 10:08:22,284 - INFO - [Batch] Epoch: 10/40, Batch: 6/25, Combined Loss: 0.038030 (Summen av L1 og STFT), L1 Loss: 0.293609 (Amplitudetap), STFT Loss: 1.015068 (Spektraltap)
2025-01-18 10:09:10,952 - INFO - [Batch] Epoch: 10/40, Batch: 7/25, Combined Loss: 0.083169 (Summen av L1 og STFT), L1 Loss: 0.958636 (Amplitudetap), STFT Loss: 2.084231 (Spektraltap)
2025-01-18 10:09:59,640 - INFO - [Batch] Epoch: 10/40, Batch: 8/25, Combined Loss: 0.149065 (Summen av L1 og STFT), L1 Loss: 1.769430 (Amplitudetap), STFT Loss: 3.713625 (Spektraltap)
2025-01-18 10:10:58,063 - INFO - [Batch] Epoch: 10/40, Batch: 9/25, Combined Loss: 0.064701 (Summen av L1 og STFT), L1 Loss: 0.664442 (Amplitudetap), STFT Loss: 1.656257 (Spektraltap)
2025-01-18 10:11:45,083 - INFO - [Batch] Epoch: 10/40, Batch: 10/25, Combined Loss: 0.068236 (Summen av L1 og STFT), L1 Loss: 0.967743 (Amplitudetap), STFT Loss: 1.632337 (Spektraltap)
2025-01-18 10:12:28,375 - INFO - [Batch] Epoch: 10/40, Batch: 11/25, Combined Loss: 0.040411 (Summen av L1 og STFT), L1 Loss: 0.476438 (Amplitudetap), STFT Loss: 1.008129 (Spektraltap)
2025-01-18 10:13:19,069 - INFO - [Batch] Epoch: 10/40, Batch: 12/25, Combined Loss: 0.112597 (Summen av L1 og STFT), L1 Loss: 1.409211 (Amplitudetap), STFT Loss: 2.773955 (Spektraltap)
2025-01-18 10:14:09,487 - INFO - [Batch] Epoch: 10/40, Batch: 13/25, Combined Loss: 0.118607 (Summen av L1 og STFT), L1 Loss: 1.349145 (Amplitudetap), STFT Loss: 2.979998 (Spektraltap)
2025-01-18 10:14:58,787 - INFO - [Batch] Epoch: 10/40, Batch: 14/25, Combined Loss: 0.032672 (Summen av L1 og STFT), L1 Loss: 0.207988 (Amplitudetap), STFT Loss: 0.891014 (Spektraltap)
2025-01-18 10:15:57,159 - INFO - [Batch] Epoch: 10/40, Batch: 15/25, Combined Loss: 0.079083 (Summen av L1 og STFT), L1 Loss: 0.823584 (Amplitudetap), STFT Loss: 2.019512 (Spektraltap)
2025-01-18 10:16:57,015 - INFO - [Batch] Epoch: 10/40, Batch: 16/25, Combined Loss: 0.080114 (Summen av L1 og STFT), L1 Loss: 0.950210 (Amplitudetap), STFT Loss: 1.996178 (Spektraltap)
2025-01-18 10:17:49,828 - INFO - [Batch] Epoch: 10/40, Batch: 17/25, Combined Loss: 0.031380 (Summen av L1 og STFT), L1 Loss: 0.340642 (Amplitudetap), STFT Loss: 0.795422 (Spektraltap)
2025-01-18 10:18:48,110 - INFO - [Batch] Epoch: 10/40, Batch: 18/25, Combined Loss: 0.199051 (Summen av L1 og STFT), L1 Loss: 2.494081 (Amplitudetap), STFT Loss: 4.902624 (Spektraltap)
2025-01-18 10:19:31,540 - INFO - [Batch] Epoch: 10/40, Batch: 19/25, Combined Loss: 0.035759 (Summen av L1 og STFT), L1 Loss: 0.266538 (Amplitudetap), STFT Loss: 0.958526 (Spektraltap)
2025-01-18 10:20:19,746 - INFO - [Batch] Epoch: 10/40, Batch: 20/25, Combined Loss: 0.131813 (Summen av L1 og STFT), L1 Loss: 1.620175 (Amplitudetap), STFT Loss: 3.260022 (Spektraltap)
2025-01-18 10:21:05,556 - INFO - [Batch] Epoch: 10/40, Batch: 21/25, Combined Loss: 0.191694 (Summen av L1 og STFT), L1 Loss: 2.366701 (Amplitudetap), STFT Loss: 4.736521 (Spektraltap)
2025-01-18 10:21:05,572 - INFO - Epoch [10/40], Batch [21/25], Gradient Norm: 0.1149
2025-01-18 10:22:00,563 - INFO - [Batch] Epoch: 10/40, Batch: 22/25, Combined Loss: 0.136781 (Summen av L1 og STFT), L1 Loss: 1.657545 (Amplitudetap), STFT Loss: 3.393057 (Spektraltap)
2025-01-18 10:22:49,580 - INFO - [Batch] Epoch: 10/40, Batch: 23/25, Combined Loss: 0.047255 (Summen av L1 og STFT), L1 Loss: 0.530726 (Amplitudetap), STFT Loss: 1.190188 (Spektraltap)
2025-01-18 10:23:46,492 - INFO - [Batch] Epoch: 10/40, Batch: 24/25, Combined Loss: 0.022969 (Summen av L1 og STFT), L1 Loss: 0.231806 (Amplitudetap), STFT Loss: 0.589731 (Spektraltap)
2025-01-18 10:24:38,828 - INFO - [Batch] Epoch: 10/40, Batch: 25/25, Combined Loss: 0.081614 (Summen av L1 og STFT), L1 Loss: 0.879132 (Amplitudetap), STFT Loss: 2.071656 (Spektraltap)
2025-01-18 10:24:38,828 - INFO - Epoch [10/40], Batch [25/25], Gradient Norm: 0.0971
2025-01-18 10:24:38,828 - INFO - Current Learning Rate 2.5e-08
2025-01-18 10:24:38,828 - INFO - [Epoch Improvement] Epoch 10: Loss improved by 0.00% from previous epoch.
2025-01-18 10:24:38,828 - INFO - [Epoch Summary] Avg Training Loss: 0.084292
2025-01-18 10:24:38,953 - INFO - [Train] New best model saved at C:\Users\didri\Desktop\UNet Models\UNet_vocal_isolation_model\Model_weights\CheckPoints\best_model_epoch-9.pth with loss 0.084292
2025-01-18 10:33:25,042 - INFO - [Validation] Epoch 10/40, [Validation] Avg Validation Loss:  1.079007
2025-01-18 10:33:25,042 - INFO - [Memory After Epoch 10] Timestamp: 2025-01-18 10:33:25
2025-01-18 10:33:25,042 - INFO - [Memory After Epoch 10] GPU 0: NVIDIA GeForce RTX 3060, Total=12.88 GB, Allocated=0.51 GB, Cached=23.26 GB
2025-01-18 10:33:25,042 - INFO - [Memory After Epoch 10] CPU Memory Usage: ~0.57 GB
2025-01-18 10:33:25,042 - INFO - [Epoch Summary] Epoch: 10/40, Avg Combined Loss: 0.084292, Avg L1 Loss: 0.980725, Avg STFT Loss: 2.108998
2025-01-18 10:33:25,042 - INFO - Previous epoch loss: 0.08429157845675946
2025-01-18 10:33:25,042 - INFO - [Train] Epoch 11/40 started.
2025-01-18 10:34:08,245 - INFO - [Batch] Epoch: 11/40, Batch: 1/25, Combined Loss: 0.052825 (Summen av L1 og STFT), L1 Loss: 0.562779 (Amplitudetap), STFT Loss: 1.343549 (Spektraltap)
2025-01-18 10:34:54,441 - INFO - [Batch] Epoch: 11/40, Batch: 2/25, Combined Loss: 0.036044 (Summen av L1 og STFT), L1 Loss: 0.271051 (Amplitudetap), STFT Loss: 0.965159 (Spektraltap)
2025-01-18 10:35:41,225 - INFO - [Batch] Epoch: 11/40, Batch: 3/25, Combined Loss: 0.168175 (Summen av L1 og STFT), L1 Loss: 2.012039 (Amplitudetap), STFT Loss: 4.182934 (Spektraltap)
2025-01-18 10:36:38,957 - INFO - [Batch] Epoch: 11/40, Batch: 4/25, Combined Loss: 0.105454 (Summen av L1 og STFT), L1 Loss: 1.435903 (Amplitudetap), STFT Loss: 2.548227 (Spektraltap)
2025-01-18 10:37:24,613 - INFO - [Batch] Epoch: 11/40, Batch: 5/25, Combined Loss: 0.136014 (Summen av L1 og STFT), L1 Loss: 1.441967 (Amplitudetap), STFT Loss: 3.462438 (Spektraltap)
2025-01-18 10:38:17,295 - INFO - [Batch] Epoch: 11/40, Batch: 6/25, Combined Loss: 0.068349 (Summen av L1 og STFT), L1 Loss: 0.764764 (Amplitudetap), STFT Loss: 1.722706 (Spektraltap)
2025-01-18 10:39:11,026 - INFO - [Batch] Epoch: 11/40, Batch: 7/25, Combined Loss: 0.076502 (Summen av L1 og STFT), L1 Loss: 0.834470 (Amplitudetap), STFT Loss: 1.937420 (Spektraltap)
2025-01-18 10:39:58,781 - INFO - [Batch] Epoch: 11/40, Batch: 8/25, Combined Loss: 0.037783 (Summen av L1 og STFT), L1 Loss: 0.308503 (Amplitudetap), STFT Loss: 1.001284 (Spektraltap)
2025-01-18 10:40:38,921 - INFO - [Batch] Epoch: 11/40, Batch: 9/25, Combined Loss: 0.037837 (Summen av L1 og STFT), L1 Loss: 0.364699 (Amplitudetap), STFT Loss: 0.978799 (Spektraltap)
2025-01-18 10:41:25,986 - INFO - [Batch] Epoch: 11/40, Batch: 10/25, Combined Loss: 0.079727 (Summen av L1 og STFT), L1 Loss: 1.020678 (Amplitudetap), STFT Loss: 1.954387 (Spektraltap)
2025-01-18 10:42:24,655 - INFO - [Batch] Epoch: 11/40, Batch: 11/25, Combined Loss: 0.127936 (Summen av L1 og STFT), L1 Loss: 1.476933 (Amplitudetap), STFT Loss: 3.205100 (Spektraltap)
2025-01-18 10:43:24,328 - INFO - [Batch] Epoch: 11/40, Batch: 12/25, Combined Loss: 0.178081 (Summen av L1 og STFT), L1 Loss: 2.170621 (Amplitudetap), STFT Loss: 4.412157 (Spektraltap)
2025-01-18 10:44:11,869 - INFO - [Batch] Epoch: 11/40, Batch: 13/25, Combined Loss: 0.075073 (Summen av L1 og STFT), L1 Loss: 0.730267 (Amplitudetap), STFT Loss: 1.939216 (Spektraltap)
2025-01-18 10:45:10,064 - INFO - [Batch] Epoch: 11/40, Batch: 14/25, Combined Loss: 0.077334 (Summen av L1 og STFT), L1 Loss: 0.845353 (Amplitudetap), STFT Loss: 1.957728 (Spektraltap)
2025-01-18 10:45:58,894 - INFO - [Batch] Epoch: 11/40, Batch: 15/25, Combined Loss: 0.049460 (Summen av L1 og STFT), L1 Loss: 0.576850 (Amplitudetap), STFT Loss: 1.236571 (Spektraltap)
2025-01-18 10:46:58,754 - INFO - [Batch] Epoch: 11/40, Batch: 16/25, Combined Loss: 0.062615 (Summen av L1 og STFT), L1 Loss: 0.704777 (Amplitudetap), STFT Loss: 1.576392 (Spektraltap)
2025-01-18 10:47:47,475 - INFO - [Batch] Epoch: 11/40, Batch: 17/25, Combined Loss: 0.129757 (Summen av L1 og STFT), L1 Loss: 1.488969 (Amplitudetap), STFT Loss: 3.254591 (Spektraltap)
2025-01-18 10:48:40,507 - INFO - [Batch] Epoch: 11/40, Batch: 18/25, Combined Loss: 0.021905 (Summen av L1 og STFT), L1 Loss: 0.178999 (Amplitudetap), STFT Loss: 0.580438 (Spektraltap)
2025-01-18 10:49:35,714 - INFO - [Batch] Epoch: 11/40, Batch: 19/25, Combined Loss: 0.134039 (Summen av L1 og STFT), L1 Loss: 1.720721 (Amplitudetap), STFT Loss: 3.283705 (Spektraltap)
2025-01-18 10:50:29,296 - INFO - [Batch] Epoch: 11/40, Batch: 20/25, Combined Loss: 0.064528 (Summen av L1 og STFT), L1 Loss: 0.622545 (Amplitudetap), STFT Loss: 1.669029 (Spektraltap)
2025-01-18 10:51:15,398 - INFO - [Batch] Epoch: 11/40, Batch: 21/25, Combined Loss: 0.098711 (Summen av L1 og STFT), L1 Loss: 1.335715 (Amplitudetap), STFT Loss: 2.388881 (Spektraltap)
2025-01-18 10:51:15,413 - INFO - Epoch [11/40], Batch [21/25], Gradient Norm: 0.0409
2025-01-18 10:52:02,097 - INFO - [Batch] Epoch: 11/40, Batch: 22/25, Combined Loss: 0.041833 (Summen av L1 og STFT), L1 Loss: 0.418945 (Amplitudetap), STFT Loss: 1.075430 (Spektraltap)
2025-01-18 10:52:49,650 - INFO - [Batch] Epoch: 11/40, Batch: 23/25, Combined Loss: 0.069076 (Summen av L1 og STFT), L1 Loss: 0.960751 (Amplitudetap), STFT Loss: 1.660523 (Spektraltap)
2025-01-18 10:53:42,242 - INFO - [Batch] Epoch: 11/40, Batch: 24/25, Combined Loss: 0.087843 (Summen av L1 og STFT), L1 Loss: 0.947063 (Amplitudetap), STFT Loss: 2.229410 (Spektraltap)
2025-01-18 10:54:28,046 - INFO - [Batch] Epoch: 11/40, Batch: 25/25, Combined Loss: 0.090567 (Summen av L1 og STFT), L1 Loss: 1.322493 (Amplitudetap), STFT Loss: 2.150213 (Spektraltap)
2025-01-18 10:54:28,046 - INFO - Epoch [11/40], Batch [25/25], Gradient Norm: 0.0424
2025-01-18 10:54:28,062 - INFO - Current Learning Rate 2.5e-08
2025-01-18 10:54:28,062 - INFO - [Epoch Improvement] Epoch 11: Loss improved by 0.00% from previous epoch.
2025-01-18 10:54:28,062 - INFO - [Epoch Summary] Avg Training Loss: 0.084299
2025-01-18 10:54:28,062 - INFO - [Train] No improvement in loss for epoch 11 with loss: 0.08429858364164829. Best loss remains 0.084292. Trigger_times: 0
2025-01-18 11:03:07,138 - INFO - [Validation] Epoch 11/40, [Validation] Avg Validation Loss:  1.078973
2025-01-18 11:03:07,138 - INFO - [Epoch Summary] Epoch: 11/40, Avg Combined Loss: 0.084299, Avg L1 Loss: 0.980724, Avg STFT Loss: 2.108967
2025-01-18 11:03:07,138 - INFO - Previous epoch loss: 0.08429858364164829
2025-01-18 11:03:07,138 - INFO - [Train] Epoch 12/40 started.
2025-01-18 11:04:00,561 - INFO - [Batch] Epoch: 12/40, Batch: 1/25, Combined Loss: 0.130915 (Summen av L1 og STFT), L1 Loss: 1.666675 (Amplitudetap), STFT Loss: 3.213173 (Spektraltap)
2025-01-18 11:04:56,680 - INFO - [Batch] Epoch: 12/40, Batch: 2/25, Combined Loss: 0.161369 (Summen av L1 og STFT), L1 Loss: 2.048339 (Amplitudetap), STFT Loss: 3.963199 (Spektraltap)
2025-01-18 11:05:46,512 - INFO - [Batch] Epoch: 12/40, Batch: 3/25, Combined Loss: 0.092892 (Summen av L1 og STFT), L1 Loss: 1.065537 (Amplitudetap), STFT Loss: 2.330096 (Spektraltap)
2025-01-18 11:06:43,983 - INFO - [Batch] Epoch: 12/40, Batch: 4/25, Combined Loss: 0.084819 (Summen av L1 og STFT), L1 Loss: 0.932553 (Amplitudetap), STFT Loss: 2.144917 (Spektraltap)
2025-01-18 11:07:24,325 - INFO - [Batch] Epoch: 12/40, Batch: 5/25, Combined Loss: 0.073132 (Summen av L1 og STFT), L1 Loss: 0.915817 (Amplitudetap), STFT Loss: 1.801479 (Spektraltap)
2025-01-18 11:08:16,606 - INFO - [Batch] Epoch: 12/40, Batch: 6/25, Combined Loss: 0.149196 (Summen av L1 og STFT), L1 Loss: 1.787816 (Amplitudetap), STFT Loss: 3.709683 (Spektraltap)
2025-01-18 11:09:10,205 - INFO - [Batch] Epoch: 12/40, Batch: 7/25, Combined Loss: 0.151660 (Summen av L1 og STFT), L1 Loss: 1.792514 (Amplitudetap), STFT Loss: 3.781565 (Spektraltap)
2025-01-18 11:10:03,034 - INFO - [Batch] Epoch: 12/40, Batch: 8/25, Combined Loss: 0.076189 (Summen av L1 og STFT), L1 Loss: 0.888144 (Amplitudetap), STFT Loss: 1.905024 (Spektraltap)
2025-01-18 11:10:53,159 - INFO - [Batch] Epoch: 12/40, Batch: 9/25, Combined Loss: 0.071432 (Summen av L1 og STFT), L1 Loss: 0.929395 (Amplitudetap), STFT Loss: 1.744655 (Spektraltap)
2025-01-18 11:11:27,876 - INFO - [Batch] Epoch: 12/40, Batch: 10/25, Combined Loss: 0.024994 (Summen av L1 og STFT), L1 Loss: 0.284579 (Amplitudetap), STFT Loss: 0.627857 (Spektraltap)
2025-01-18 11:12:17,420 - INFO - [Batch] Epoch: 12/40, Batch: 11/25, Combined Loss: 0.026921 (Summen av L1 og STFT), L1 Loss: 0.253325 (Amplitudetap), STFT Loss: 0.699076 (Spektraltap)
2025-01-18 11:12:57,364 - INFO - [Batch] Epoch: 12/40, Batch: 12/25, Combined Loss: 0.040425 (Summen av L1 og STFT), L1 Loss: 0.542916 (Amplitudetap), STFT Loss: 0.980057 (Spektraltap)
2025-01-18 11:13:48,326 - INFO - [Batch] Epoch: 12/40, Batch: 13/25, Combined Loss: 0.055826 (Summen av L1 og STFT), L1 Loss: 0.525642 (Amplitudetap), STFT Loss: 1.449520 (Spektraltap)
2025-01-18 11:14:34,701 - INFO - [Batch] Epoch: 12/40, Batch: 14/25, Combined Loss: 0.053433 (Summen av L1 og STFT), L1 Loss: 0.693086 (Amplitudetap), STFT Loss: 1.305948 (Spektraltap)
2025-01-18 11:15:22,313 - INFO - [Batch] Epoch: 12/40, Batch: 15/25, Combined Loss: 0.095857 (Summen av L1 og STFT), L1 Loss: 1.094572 (Amplitudetap), STFT Loss: 2.406602 (Spektraltap)
2025-01-18 11:16:06,261 - INFO - [Batch] Epoch: 12/40, Batch: 16/25, Combined Loss: 0.022374 (Summen av L1 og STFT), L1 Loss: 0.205431 (Amplitudetap), STFT Loss: 0.583180 (Spektraltap)
2025-01-18 11:16:57,645 - INFO - [Batch] Epoch: 12/40, Batch: 17/25, Combined Loss: 0.017439 (Summen av L1 og STFT), L1 Loss: 0.153134 (Amplitudetap), STFT Loss: 0.457532 (Spektraltap)
2025-01-18 11:17:41,607 - INFO - [Batch] Epoch: 12/40, Batch: 18/25, Combined Loss: 0.091854 (Summen av L1 og STFT), L1 Loss: 0.964890 (Amplitudetap), STFT Loss: 2.342105 (Spektraltap)
2025-01-18 11:18:30,393 - INFO - [Batch] Epoch: 12/40, Batch: 19/25, Combined Loss: 0.123867 (Summen av L1 og STFT), L1 Loss: 1.428192 (Amplitudetap), STFT Loss: 3.103931 (Spektraltap)
2025-01-18 11:19:17,160 - INFO - [Batch] Epoch: 12/40, Batch: 20/25, Combined Loss: 0.047570 (Summen av L1 og STFT), L1 Loss: 0.445717 (Amplitudetap), STFT Loss: 1.236082 (Spektraltap)
2025-01-18 11:20:06,206 - INFO - [Batch] Epoch: 12/40, Batch: 21/25, Combined Loss: 0.096874 (Summen av L1 og STFT), L1 Loss: 1.023577 (Amplitudetap), STFT Loss: 2.467546 (Spektraltap)
2025-01-18 11:20:06,206 - INFO - Epoch [12/40], Batch [21/25], Gradient Norm: 0.0354
2025-01-18 11:20:58,736 - INFO - [Batch] Epoch: 12/40, Batch: 22/25, Combined Loss: 0.087756 (Summen av L1 og STFT), L1 Loss: 0.994473 (Amplitudetap), STFT Loss: 2.206467 (Spektraltap)
2025-01-18 11:21:52,385 - INFO - [Batch] Epoch: 12/40, Batch: 23/25, Combined Loss: 0.072896 (Summen av L1 og STFT), L1 Loss: 0.848429 (Amplitudetap), STFT Loss: 1.823257 (Spektraltap)
2025-01-18 11:22:49,203 - INFO - [Batch] Epoch: 12/40, Batch: 24/25, Combined Loss: 0.123648 (Summen av L1 og STFT), L1 Loss: 1.464190 (Amplitudetap), STFT Loss: 3.081939 (Spektraltap)
2025-01-18 11:23:50,531 - INFO - [Batch] Epoch: 12/40, Batch: 25/25, Combined Loss: 0.134079 (Summen av L1 og STFT), L1 Loss: 1.566141 (Amplitudetap), STFT Loss: 3.351152 (Spektraltap)
2025-01-18 11:23:50,547 - INFO - Epoch [12/40], Batch [25/25], Gradient Norm: 0.0569
2025-01-18 11:23:50,547 - INFO - Current Learning Rate 2.5e-08
2025-01-18 11:23:50,547 - INFO - [Epoch Improvement] Epoch 12: Loss improved by 0.00% from previous epoch.
2025-01-18 11:23:50,547 - INFO - [Epoch Summary] Avg Training Loss: 0.084297
2025-01-18 11:23:50,547 - INFO - [Train] No improvement in loss for epoch 12 with loss: 0.08429667979478836. Best loss remains 0.084292. Trigger_times: 1
2025-01-18 11:32:31,634 - INFO - [Validation] Epoch 12/40, [Validation] Avg Validation Loss:  1.078970
2025-01-18 11:32:31,634 - INFO - [Epoch Summary] Epoch: 12/40, Avg Combined Loss: 0.084297, Avg L1 Loss: 0.980714, Avg STFT Loss: 2.108939
2025-01-18 11:32:31,639 - INFO - Previous epoch loss: 0.08429667979478836
2025-01-18 11:32:31,639 - INFO - [Train] Epoch 13/40 started.
2025-01-18 11:33:16,164 - INFO - [Batch] Epoch: 13/40, Batch: 1/25, Combined Loss: 0.057901 (Summen av L1 og STFT), L1 Loss: 0.687943 (Amplitudetap), STFT Loss: 1.442206 (Spektraltap)
2025-01-18 11:34:15,568 - INFO - [Batch] Epoch: 13/40, Batch: 2/25, Combined Loss: 0.015787 (Summen av L1 og STFT), L1 Loss: 0.145941 (Amplitudetap), STFT Loss: 0.411056 (Spektraltap)
2025-01-18 11:35:11,439 - INFO - [Batch] Epoch: 13/40, Batch: 3/25, Combined Loss: 0.153855 (Summen av L1 og STFT), L1 Loss: 1.942315 (Amplitudetap), STFT Loss: 3.783222 (Spektraltap)
2025-01-18 11:35:57,601 - INFO - [Batch] Epoch: 13/40, Batch: 4/25, Combined Loss: 0.040417 (Summen av L1 og STFT), L1 Loss: 0.356284 (Amplitudetap), STFT Loss: 1.059825 (Spektraltap)
2025-01-18 11:37:01,096 - INFO - [Batch] Epoch: 13/40, Batch: 5/25, Combined Loss: 0.123618 (Summen av L1 og STFT), L1 Loss: 1.523534 (Amplitudetap), STFT Loss: 3.055589 (Spektraltap)
2025-01-18 11:37:51,265 - INFO - [Batch] Epoch: 13/40, Batch: 6/25, Combined Loss: 0.027057 (Summen av L1 og STFT), L1 Loss: 0.261765 (Amplitudetap), STFT Loss: 0.699534 (Spektraltap)
2025-01-18 11:38:44,685 - INFO - [Batch] Epoch: 13/40, Batch: 7/25, Combined Loss: 0.063930 (Summen av L1 og STFT), L1 Loss: 0.824744 (Amplitudetap), STFT Loss: 1.564448 (Spektraltap)
2025-01-18 11:39:31,854 - INFO - [Batch] Epoch: 13/40, Batch: 8/25, Combined Loss: 0.035087 (Summen av L1 og STFT), L1 Loss: 0.272756 (Amplitudetap), STFT Loss: 0.935704 (Spektraltap)
2025-01-18 11:40:30,886 - INFO - [Batch] Epoch: 13/40, Batch: 9/25, Combined Loss: 0.133944 (Summen av L1 og STFT), L1 Loss: 1.637133 (Amplitudetap), STFT Loss: 3.316689 (Spektraltap)
2025-01-18 11:41:14,143 - INFO - [Batch] Epoch: 13/40, Batch: 10/25, Combined Loss: 0.085077 (Summen av L1 og STFT), L1 Loss: 0.923515 (Amplitudetap), STFT Loss: 2.156527 (Spektraltap)
2025-01-18 11:42:01,822 - INFO - [Batch] Epoch: 13/40, Batch: 11/25, Combined Loss: 0.110980 (Summen av L1 og STFT), L1 Loss: 1.264813 (Amplitudetap), STFT Loss: 2.787345 (Spektraltap)
2025-01-18 11:42:56,510 - INFO - [Batch] Epoch: 13/40, Batch: 12/25, Combined Loss: 0.121199 (Summen av L1 og STFT), L1 Loss: 1.493099 (Amplitudetap), STFT Loss: 2.996057 (Spektraltap)
2025-01-18 11:43:51,616 - INFO - [Batch] Epoch: 13/40, Batch: 13/25, Combined Loss: 0.070983 (Summen av L1 og STFT), L1 Loss: 0.798639 (Amplitudetap), STFT Loss: 1.787210 (Spektraltap)
2025-01-18 11:44:39,696 - INFO - [Batch] Epoch: 13/40, Batch: 14/25, Combined Loss: 0.089756 (Summen av L1 og STFT), L1 Loss: 1.049397 (Amplitudetap), STFT Loss: 2.242936 (Spektraltap)
2025-01-18 11:45:23,668 - INFO - [Batch] Epoch: 13/40, Batch: 15/25, Combined Loss: 0.085976 (Summen av L1 og STFT), L1 Loss: 1.074029 (Amplitudetap), STFT Loss: 2.118969 (Spektraltap)
2025-01-18 11:46:09,465 - INFO - [Batch] Epoch: 13/40, Batch: 16/25, Combined Loss: 0.079615 (Summen av L1 og STFT), L1 Loss: 0.867235 (Amplitudetap), STFT Loss: 2.016790 (Spektraltap)
2025-01-18 11:46:59,038 - INFO - [Batch] Epoch: 13/40, Batch: 17/25, Combined Loss: 0.095367 (Summen av L1 og STFT), L1 Loss: 1.187573 (Amplitudetap), STFT Loss: 2.352057 (Spektraltap)
2025-01-18 11:47:41,909 - INFO - [Batch] Epoch: 13/40, Batch: 18/25, Combined Loss: 0.078091 (Summen av L1 og STFT), L1 Loss: 0.846148 (Amplitudetap), STFT Loss: 1.980090 (Spektraltap)
2025-01-18 11:48:21,983 - INFO - [Batch] Epoch: 13/40, Batch: 19/25, Combined Loss: 0.135159 (Summen av L1 og STFT), L1 Loss: 1.442891 (Amplitudetap), STFT Loss: 3.436392 (Spektraltap)
2025-01-18 11:49:00,614 - INFO - [Batch] Epoch: 13/40, Batch: 20/25, Combined Loss: 0.020008 (Summen av L1 og STFT), L1 Loss: 0.208589 (Amplitudetap), STFT Loss: 0.510831 (Spektraltap)
2025-01-18 11:49:52,364 - INFO - [Batch] Epoch: 13/40, Batch: 21/25, Combined Loss: 0.182572 (Summen av L1 og STFT), L1 Loss: 2.269459 (Amplitudetap), STFT Loss: 4.504539 (Spektraltap)
2025-01-18 11:49:52,364 - INFO - Epoch [13/40], Batch [21/25], Gradient Norm: 0.1350
2025-01-18 11:50:50,710 - INFO - [Batch] Epoch: 13/40, Batch: 22/25, Combined Loss: 0.059445 (Summen av L1 og STFT), L1 Loss: 0.649563 (Amplitudetap), STFT Loss: 1.504976 (Spektraltap)
2025-01-18 11:51:47,850 - INFO - [Batch] Epoch: 13/40, Batch: 23/25, Combined Loss: 0.152659 (Summen av L1 og STFT), L1 Loss: 1.903584 (Amplitudetap), STFT Loss: 3.763963 (Spektraltap)
2025-01-18 11:52:34,808 - INFO - [Batch] Epoch: 13/40, Batch: 24/25, Combined Loss: 0.043608 (Summen av L1 og STFT), L1 Loss: 0.485675 (Amplitudetap), STFT Loss: 1.100105 (Spektraltap)
2025-01-18 11:53:24,808 - INFO - [Batch] Epoch: 13/40, Batch: 25/25, Combined Loss: 0.045030 (Summen av L1 og STFT), L1 Loss: 0.413790 (Amplitudetap), STFT Loss: 1.173576 (Spektraltap)
2025-01-18 11:53:24,816 - INFO - Epoch [13/40], Batch [25/25], Gradient Norm: 0.0358
2025-01-18 11:53:24,819 - INFO - Current Learning Rate 2.5e-08
2025-01-18 11:53:24,819 - INFO - [Epoch Improvement] Epoch 13: Loss improved by 0.00% from previous epoch.
2025-01-18 11:53:24,819 - INFO - [Epoch Summary] Avg Training Loss: 0.084285
2025-01-18 11:53:24,928 - INFO - [Train] New best model saved at C:\Users\didri\Desktop\UNet Models\UNet_vocal_isolation_model\Model_weights\CheckPoints\best_model_epoch-12.pth with loss 0.084285
2025-01-18 12:02:02,921 - INFO - [Validation] Epoch 13/40, [Validation] Avg Validation Loss:  1.078954
2025-01-18 12:02:02,921 - INFO - [Epoch Summary] Epoch: 13/40, Avg Combined Loss: 0.084285, Avg L1 Loss: 0.980753, Avg STFT Loss: 2.108869
2025-01-18 12:02:02,921 - INFO - Previous epoch loss: 0.08428489454090596
2025-01-18 12:02:02,921 - INFO - [Train] Epoch 14/40 started.
2025-01-18 12:02:42,987 - INFO - [Batch] Epoch: 14/40, Batch: 1/25, Combined Loss: 0.126127 (Summen av L1 og STFT), L1 Loss: 1.501226 (Amplitudetap), STFT Loss: 3.140431 (Spektraltap)
2025-01-18 12:03:35,937 - INFO - [Batch] Epoch: 14/40, Batch: 2/25, Combined Loss: 0.069220 (Summen av L1 og STFT), L1 Loss: 0.712287 (Amplitudetap), STFT Loss: 1.771320 (Spektraltap)
2025-01-18 12:04:33,858 - INFO - [Batch] Epoch: 14/40, Batch: 3/25, Combined Loss: 0.063509 (Summen av L1 og STFT), L1 Loss: 0.778784 (Amplitudetap), STFT Loss: 1.571493 (Spektraltap)
2025-01-18 12:05:27,395 - INFO - [Batch] Epoch: 14/40, Batch: 4/25, Combined Loss: 0.128382 (Summen av L1 og STFT), L1 Loss: 1.576968 (Amplitudetap), STFT Loss: 3.175606 (Spektraltap)
2025-01-18 12:06:08,310 - INFO - [Batch] Epoch: 14/40, Batch: 5/25, Combined Loss: 0.131035 (Summen av L1 og STFT), L1 Loss: 1.490056 (Amplitudetap), STFT Loss: 3.292452 (Spektraltap)
2025-01-18 12:06:53,888 - INFO - [Batch] Epoch: 14/40, Batch: 6/25, Combined Loss: 0.021253 (Summen av L1 og STFT), L1 Loss: 0.104407 (Amplitudetap), STFT Loss: 0.592847 (Spektraltap)
2025-01-18 12:07:35,230 - INFO - [Batch] Epoch: 14/40, Batch: 7/25, Combined Loss: 0.041984 (Summen av L1 og STFT), L1 Loss: 0.413137 (Amplitudetap), STFT Loss: 1.082465 (Spektraltap)
2025-01-18 12:08:23,055 - INFO - [Batch] Epoch: 14/40, Batch: 8/25, Combined Loss: 0.184754 (Summen av L1 og STFT), L1 Loss: 2.332433 (Amplitudetap), STFT Loss: 4.543013 (Spektraltap)
2025-01-18 12:09:12,931 - INFO - [Batch] Epoch: 14/40, Batch: 9/25, Combined Loss: 0.066337 (Summen av L1 og STFT), L1 Loss: 0.692784 (Amplitudetap), STFT Loss: 1.693215 (Spektraltap)
2025-01-18 12:10:04,555 - INFO - [Batch] Epoch: 14/40, Batch: 10/25, Combined Loss: 0.055490 (Summen av L1 og STFT), L1 Loss: 0.570442 (Amplitudetap), STFT Loss: 1.420222 (Spektraltap)
2025-01-18 12:10:58,242 - INFO - [Batch] Epoch: 14/40, Batch: 11/25, Combined Loss: 0.128836 (Summen av L1 og STFT), L1 Loss: 1.825307 (Amplitudetap), STFT Loss: 3.082804 (Spektraltap)
2025-01-18 12:11:48,363 - INFO - [Batch] Epoch: 14/40, Batch: 12/25, Combined Loss: 0.100412 (Summen av L1 og STFT), L1 Loss: 1.190310 (Amplitudetap), STFT Loss: 2.502233 (Spektraltap)
2025-01-18 12:12:36,890 - INFO - [Batch] Epoch: 14/40, Batch: 13/25, Combined Loss: 0.114721 (Summen av L1 og STFT), L1 Loss: 1.165840 (Amplitudetap), STFT Loss: 2.941982 (Spektraltap)
2025-01-18 12:13:27,679 - INFO - [Batch] Epoch: 14/40, Batch: 14/25, Combined Loss: 0.063528 (Summen av L1 og STFT), L1 Loss: 0.753753 (Amplitudetap), STFT Loss: 1.582788 (Spektraltap)
2025-01-18 12:14:25,207 - INFO - [Batch] Epoch: 14/40, Batch: 15/25, Combined Loss: 0.058446 (Summen av L1 og STFT), L1 Loss: 0.828669 (Amplitudetap), STFT Loss: 1.398240 (Spektraltap)
2025-01-18 12:15:16,464 - INFO - [Batch] Epoch: 14/40, Batch: 16/25, Combined Loss: 0.030574 (Summen av L1 og STFT), L1 Loss: 0.297691 (Amplitudetap), STFT Loss: 0.789640 (Spektraltap)
2025-01-18 12:16:05,921 - INFO - [Batch] Epoch: 14/40, Batch: 17/25, Combined Loss: 0.019040 (Summen av L1 og STFT), L1 Loss: 0.110606 (Amplitudetap), STFT Loss: 0.523809 (Spektraltap)
2025-01-18 12:16:58,659 - INFO - [Batch] Epoch: 14/40, Batch: 18/25, Combined Loss: 0.119498 (Summen av L1 og STFT), L1 Loss: 1.605415 (Amplitudetap), STFT Loss: 2.896898 (Spektraltap)
2025-01-18 12:17:52,263 - INFO - [Batch] Epoch: 14/40, Batch: 19/25, Combined Loss: 0.062758 (Summen av L1 og STFT), L1 Loss: 0.587628 (Amplitudetap), STFT Loss: 1.630901 (Spektraltap)
2025-01-18 12:18:42,494 - INFO - [Batch] Epoch: 14/40, Batch: 20/25, Combined Loss: 0.025173 (Summen av L1 og STFT), L1 Loss: 0.232550 (Amplitudetap), STFT Loss: 0.655529 (Spektraltap)
2025-01-18 12:19:44,855 - INFO - [Batch] Epoch: 14/40, Batch: 21/25, Combined Loss: 0.129965 (Summen av L1 og STFT), L1 Loss: 1.515001 (Amplitudetap), STFT Loss: 3.249675 (Spektraltap)
2025-01-18 12:19:44,871 - INFO - Epoch [14/40], Batch [21/25], Gradient Norm: 0.0640
2025-01-18 12:20:34,871 - INFO - [Batch] Epoch: 14/40, Batch: 22/25, Combined Loss: 0.119932 (Summen av L1 og STFT), L1 Loss: 1.325604 (Amplitudetap), STFT Loss: 3.029830 (Spektraltap)
2025-01-18 12:21:09,981 - INFO - [Batch] Epoch: 14/40, Batch: 23/25, Combined Loss: 0.020192 (Summen av L1 og STFT), L1 Loss: 0.126826 (Amplitudetap), STFT Loss: 0.551410 (Spektraltap)
2025-01-18 12:21:58,508 - INFO - [Batch] Epoch: 14/40, Batch: 24/25, Combined Loss: 0.040894 (Summen av L1 og STFT), L1 Loss: 0.381066 (Amplitudetap), STFT Loss: 1.063514 (Spektraltap)
2025-01-18 12:22:51,290 - INFO - [Batch] Epoch: 14/40, Batch: 25/25, Combined Loss: 0.185170 (Summen av L1 og STFT), L1 Loss: 2.388641 (Amplitudetap), STFT Loss: 4.531384 (Spektraltap)
2025-01-18 12:22:51,290 - INFO - Epoch [14/40], Batch [25/25], Gradient Norm: 0.0659
2025-01-18 12:22:51,306 - INFO - Current Learning Rate 1.25e-08
2025-01-18 12:22:51,306 - INFO - [Epoch Improvement] Epoch 14: Loss improved by 0.00% from previous epoch.
2025-01-18 12:22:51,306 - INFO - [Epoch Summary] Avg Training Loss: 0.084289
2025-01-18 12:22:51,306 - INFO - [Train] No improvement in loss for epoch 14 with loss: 0.08428918175399304. Best loss remains 0.084285. Trigger_times: 0
2025-01-18 12:31:36,368 - INFO - [Validation] Epoch 14/40, [Validation] Avg Validation Loss:  1.078955
2025-01-18 12:31:36,368 - INFO - [Epoch Summary] Epoch: 14/40, Avg Combined Loss: 0.084289, Avg L1 Loss: 0.980720, Avg STFT Loss: 2.108846
2025-01-18 12:31:36,368 - INFO - Previous epoch loss: 0.08428918175399304
2025-01-18 12:31:36,368 - INFO - [Train] Epoch 15/40 started.
2025-01-18 12:32:17,696 - INFO - [Batch] Epoch: 15/40, Batch: 1/25, Combined Loss: 0.083471 (Summen av L1 og STFT), L1 Loss: 0.914799 (Amplitudetap), STFT Loss: 2.112078 (Spektraltap)
2025-01-18 12:33:10,579 - INFO - [Batch] Epoch: 15/40, Batch: 2/25, Combined Loss: 0.093105 (Summen av L1 og STFT), L1 Loss: 0.982909 (Amplitudetap), STFT Loss: 2.371895 (Spektraltap)
2025-01-18 12:33:49,329 - INFO - [Batch] Epoch: 15/40, Batch: 3/25, Combined Loss: 0.090680 (Summen av L1 og STFT), L1 Loss: 1.111536 (Amplitudetap), STFT Loss: 2.244020 (Spektraltap)
2025-01-18 12:34:33,009 - INFO - [Batch] Epoch: 15/40, Batch: 4/25, Combined Loss: 0.063250 (Summen av L1 og STFT), L1 Loss: 0.668619 (Amplitudetap), STFT Loss: 1.610954 (Spektraltap)
2025-01-18 12:35:22,802 - INFO - [Batch] Epoch: 15/40, Batch: 5/25, Combined Loss: 0.145257 (Summen av L1 og STFT), L1 Loss: 1.723243 (Amplitudetap), STFT Loss: 3.619174 (Spektraltap)
2025-01-18 12:36:04,220 - INFO - [Batch] Epoch: 15/40, Batch: 6/25, Combined Loss: 0.058623 (Summen av L1 og STFT), L1 Loss: 0.686639 (Amplitudetap), STFT Loss: 1.464410 (Spektraltap)
2025-01-18 12:37:02,828 - INFO - [Batch] Epoch: 15/40, Batch: 7/25, Combined Loss: 0.078234 (Summen av L1 og STFT), L1 Loss: 0.883582 (Amplitudetap), STFT Loss: 1.968332 (Spektraltap)
2025-01-18 12:37:52,699 - INFO - [Batch] Epoch: 15/40, Batch: 8/25, Combined Loss: 0.066100 (Summen av L1 og STFT), L1 Loss: 0.752156 (Amplitudetap), STFT Loss: 1.660635 (Spektraltap)
2025-01-18 12:38:49,053 - INFO - [Batch] Epoch: 15/40, Batch: 9/25, Combined Loss: 0.122571 (Summen av L1 og STFT), L1 Loss: 1.473653 (Amplitudetap), STFT Loss: 3.045576 (Spektraltap)
2025-01-18 12:39:43,193 - INFO - [Batch] Epoch: 15/40, Batch: 10/25, Combined Loss: 0.069782 (Summen av L1 og STFT), L1 Loss: 0.776846 (Amplitudetap), STFT Loss: 1.760532 (Spektraltap)
2025-01-18 12:40:25,184 - INFO - [Batch] Epoch: 15/40, Batch: 11/25, Combined Loss: 0.096679 (Summen av L1 og STFT), L1 Loss: 1.122725 (Amplitudetap), STFT Loss: 2.419198 (Spektraltap)
2025-01-18 12:41:20,019 - INFO - [Batch] Epoch: 15/40, Batch: 12/25, Combined Loss: 0.043609 (Summen av L1 og STFT), L1 Loss: 0.274230 (Amplitudetap), STFT Loss: 1.190757 (Spektraltap)
2025-01-18 12:42:10,329 - INFO - [Batch] Epoch: 15/40, Batch: 13/25, Combined Loss: 0.077342 (Summen av L1 og STFT), L1 Loss: 0.853636 (Amplitudetap), STFT Loss: 1.954417 (Spektraltap)
2025-01-18 12:42:51,026 - INFO - [Batch] Epoch: 15/40, Batch: 14/25, Combined Loss: 0.126679 (Summen av L1 og STFT), L1 Loss: 1.443638 (Amplitudetap), STFT Loss: 3.181662 (Spektraltap)
2025-01-18 12:43:44,362 - INFO - [Batch] Epoch: 15/40, Batch: 15/25, Combined Loss: 0.151385 (Summen av L1 og STFT), L1 Loss: 2.029330 (Amplitudetap), STFT Loss: 3.671841 (Spektraltap)
2025-01-18 12:44:34,578 - INFO - [Batch] Epoch: 15/40, Batch: 16/25, Combined Loss: 0.040626 (Summen av L1 og STFT), L1 Loss: 0.556966 (Amplitudetap), STFT Loss: 0.980081 (Spektraltap)
2025-01-18 12:45:29,235 - INFO - [Batch] Epoch: 15/40, Batch: 17/25, Combined Loss: 0.015436 (Summen av L1 og STFT), L1 Loss: 0.309150 (Amplitudetap), STFT Loss: 0.330579 (Spektraltap)
2025-01-18 12:46:17,664 - INFO - [Batch] Epoch: 15/40, Batch: 18/25, Combined Loss: 0.031578 (Summen av L1 og STFT), L1 Loss: 0.282384 (Amplitudetap), STFT Loss: 0.826322 (Spektraltap)
2025-01-18 12:47:00,079 - INFO - [Batch] Epoch: 15/40, Batch: 19/25, Combined Loss: 0.038579 (Summen av L1 og STFT), L1 Loss: 0.378728 (Amplitudetap), STFT Loss: 0.995065 (Spektraltap)
2025-01-18 12:47:56,382 - INFO - [Batch] Epoch: 15/40, Batch: 20/25, Combined Loss: 0.021294 (Summen av L1 og STFT), L1 Loss: 0.189578 (Amplitudetap), STFT Loss: 0.557576 (Spektraltap)
2025-01-18 12:48:46,744 - INFO - [Batch] Epoch: 15/40, Batch: 21/25, Combined Loss: 0.088178 (Summen av L1 og STFT), L1 Loss: 1.046266 (Amplitudetap), STFT Loss: 2.196927 (Spektraltap)
2025-01-18 12:48:46,744 - INFO - Epoch [15/40], Batch [21/25], Gradient Norm: 0.0455
2025-01-18 12:49:38,085 - INFO - [Batch] Epoch: 15/40, Batch: 22/25, Combined Loss: 0.172151 (Summen av L1 og STFT), L1 Loss: 2.152268 (Amplitudetap), STFT Loss: 4.242131 (Spektraltap)
2025-01-18 12:50:40,822 - INFO - [Batch] Epoch: 15/40, Batch: 23/25, Combined Loss: 0.071886 (Summen av L1 og STFT), L1 Loss: 0.774699 (Amplitudetap), STFT Loss: 1.824569 (Spektraltap)
2025-01-18 12:51:39,493 - INFO - [Batch] Epoch: 15/40, Batch: 24/25, Combined Loss: 0.176714 (Summen av L1 og STFT), L1 Loss: 2.135981 (Amplitudetap), STFT Loss: 4.386014 (Spektraltap)
2025-01-18 12:52:39,545 - INFO - [Batch] Epoch: 15/40, Batch: 25/25, Combined Loss: 0.084021 (Summen av L1 og STFT), L1 Loss: 0.993654 (Amplitudetap), STFT Loss: 2.094769 (Spektraltap)
2025-01-18 12:52:39,561 - INFO - Epoch [15/40], Batch [25/25], Gradient Norm: 0.0573
2025-01-18 12:52:39,592 - INFO - Current Learning Rate 1.25e-08
2025-01-18 12:52:39,592 - INFO - [Epoch Improvement] Epoch 15: Loss improved by 0.00% from previous epoch.
2025-01-18 12:52:39,592 - INFO - [Epoch Summary] Avg Training Loss: 0.084289
2025-01-18 12:52:39,592 - INFO - [Train] No improvement in loss for epoch 15 with loss: 0.08428918924182653. Best loss remains 0.084285. Trigger_times: 1
2025-01-18 13:01:01,482 - INFO - [Validation] Epoch 15/40, [Validation] Avg Validation Loss:  1.078944
2025-01-18 13:01:01,482 - INFO - [Memory After Epoch 15] Timestamp: 2025-01-18 13:01:01
2025-01-18 13:01:01,482 - INFO - [Memory After Epoch 15] GPU 0: NVIDIA GeForce RTX 3060, Total=12.88 GB, Allocated=0.51 GB, Cached=23.26 GB
2025-01-18 13:01:01,482 - INFO - [Memory After Epoch 15] CPU Memory Usage: ~0.57 GB
2025-01-18 13:01:01,482 - INFO - [Epoch Summary] Epoch: 15/40, Avg Combined Loss: 0.084289, Avg L1 Loss: 0.980718, Avg STFT Loss: 2.108815
2025-01-18 13:01:01,482 - INFO - Previous epoch loss: 0.08428918924182653
2025-01-18 13:01:01,482 - INFO - [Train] Epoch 16/40 started.
2025-01-18 13:01:54,824 - INFO - [Batch] Epoch: 16/40, Batch: 1/25, Combined Loss: 0.147575 (Summen av L1 og STFT), L1 Loss: 1.744310 (Amplitudetap), STFT Loss: 3.679685 (Spektraltap)
2025-01-18 13:02:53,112 - INFO - [Batch] Epoch: 16/40, Batch: 2/25, Combined Loss: 0.124681 (Summen av L1 og STFT), L1 Loss: 1.409500 (Amplitudetap), STFT Loss: 3.136354 (Spektraltap)
2025-01-18 13:03:38,956 - INFO - [Batch] Epoch: 16/40, Batch: 3/25, Combined Loss: 0.171374 (Summen av L1 og STFT), L1 Loss: 2.053511 (Amplitudetap), STFT Loss: 4.261130 (Spektraltap)
2025-01-18 13:04:34,335 - INFO - [Batch] Epoch: 16/40, Batch: 4/25, Combined Loss: 0.028034 (Summen av L1 og STFT), L1 Loss: 0.239049 (Amplitudetap), STFT Loss: 0.738564 (Spektraltap)
2025-01-18 13:05:30,470 - INFO - [Batch] Epoch: 16/40, Batch: 5/25, Combined Loss: 0.042995 (Summen av L1 og STFT), L1 Loss: 0.388469 (Amplitudetap), STFT Loss: 1.123372 (Spektraltap)
2025-01-18 13:06:25,595 - INFO - [Batch] Epoch: 16/40, Batch: 6/25, Combined Loss: 0.063355 (Summen av L1 og STFT), L1 Loss: 0.713276 (Amplitudetap), STFT Loss: 1.594972 (Spektraltap)
2025-01-18 13:07:09,078 - INFO - [Batch] Epoch: 16/40, Batch: 7/25, Combined Loss: 0.077319 (Summen av L1 og STFT), L1 Loss: 0.902696 (Amplitudetap), STFT Loss: 1.932713 (Spektraltap)
2025-01-18 13:08:08,529 - INFO - [Batch] Epoch: 16/40, Batch: 8/25, Combined Loss: 0.043883 (Summen av L1 og STFT), L1 Loss: 0.634379 (Amplitudetap), STFT Loss: 1.044608 (Spektraltap)
2025-01-18 13:08:50,116 - INFO - [Batch] Epoch: 16/40, Batch: 9/25, Combined Loss: 0.038655 (Summen av L1 og STFT), L1 Loss: 0.246729 (Amplitudetap), STFT Loss: 1.053901 (Spektraltap)
2025-01-18 13:09:45,131 - INFO - [Batch] Epoch: 16/40, Batch: 10/25, Combined Loss: 0.026834 (Summen av L1 og STFT), L1 Loss: 0.227511 (Amplitudetap), STFT Loss: 0.707518 (Spektraltap)
2025-01-18 13:10:36,510 - INFO - [Batch] Epoch: 16/40, Batch: 11/25, Combined Loss: 0.083736 (Summen av L1 og STFT), L1 Loss: 0.827069 (Amplitudetap), STFT Loss: 2.157630 (Spektraltap)
2025-01-18 13:11:19,432 - INFO - [Batch] Epoch: 16/40, Batch: 12/25, Combined Loss: 0.097512 (Summen av L1 og STFT), L1 Loss: 1.034130 (Amplitudetap), STFT Loss: 2.482162 (Spektraltap)
2025-01-18 13:12:08,818 - INFO - [Batch] Epoch: 16/40, Batch: 13/25, Combined Loss: 0.085054 (Summen av L1 og STFT), L1 Loss: 0.956500 (Amplitudetap), STFT Loss: 2.141693 (Spektraltap)
2025-01-18 13:13:07,926 - INFO - [Batch] Epoch: 16/40, Batch: 14/25, Combined Loss: 0.211221 (Summen av L1 og STFT), L1 Loss: 2.863928 (Amplitudetap), STFT Loss: 5.109223 (Spektraltap)
2025-01-18 13:13:53,921 - INFO - [Batch] Epoch: 16/40, Batch: 15/25, Combined Loss: 0.073106 (Summen av L1 og STFT), L1 Loss: 0.916344 (Amplitudetap), STFT Loss: 1.800451 (Spektraltap)
2025-01-18 13:14:50,793 - INFO - [Batch] Epoch: 16/40, Batch: 16/25, Combined Loss: 0.064470 (Summen av L1 og STFT), L1 Loss: 0.706697 (Amplitudetap), STFT Loss: 1.631219 (Spektraltap)
2025-01-18 13:15:39,969 - INFO - [Batch] Epoch: 16/40, Batch: 17/25, Combined Loss: 0.105801 (Summen av L1 og STFT), L1 Loss: 1.185413 (Amplitudetap), STFT Loss: 2.665988 (Spektraltap)
2025-01-18 13:16:28,493 - INFO - [Batch] Epoch: 16/40, Batch: 18/25, Combined Loss: 0.036004 (Summen av L1 og STFT), L1 Loss: 0.410321 (Amplitudetap), STFT Loss: 0.904267 (Spektraltap)
2025-01-18 13:17:24,861 - INFO - [Batch] Epoch: 16/40, Batch: 19/25, Combined Loss: 0.082724 (Summen av L1 og STFT), L1 Loss: 1.162682 (Amplitudetap), STFT Loss: 1.983432 (Spektraltap)
2025-01-18 13:18:15,815 - INFO - [Batch] Epoch: 16/40, Batch: 20/25, Combined Loss: 0.083739 (Summen av L1 og STFT), L1 Loss: 0.947053 (Amplitudetap), STFT Loss: 2.106302 (Spektraltap)
2025-01-18 13:19:08,294 - INFO - [Batch] Epoch: 16/40, Batch: 21/25, Combined Loss: 0.024777 (Summen av L1 og STFT), L1 Loss: 0.230626 (Amplitudetap), STFT Loss: 0.644459 (Spektraltap)
2025-01-18 13:19:08,294 - INFO - Epoch [16/40], Batch [21/25], Gradient Norm: 0.0321
2025-01-18 13:19:48,835 - INFO - [Batch] Epoch: 16/40, Batch: 22/25, Combined Loss: 0.038200 (Summen av L1 og STFT), L1 Loss: 0.565965 (Amplitudetap), STFT Loss: 0.903434 (Spektraltap)
2025-01-18 13:20:33,978 - INFO - [Batch] Epoch: 16/40, Batch: 23/25, Combined Loss: 0.171133 (Summen av L1 og STFT), L1 Loss: 1.960899 (Amplitudetap), STFT Loss: 4.293603 (Spektraltap)
2025-01-18 13:21:17,872 - INFO - [Batch] Epoch: 16/40, Batch: 24/25, Combined Loss: 0.110985 (Summen av L1 og STFT), L1 Loss: 1.328140 (Amplitudetap), STFT Loss: 2.760342 (Spektraltap)
2025-01-18 13:22:00,460 - INFO - [Batch] Epoch: 16/40, Batch: 25/25, Combined Loss: 0.073741 (Summen av L1 og STFT), L1 Loss: 0.856187 (Amplitudetap), STFT Loss: 1.845305 (Spektraltap)
2025-01-18 13:22:00,460 - INFO - Epoch [16/40], Batch [25/25], Gradient Norm: 0.0324
2025-01-18 13:22:00,476 - INFO - Current Learning Rate 1.25e-08
2025-01-18 13:22:00,476 - INFO - [Epoch Improvement] Epoch 16: Loss improved by 0.00% from previous epoch.
2025-01-18 13:22:00,476 - INFO - [Epoch Summary] Avg Training Loss: 0.084276
2025-01-18 13:22:00,570 - INFO - [Train] New best model saved at C:\Users\didri\Desktop\UNet Models\UNet_vocal_isolation_model\Model_weights\CheckPoints\best_model_epoch-15.pth with loss 0.084276
2025-01-18 13:30:47,401 - INFO - [Validation] Epoch 16/40, [Validation] Avg Validation Loss:  1.078937
2025-01-18 13:30:47,401 - INFO - [Epoch Summary] Epoch: 16/40, Avg Combined Loss: 0.084276, Avg L1 Loss: 0.980702, Avg STFT Loss: 2.108770
2025-01-18 13:30:47,401 - INFO - Previous epoch loss: 0.08427627615630627
2025-01-18 13:30:47,401 - INFO - [Train] Epoch 17/40 started.
2025-01-18 13:31:43,321 - INFO - [Batch] Epoch: 17/40, Batch: 1/25, Combined Loss: 0.174654 (Summen av L1 og STFT), L1 Loss: 2.247575 (Amplitudetap), STFT Loss: 4.276368 (Spektraltap)
2025-01-18 13:32:35,970 - INFO - [Batch] Epoch: 17/40, Batch: 2/25, Combined Loss: 0.129211 (Summen av L1 og STFT), L1 Loss: 1.783886 (Amplitudetap), STFT Loss: 3.111816 (Spektraltap)
2025-01-18 13:33:33,418 - INFO - [Batch] Epoch: 17/40, Batch: 3/25, Combined Loss: 0.073689 (Summen av L1 og STFT), L1 Loss: 1.056285 (Amplitudetap), STFT Loss: 1.757984 (Spektraltap)
2025-01-18 13:34:11,243 - INFO - [Batch] Epoch: 17/40, Batch: 4/25, Combined Loss: 0.026996 (Summen av L1 og STFT), L1 Loss: 0.214701 (Amplitudetap), STFT Loss: 0.717860 (Spektraltap)
2025-01-18 13:35:02,769 - INFO - [Batch] Epoch: 17/40, Batch: 5/25, Combined Loss: 0.059103 (Summen av L1 og STFT), L1 Loss: 0.564588 (Amplitudetap), STFT Loss: 1.531132 (Spektraltap)
2025-01-18 13:35:41,956 - INFO - [Batch] Epoch: 17/40, Batch: 6/25, Combined Loss: 0.091282 (Summen av L1 og STFT), L1 Loss: 1.267764 (Amplitudetap), STFT Loss: 2.195127 (Spektraltap)
2025-01-18 13:36:21,401 - INFO - [Batch] Epoch: 17/40, Batch: 7/25, Combined Loss: 0.035418 (Summen av L1 og STFT), L1 Loss: 0.466944 (Amplitudetap), STFT Loss: 0.862415 (Spektraltap)
2025-01-18 13:37:13,752 - INFO - [Batch] Epoch: 17/40, Batch: 8/25, Combined Loss: 0.102466 (Summen av L1 og STFT), L1 Loss: 1.252407 (Amplitudetap), STFT Loss: 2.537221 (Spektraltap)
2025-01-18 13:38:14,428 - INFO - [Batch] Epoch: 17/40, Batch: 9/25, Combined Loss: 0.021831 (Summen av L1 og STFT), L1 Loss: 0.129839 (Amplitudetap), STFT Loss: 0.599298 (Spektraltap)
2025-01-18 13:39:11,907 - INFO - [Batch] Epoch: 17/40, Batch: 10/25, Combined Loss: 0.072954 (Summen av L1 og STFT), L1 Loss: 0.827677 (Amplitudetap), STFT Loss: 1.833906 (Spektraltap)
2025-01-18 13:40:15,023 - INFO - [Batch] Epoch: 17/40, Batch: 11/25, Combined Loss: 0.075566 (Summen av L1 og STFT), L1 Loss: 0.814715 (Amplitudetap), STFT Loss: 1.917812 (Spektraltap)
2025-01-18 13:40:59,773 - INFO - [Batch] Epoch: 17/40, Batch: 12/25, Combined Loss: 0.035362 (Summen av L1 og STFT), L1 Loss: 0.310600 (Amplitudetap), STFT Loss: 0.927745 (Spektraltap)
2025-01-18 13:41:44,301 - INFO - [Batch] Epoch: 17/40, Batch: 13/25, Combined Loss: 0.140807 (Summen av L1 og STFT), L1 Loss: 1.652023 (Amplitudetap), STFT Loss: 3.516200 (Spektraltap)
2025-01-18 13:42:45,209 - INFO - [Batch] Epoch: 17/40, Batch: 14/25, Combined Loss: 0.045377 (Summen av L1 og STFT), L1 Loss: 0.433347 (Amplitudetap), STFT Loss: 1.175580 (Spektraltap)
2025-01-18 13:43:40,023 - INFO - [Batch] Epoch: 17/40, Batch: 15/25, Combined Loss: 0.038994 (Summen av L1 og STFT), L1 Loss: 0.382851 (Amplitudetap), STFT Loss: 1.005750 (Spektraltap)
2025-01-18 13:44:34,605 - INFO - [Batch] Epoch: 17/40, Batch: 16/25, Combined Loss: 0.096185 (Summen av L1 og STFT), L1 Loss: 1.013637 (Amplitudetap), STFT Loss: 2.451132 (Spektraltap)
2025-01-18 13:45:18,088 - INFO - [Batch] Epoch: 17/40, Batch: 17/25, Combined Loss: 0.127203 (Summen av L1 og STFT), L1 Loss: 1.521153 (Amplitudetap), STFT Loss: 3.164153 (Spektraltap)
2025-01-18 13:46:16,639 - INFO - [Batch] Epoch: 17/40, Batch: 18/25, Combined Loss: 0.089939 (Summen av L1 og STFT), L1 Loss: 0.981612 (Amplitudetap), STFT Loss: 2.277483 (Spektraltap)
2025-01-18 13:47:02,396 - INFO - [Batch] Epoch: 17/40, Batch: 19/25, Combined Loss: 0.167483 (Summen av L1 og STFT), L1 Loss: 1.978731 (Amplitudetap), STFT Loss: 4.176459 (Spektraltap)
2025-01-18 13:48:03,567 - INFO - [Batch] Epoch: 17/40, Batch: 20/25, Combined Loss: 0.101196 (Summen av L1 og STFT), L1 Loss: 1.202571 (Amplitudetap), STFT Loss: 2.520502 (Spektraltap)
2025-01-18 13:48:49,208 - INFO - [Batch] Epoch: 17/40, Batch: 21/25, Combined Loss: 0.059199 (Summen av L1 og STFT), L1 Loss: 0.609321 (Amplitudetap), STFT Loss: 1.514822 (Spektraltap)
2025-01-18 13:48:49,208 - INFO - Epoch [17/40], Batch [21/25], Gradient Norm: 0.0765
2025-01-18 13:49:24,770 - INFO - [Batch] Epoch: 17/40, Batch: 22/25, Combined Loss: 0.066432 (Summen av L1 og STFT), L1 Loss: 0.781508 (Amplitudetap), STFT Loss: 1.658013 (Spektraltap)
2025-01-18 13:50:20,313 - INFO - [Batch] Epoch: 17/40, Batch: 23/25, Combined Loss: 0.095443 (Summen av L1 og STFT), L1 Loss: 1.042698 (Amplitudetap), STFT Loss: 2.416417 (Spektraltap)
2025-01-18 13:51:00,597 - INFO - [Batch] Epoch: 17/40, Batch: 24/25, Combined Loss: 0.157902 (Summen av L1 og STFT), L1 Loss: 1.756608 (Amplitudetap), STFT Loss: 3.984233 (Spektraltap)
2025-01-18 13:51:49,676 - INFO - [Batch] Epoch: 17/40, Batch: 25/25, Combined Loss: 0.022566 (Summen av L1 og STFT), L1 Loss: 0.226528 (Amplitudetap), STFT Loss: 0.579909 (Spektraltap)
2025-01-18 13:51:49,692 - INFO - Epoch [17/40], Batch [25/25], Gradient Norm: 0.0375
2025-01-18 13:51:49,692 - INFO - Current Learning Rate 1.25e-08
2025-01-18 13:51:49,692 - INFO - [Epoch Improvement] Epoch 17: Loss improved by 0.00% from previous epoch.
2025-01-18 13:51:49,692 - INFO - [Epoch Summary] Avg Training Loss: 0.084290
2025-01-18 13:51:49,692 - INFO - [Train] No improvement in loss for epoch 17 with loss: 0.08429030001163483. Best loss remains 0.084276. Trigger_times: 0
2025-01-18 14:00:29,818 - INFO - [Validation] Epoch 17/40, [Validation] Avg Validation Loss:  1.078934
2025-01-18 14:00:29,818 - INFO - [Epoch Summary] Epoch: 17/40, Avg Combined Loss: 0.084290, Avg L1 Loss: 0.980706, Avg STFT Loss: 2.108747
2025-01-18 14:00:29,834 - INFO - Previous epoch loss: 0.08429030001163483
2025-01-18 14:00:29,834 - INFO - [Train] Epoch 18/40 started.
2025-01-18 14:01:11,596 - INFO - [Batch] Epoch: 18/40, Batch: 1/25, Combined Loss: 0.182078 (Summen av L1 og STFT), L1 Loss: 2.306480 (Amplitudetap), STFT Loss: 4.473854 (Spektraltap)
2025-01-18 14:02:13,615 - INFO - [Batch] Epoch: 18/40, Batch: 2/25, Combined Loss: 0.128613 (Summen av L1 og STFT), L1 Loss: 1.554325 (Amplitudetap), STFT Loss: 3.192266 (Spektraltap)
2025-01-18 14:03:05,129 - INFO - [Batch] Epoch: 18/40, Batch: 3/25, Combined Loss: 0.027900 (Summen av L1 og STFT), L1 Loss: 0.234662 (Amplitudetap), STFT Loss: 0.736439 (Spektraltap)
2025-01-18 14:03:56,345 - INFO - [Batch] Epoch: 18/40, Batch: 4/25, Combined Loss: 0.073441 (Summen av L1 og STFT), L1 Loss: 0.837299 (Amplitudetap), STFT Loss: 1.844396 (Spektraltap)
2025-01-18 14:04:46,957 - INFO - [Batch] Epoch: 18/40, Batch: 5/25, Combined Loss: 0.051777 (Summen av L1 og STFT), L1 Loss: 0.475125 (Amplitudetap), STFT Loss: 1.349698 (Spektraltap)
2025-01-18 14:05:45,037 - INFO - [Batch] Epoch: 18/40, Batch: 6/25, Combined Loss: 0.067384 (Summen av L1 og STFT), L1 Loss: 0.743708 (Amplitudetap), STFT Loss: 1.702786 (Spektraltap)
2025-01-18 14:06:32,479 - INFO - [Batch] Epoch: 18/40, Batch: 7/25, Combined Loss: 0.076545 (Summen av L1 og STFT), L1 Loss: 1.289476 (Amplitudetap), STFT Loss: 1.743730 (Spektraltap)
2025-01-18 14:07:24,384 - INFO - [Batch] Epoch: 18/40, Batch: 8/25, Combined Loss: 0.010882 (Summen av L1 og STFT), L1 Loss: 0.112353 (Amplitudetap), STFT Loss: 0.278313 (Spektraltap)
2025-01-18 14:08:15,688 - INFO - [Batch] Epoch: 18/40, Batch: 9/25, Combined Loss: 0.078516 (Summen av L1 og STFT), L1 Loss: 0.933909 (Amplitudetap), STFT Loss: 1.955243 (Spektraltap)
2025-01-18 14:09:08,989 - INFO - [Batch] Epoch: 18/40, Batch: 10/25, Combined Loss: 0.035277 (Summen av L1 og STFT), L1 Loss: 0.344160 (Amplitudetap), STFT Loss: 0.910807 (Spektraltap)
2025-01-18 14:09:53,275 - INFO - [Batch] Epoch: 18/40, Batch: 11/25, Combined Loss: 0.171850 (Summen av L1 og STFT), L1 Loss: 2.205355 (Amplitudetap), STFT Loss: 4.210361 (Spektraltap)
2025-01-18 14:10:43,794 - INFO - [Batch] Epoch: 18/40, Batch: 12/25, Combined Loss: 0.081626 (Summen av L1 og STFT), L1 Loss: 0.845956 (Amplitudetap), STFT Loss: 2.086225 (Spektraltap)
2025-01-18 14:11:22,083 - INFO - [Batch] Epoch: 18/40, Batch: 13/25, Combined Loss: 0.063966 (Summen av L1 og STFT), L1 Loss: 0.603734 (Amplitudetap), STFT Loss: 1.660229 (Spektraltap)
2025-01-18 14:12:06,334 - INFO - [Batch] Epoch: 18/40, Batch: 14/25, Combined Loss: 0.042107 (Summen av L1 og STFT), L1 Loss: 0.308612 (Amplitudetap), STFT Loss: 1.130945 (Spektraltap)
2025-01-18 14:12:55,427 - INFO - [Batch] Epoch: 18/40, Batch: 15/25, Combined Loss: 0.092298 (Summen av L1 og STFT), L1 Loss: 1.054573 (Amplitudetap), STFT Loss: 2.316979 (Spektraltap)
2025-01-18 14:13:39,102 - INFO - [Batch] Epoch: 18/40, Batch: 16/25, Combined Loss: 0.078022 (Summen av L1 og STFT), L1 Loss: 0.897184 (Amplitudetap), STFT Loss: 1.956166 (Spektraltap)
2025-01-18 14:14:28,415 - INFO - [Batch] Epoch: 18/40, Batch: 17/25, Combined Loss: 0.054750 (Summen av L1 og STFT), L1 Loss: 0.542190 (Amplitudetap), STFT Loss: 1.410121 (Spektraltap)
2025-01-18 14:15:18,929 - INFO - [Batch] Epoch: 18/40, Batch: 18/25, Combined Loss: 0.178433 (Summen av L1 og STFT), L1 Loss: 2.073284 (Amplitudetap), STFT Loss: 4.464448 (Spektraltap)
2025-01-18 14:16:10,352 - INFO - [Batch] Epoch: 18/40, Batch: 19/25, Combined Loss: 0.037345 (Summen av L1 og STFT), L1 Loss: 0.356801 (Amplitudetap), STFT Loss: 0.967431 (Spektraltap)
2025-01-18 14:17:05,197 - INFO - [Batch] Epoch: 18/40, Batch: 20/25, Combined Loss: 0.022442 (Summen av L1 og STFT), L1 Loss: 0.167481 (Amplitudetap), STFT Loss: 0.601484 (Spektraltap)
2025-01-18 14:18:00,421 - INFO - [Batch] Epoch: 18/40, Batch: 21/25, Combined Loss: 0.130200 (Summen av L1 og STFT), L1 Loss: 1.552631 (Amplitudetap), STFT Loss: 3.240594 (Spektraltap)
2025-01-18 14:18:00,437 - INFO - Epoch [18/40], Batch [21/25], Gradient Norm: 0.0909
2025-01-18 14:18:55,057 - INFO - [Batch] Epoch: 18/40, Batch: 22/25, Combined Loss: 0.024661 (Summen av L1 og STFT), L1 Loss: 0.203228 (Amplitudetap), STFT Loss: 0.652735 (Spektraltap)
2025-01-18 14:19:42,277 - INFO - [Batch] Epoch: 18/40, Batch: 23/25, Combined Loss: 0.166189 (Summen av L1 og STFT), L1 Loss: 2.194798 (Amplitudetap), STFT Loss: 4.045043 (Spektraltap)
2025-01-18 14:20:40,540 - INFO - [Batch] Epoch: 18/40, Batch: 24/25, Combined Loss: 0.036430 (Summen av L1 og STFT), L1 Loss: 0.283405 (Amplitudetap), STFT Loss: 0.971426 (Spektraltap)
2025-01-18 14:21:36,306 - INFO - [Batch] Epoch: 18/40, Batch: 25/25, Combined Loss: 0.194161 (Summen av L1 og STFT), L1 Loss: 2.391464 (Amplitudetap), STFT Loss: 4.799924 (Spektraltap)
2025-01-18 14:21:36,306 - INFO - Epoch [18/40], Batch [25/25], Gradient Norm: 0.0605
2025-01-18 14:21:36,322 - INFO - Current Learning Rate 1.25e-08
2025-01-18 14:21:36,322 - INFO - [Epoch Improvement] Epoch 18: Loss improved by 0.00% from previous epoch.
2025-01-18 14:21:36,322 - INFO - [Epoch Summary] Avg Training Loss: 0.084276
2025-01-18 14:21:36,431 - INFO - [Train] New best model saved at C:\Users\didri\Desktop\UNet Models\UNet_vocal_isolation_model\Model_weights\CheckPoints\best_model_epoch-17.pth with loss 0.084276
2025-01-18 14:30:21,907 - INFO - [Validation] Epoch 18/40, [Validation] Avg Validation Loss:  1.078930
2025-01-18 14:30:21,907 - INFO - [Epoch Summary] Epoch: 18/40, Avg Combined Loss: 0.084276, Avg L1 Loss: 0.980694, Avg STFT Loss: 2.108709
2025-01-18 14:30:21,907 - INFO - Previous epoch loss: 0.08427582480013371
2025-01-18 14:30:21,907 - INFO - [Train] Epoch 19/40 started.
2025-01-18 14:31:15,686 - INFO - [Batch] Epoch: 19/40, Batch: 1/25, Combined Loss: 0.095740 (Summen av L1 og STFT), L1 Loss: 1.086265 (Amplitudetap), STFT Loss: 2.406666 (Spektraltap)
2025-01-18 14:32:16,793 - INFO - [Batch] Epoch: 19/40, Batch: 2/25, Combined Loss: 0.176369 (Summen av L1 og STFT), L1 Loss: 2.094948 (Amplitudetap), STFT Loss: 4.393247 (Spektraltap)
2025-01-18 14:33:05,668 - INFO - [Batch] Epoch: 19/40, Batch: 3/25, Combined Loss: 0.106656 (Summen av L1 og STFT), L1 Loss: 1.183820 (Amplitudetap), STFT Loss: 2.692325 (Spektraltap)
2025-01-18 14:33:56,795 - INFO - [Batch] Epoch: 19/40, Batch: 4/25, Combined Loss: 0.031843 (Summen av L1 og STFT), L1 Loss: 0.257058 (Amplitudetap), STFT Loss: 0.845110 (Spektraltap)
2025-01-18 14:34:41,138 - INFO - [Batch] Epoch: 19/40, Batch: 5/25, Combined Loss: 0.070080 (Summen av L1 og STFT), L1 Loss: 0.791460 (Amplitudetap), STFT Loss: 1.763192 (Spektraltap)
2025-01-18 14:35:26,342 - INFO - [Batch] Epoch: 19/40, Batch: 6/25, Combined Loss: 0.034448 (Summen av L1 og STFT), L1 Loss: 0.355630 (Amplitudetap), STFT Loss: 0.881024 (Spektraltap)
2025-01-18 14:36:27,490 - INFO - [Batch] Epoch: 19/40, Batch: 7/25, Combined Loss: 0.082294 (Summen av L1 og STFT), L1 Loss: 0.996193 (Amplitudetap), STFT Loss: 2.041880 (Spektraltap)
2025-01-18 14:37:15,871 - INFO - [Batch] Epoch: 19/40, Batch: 8/25, Combined Loss: 0.115481 (Summen av L1 og STFT), L1 Loss: 1.350816 (Amplitudetap), STFT Loss: 2.885503 (Spektraltap)
2025-01-18 14:38:02,823 - INFO - [Batch] Epoch: 19/40, Batch: 9/25, Combined Loss: 0.023670 (Summen av L1 og STFT), L1 Loss: 0.350843 (Amplitudetap), STFT Loss: 0.559748 (Spektraltap)
2025-01-18 14:38:45,071 - INFO - [Batch] Epoch: 19/40, Batch: 10/25, Combined Loss: 0.030589 (Summen av L1 og STFT), L1 Loss: 0.291973 (Amplitudetap), STFT Loss: 0.792542 (Spektraltap)
2025-01-18 14:39:40,708 - INFO - [Batch] Epoch: 19/40, Batch: 11/25, Combined Loss: 0.043369 (Summen av L1 og STFT), L1 Loss: 0.417667 (Amplitudetap), STFT Loss: 1.122081 (Spektraltap)
2025-01-18 14:40:33,394 - INFO - [Batch] Epoch: 19/40, Batch: 12/25, Combined Loss: 0.096131 (Summen av L1 og STFT), L1 Loss: 1.060336 (Amplitudetap), STFT Loss: 2.429495 (Spektraltap)
2025-01-18 14:41:31,051 - INFO - [Batch] Epoch: 19/40, Batch: 13/25, Combined Loss: 0.024373 (Summen av L1 og STFT), L1 Loss: 0.225207 (Amplitudetap), STFT Loss: 0.634676 (Spektraltap)
2025-01-18 14:42:27,884 - INFO - [Batch] Epoch: 19/40, Batch: 14/25, Combined Loss: 0.067662 (Summen av L1 og STFT), L1 Loss: 0.832331 (Amplitudetap), STFT Loss: 1.673147 (Spektraltap)
2025-01-18 14:43:20,390 - INFO - [Batch] Epoch: 19/40, Batch: 15/25, Combined Loss: 0.131624 (Summen av L1 og STFT), L1 Loss: 1.555655 (Amplitudetap), STFT Loss: 3.282017 (Spektraltap)
2025-01-18 14:44:12,995 - INFO - [Batch] Epoch: 19/40, Batch: 16/25, Combined Loss: 0.034454 (Summen av L1 og STFT), L1 Loss: 0.237090 (Amplitudetap), STFT Loss: 0.931999 (Spektraltap)
2025-01-18 14:45:06,344 - INFO - [Batch] Epoch: 19/40, Batch: 17/25, Combined Loss: 0.081516 (Summen av L1 og STFT), L1 Loss: 0.905322 (Amplitudetap), STFT Loss: 2.057483 (Spektraltap)
2025-01-18 14:45:53,501 - INFO - [Batch] Epoch: 19/40, Batch: 18/25, Combined Loss: 0.029786 (Summen av L1 og STFT), L1 Loss: 0.270012 (Amplitudetap), STFT Loss: 0.777851 (Spektraltap)
2025-01-18 14:46:50,612 - INFO - [Batch] Epoch: 19/40, Batch: 19/25, Combined Loss: 0.136916 (Summen av L1 og STFT), L1 Loss: 1.666373 (Amplitudetap), STFT Loss: 3.393319 (Spektraltap)
2025-01-18 14:47:31,299 - INFO - [Batch] Epoch: 19/40, Batch: 20/25, Combined Loss: 0.183614 (Summen av L1 og STFT), L1 Loss: 2.107852 (Amplitudetap), STFT Loss: 4.605059 (Spektraltap)
2025-01-18 14:48:22,175 - INFO - [Batch] Epoch: 19/40, Batch: 21/25, Combined Loss: 0.121405 (Summen av L1 og STFT), L1 Loss: 1.331365 (Amplitudetap), STFT Loss: 3.071561 (Spektraltap)
2025-01-18 14:48:22,175 - INFO - Epoch [19/40], Batch [21/25], Gradient Norm: 0.0436
2025-01-18 14:49:16,694 - INFO - [Batch] Epoch: 19/40, Batch: 22/25, Combined Loss: 0.113289 (Summen av L1 og STFT), L1 Loss: 1.425107 (Amplitudetap), STFT Loss: 2.787904 (Spektraltap)
2025-01-18 14:49:47,062 - INFO - [Batch] Epoch: 19/40, Batch: 23/25, Combined Loss: 0.084600 (Summen av L1 og STFT), L1 Loss: 1.200301 (Amplitudetap), STFT Loss: 2.023571 (Spektraltap)
2025-01-18 14:50:37,803 - INFO - [Batch] Epoch: 19/40, Batch: 24/25, Combined Loss: 0.114775 (Summen av L1 og STFT), L1 Loss: 1.458467 (Amplitudetap), STFT Loss: 2.818178 (Spektraltap)
2025-01-18 14:51:27,939 - INFO - [Batch] Epoch: 19/40, Batch: 25/25, Combined Loss: 0.076676 (Summen av L1 og STFT), L1 Loss: 1.059819 (Amplitudetap), STFT Loss: 1.846064 (Spektraltap)
2025-01-18 14:51:27,971 - INFO - Epoch [19/40], Batch [25/25], Gradient Norm: 0.0489
2025-01-18 14:51:27,986 - INFO - Current Learning Rate 1.25e-08
2025-01-18 14:51:27,986 - INFO - [Epoch Improvement] Epoch 19: Loss improved by 0.00% from previous epoch.
2025-01-18 14:51:27,986 - INFO - [Epoch Summary] Avg Training Loss: 0.084294
2025-01-18 14:51:27,986 - INFO - [Train] No improvement in loss for epoch 19 with loss: 0.08429432608187198. Best loss remains 0.084276. Trigger_times: 0
2025-01-18 14:59:51,516 - INFO - [Validation] Epoch 19/40, [Validation] Avg Validation Loss:  1.078927
2025-01-18 14:59:51,516 - INFO - [Epoch Summary] Epoch: 19/40, Avg Combined Loss: 0.084294, Avg L1 Loss: 0.980683, Avg STFT Loss: 2.108704
2025-01-18 14:59:51,516 - INFO - Previous epoch loss: 0.08429432608187198
2025-01-18 14:59:51,516 - INFO - [Train] Epoch 20/40 started.
2025-01-18 15:00:55,394 - INFO - [Batch] Epoch: 20/40, Batch: 1/25, Combined Loss: 0.104383 (Summen av L1 og STFT), L1 Loss: 1.236097 (Amplitudetap), STFT Loss: 2.601739 (Spektraltap)
2025-01-18 15:01:48,189 - INFO - [Batch] Epoch: 20/40, Batch: 2/25, Combined Loss: 0.068347 (Summen av L1 og STFT), L1 Loss: 0.831139 (Amplitudetap), STFT Loss: 1.694199 (Spektraltap)
2025-01-18 15:02:44,164 - INFO - [Batch] Epoch: 20/40, Batch: 3/25, Combined Loss: 0.101891 (Summen av L1 og STFT), L1 Loss: 1.282084 (Amplitudetap), STFT Loss: 2.507251 (Spektraltap)
2025-01-18 15:03:33,089 - INFO - [Batch] Epoch: 20/40, Batch: 4/25, Combined Loss: 0.072402 (Summen av L1 og STFT), L1 Loss: 0.868022 (Amplitudetap), STFT Loss: 1.800040 (Spektraltap)
2025-01-18 15:04:17,425 - INFO - [Batch] Epoch: 20/40, Batch: 5/25, Combined Loss: 0.071363 (Summen av L1 og STFT), L1 Loss: 0.937315 (Amplitudetap), STFT Loss: 1.739191 (Spektraltap)
2025-01-18 15:05:00,525 - INFO - [Batch] Epoch: 20/40, Batch: 6/25, Combined Loss: 0.054632 (Summen av L1 og STFT), L1 Loss: 0.708017 (Amplitudetap), STFT Loss: 1.335536 (Spektraltap)
2025-01-18 15:05:51,020 - INFO - [Batch] Epoch: 20/40, Batch: 7/25, Combined Loss: 0.037749 (Summen av L1 og STFT), L1 Loss: 0.345272 (Amplitudetap), STFT Loss: 0.984505 (Spektraltap)
2025-01-18 15:06:41,674 - INFO - [Batch] Epoch: 20/40, Batch: 8/25, Combined Loss: 0.074592 (Summen av L1 og STFT), L1 Loss: 0.750398 (Amplitudetap), STFT Loss: 1.916160 (Spektraltap)
2025-01-18 15:07:37,975 - INFO - [Batch] Epoch: 20/40, Batch: 9/25, Combined Loss: 0.071520 (Summen av L1 og STFT), L1 Loss: 0.765011 (Amplitudetap), STFT Loss: 1.817745 (Spektraltap)
2025-01-18 15:08:28,147 - INFO - [Batch] Epoch: 20/40, Batch: 10/25, Combined Loss: 0.087025 (Summen av L1 og STFT), L1 Loss: 1.010356 (Amplitudetap), STFT Loss: 2.177735 (Spektraltap)
2025-01-18 15:09:22,707 - INFO - [Batch] Epoch: 20/40, Batch: 11/25, Combined Loss: 0.193272 (Summen av L1 og STFT), L1 Loss: 2.375498 (Amplitudetap), STFT Loss: 4.780101 (Spektraltap)
2025-01-18 15:10:12,601 - INFO - [Batch] Epoch: 20/40, Batch: 12/25, Combined Loss: 0.046016 (Summen av L1 og STFT), L1 Loss: 0.365658 (Amplitudetap), STFT Loss: 1.223764 (Spektraltap)
2025-01-18 15:11:00,671 - INFO - [Batch] Epoch: 20/40, Batch: 13/25, Combined Loss: 0.075722 (Summen av L1 og STFT), L1 Loss: 0.851220 (Amplitudetap), STFT Loss: 1.906837 (Spektraltap)
2025-01-18 15:11:53,300 - INFO - [Batch] Epoch: 20/40, Batch: 14/25, Combined Loss: 0.092643 (Summen av L1 og STFT), L1 Loss: 1.065296 (Amplitudetap), STFT Loss: 2.322748 (Spektraltap)
2025-01-18 15:12:32,310 - INFO - [Batch] Epoch: 20/40, Batch: 15/25, Combined Loss: 0.054800 (Summen av L1 og STFT), L1 Loss: 0.484641 (Amplitudetap), STFT Loss: 1.436292 (Spektraltap)
2025-01-18 15:13:23,839 - INFO - [Batch] Epoch: 20/40, Batch: 16/25, Combined Loss: 0.135098 (Summen av L1 og STFT), L1 Loss: 1.744890 (Amplitudetap), STFT Loss: 3.305136 (Spektraltap)
2025-01-18 15:14:11,597 - INFO - [Batch] Epoch: 20/40, Batch: 17/25, Combined Loss: 0.024989 (Summen av L1 og STFT), L1 Loss: 0.168396 (Amplitudetap), STFT Loss: 0.677489 (Spektraltap)
2025-01-18 15:14:58,215 - INFO - [Batch] Epoch: 20/40, Batch: 18/25, Combined Loss: 0.122381 (Summen av L1 og STFT), L1 Loss: 1.459608 (Amplitudetap), STFT Loss: 3.045876 (Spektraltap)
2025-01-18 15:15:48,206 - INFO - [Batch] Epoch: 20/40, Batch: 19/25, Combined Loss: 0.027320 (Summen av L1 og STFT), L1 Loss: 0.195962 (Amplitudetap), STFT Loss: 0.735628 (Spektraltap)
2025-01-18 15:16:43,169 - INFO - [Batch] Epoch: 20/40, Batch: 20/25, Combined Loss: 0.129627 (Summen av L1 og STFT), L1 Loss: 1.833537 (Amplitudetap), STFT Loss: 3.103013 (Spektraltap)
2025-01-18 15:17:38,314 - INFO - [Batch] Epoch: 20/40, Batch: 21/25, Combined Loss: 0.094492 (Summen av L1 og STFT), L1 Loss: 1.008822 (Amplitudetap), STFT Loss: 2.402399 (Spektraltap)
2025-01-18 15:17:38,329 - INFO - Epoch [20/40], Batch [21/25], Gradient Norm: 0.0614
2025-01-18 15:18:27,099 - INFO - [Batch] Epoch: 20/40, Batch: 22/25, Combined Loss: 0.040922 (Summen av L1 og STFT), L1 Loss: 0.371758 (Amplitudetap), STFT Loss: 1.068344 (Spektraltap)
2025-01-18 15:19:13,464 - INFO - [Batch] Epoch: 20/40, Batch: 23/25, Combined Loss: 0.026064 (Summen av L1 og STFT), L1 Loss: 0.294878 (Amplitudetap), STFT Loss: 0.655552 (Spektraltap)
2025-01-18 15:20:01,021 - INFO - [Batch] Epoch: 20/40, Batch: 24/25, Combined Loss: 0.089557 (Summen av L1 og STFT), L1 Loss: 0.983774 (Amplitudetap), STFT Loss: 2.265105 (Spektraltap)
2025-01-18 15:20:52,428 - INFO - [Batch] Epoch: 20/40, Batch: 25/25, Combined Loss: 0.210245 (Summen av L1 og STFT), L1 Loss: 2.571942 (Amplitudetap), STFT Loss: 5.205088 (Spektraltap)
2025-01-18 15:20:52,443 - INFO - Epoch [20/40], Batch [25/25], Gradient Norm: 0.0931
2025-01-18 15:20:52,443 - INFO - Current Learning Rate 1.25e-08
2025-01-18 15:20:52,443 - INFO - [Epoch Improvement] Epoch 20: Loss improved by 0.00% from previous epoch.
2025-01-18 15:20:52,443 - INFO - [Epoch Summary] Avg Training Loss: 0.084282
2025-01-18 15:20:52,443 - INFO - [Train] No improvement in loss for epoch 20 with loss: 0.08428211182355881. Best loss remains 0.084276. Trigger_times: 1
2025-01-18 15:29:38,261 - INFO - [Validation] Epoch 20/40, [Validation] Avg Validation Loss:  1.078936
2025-01-18 15:29:38,261 - INFO - [Memory After Epoch 20] Timestamp: 2025-01-18 15:29:38
2025-01-18 15:29:38,261 - INFO - [Memory After Epoch 20] GPU 0: NVIDIA GeForce RTX 3060, Total=12.88 GB, Allocated=0.51 GB, Cached=23.26 GB
2025-01-18 15:29:38,261 - INFO - [Memory After Epoch 20] CPU Memory Usage: ~0.57 GB
2025-01-18 15:29:38,261 - INFO - [Epoch Summary] Epoch: 20/40, Avg Combined Loss: 0.084282, Avg L1 Loss: 0.980668, Avg STFT Loss: 2.108684
2025-01-18 15:29:38,261 - INFO - Previous epoch loss: 0.08428211182355881
2025-01-18 15:29:38,261 - INFO - [Train] Epoch 21/40 started.
2025-01-18 15:30:26,114 - INFO - [Batch] Epoch: 21/40, Batch: 1/25, Combined Loss: 0.027559 (Summen av L1 og STFT), L1 Loss: 0.212913 (Amplitudetap), STFT Loss: 0.735524 (Spektraltap)
2025-01-18 15:31:21,366 - INFO - [Batch] Epoch: 21/40, Batch: 2/25, Combined Loss: 0.017565 (Summen av L1 og STFT), L1 Loss: 0.194253 (Amplitudetap), STFT Loss: 0.443692 (Spektraltap)
2025-01-18 15:32:16,579 - INFO - [Batch] Epoch: 21/40, Batch: 3/25, Combined Loss: 0.078513 (Summen av L1 og STFT), L1 Loss: 1.126889 (Amplitudetap), STFT Loss: 1.872436 (Spektraltap)
2025-01-18 15:33:07,895 - INFO - [Batch] Epoch: 21/40, Batch: 4/25, Combined Loss: 0.168730 (Summen av L1 og STFT), L1 Loss: 1.953728 (Amplitudetap), STFT Loss: 4.224590 (Spektraltap)
2025-01-18 15:33:50,081 - INFO - [Batch] Epoch: 21/40, Batch: 5/25, Combined Loss: 0.090656 (Summen av L1 og STFT), L1 Loss: 1.144871 (Amplitudetap), STFT Loss: 2.229022 (Spektraltap)
2025-01-18 15:34:40,056 - INFO - [Batch] Epoch: 21/40, Batch: 6/25, Combined Loss: 0.023922 (Summen av L1 og STFT), L1 Loss: 0.400157 (Amplitudetap), STFT Loss: 0.546153 (Spektraltap)
2025-01-18 15:35:34,829 - INFO - [Batch] Epoch: 21/40, Batch: 7/25, Combined Loss: 0.127119 (Summen av L1 og STFT), L1 Loss: 1.571779 (Amplitudetap), STFT Loss: 3.139952 (Spektraltap)
2025-01-18 15:36:08,856 - INFO - [Batch] Epoch: 21/40, Batch: 8/25, Combined Loss: 0.057126 (Summen av L1 og STFT), L1 Loss: 0.568774 (Amplitudetap), STFT Loss: 1.470021 (Spektraltap)
2025-01-18 15:37:00,862 - INFO - [Batch] Epoch: 21/40, Batch: 9/25, Combined Loss: 0.109728 (Summen av L1 og STFT), L1 Loss: 1.399516 (Amplitudetap), STFT Loss: 2.692043 (Spektraltap)
2025-01-18 15:37:49,783 - INFO - [Batch] Epoch: 21/40, Batch: 10/25, Combined Loss: 0.112767 (Summen av L1 og STFT), L1 Loss: 1.093552 (Amplitudetap), STFT Loss: 2.914352 (Spektraltap)
2025-01-18 15:38:42,231 - INFO - [Batch] Epoch: 21/40, Batch: 11/25, Combined Loss: 0.127058 (Summen av L1 og STFT), L1 Loss: 1.485062 (Amplitudetap), STFT Loss: 3.175273 (Spektraltap)
2025-01-18 15:39:36,691 - INFO - [Batch] Epoch: 21/40, Batch: 12/25, Combined Loss: 0.088246 (Summen av L1 og STFT), L1 Loss: 1.148100 (Amplitudetap), STFT Loss: 2.155327 (Spektraltap)
2025-01-18 15:40:12,466 - INFO - [Batch] Epoch: 21/40, Batch: 13/25, Combined Loss: 0.122947 (Summen av L1 og STFT), L1 Loss: 1.372505 (Amplitudetap), STFT Loss: 3.100203 (Spektraltap)
2025-01-18 15:41:02,085 - INFO - [Batch] Epoch: 21/40, Batch: 14/25, Combined Loss: 0.082211 (Summen av L1 og STFT), L1 Loss: 0.933470 (Amplitudetap), STFT Loss: 2.066284 (Spektraltap)
2025-01-18 15:41:56,159 - INFO - [Batch] Epoch: 21/40, Batch: 15/25, Combined Loss: 0.086579 (Summen av L1 og STFT), L1 Loss: 1.038572 (Amplitudetap), STFT Loss: 2.152253 (Spektraltap)
2025-01-18 15:42:54,420 - INFO - [Batch] Epoch: 21/40, Batch: 16/25, Combined Loss: 0.065247 (Summen av L1 og STFT), L1 Loss: 0.750192 (Amplitudetap), STFT Loss: 1.635912 (Spektraltap)
2025-01-18 15:43:51,229 - INFO - [Batch] Epoch: 21/40, Batch: 17/25, Combined Loss: 0.088387 (Summen av L1 og STFT), L1 Loss: 0.994131 (Amplitudetap), STFT Loss: 2.225554 (Spektraltap)
2025-01-18 15:44:31,861 - INFO - [Batch] Epoch: 21/40, Batch: 18/25, Combined Loss: 0.077731 (Summen av L1 og STFT), L1 Loss: 0.811292 (Amplitudetap), STFT Loss: 1.984226 (Spektraltap)
2025-01-18 15:45:18,792 - INFO - [Batch] Epoch: 21/40, Batch: 19/25, Combined Loss: 0.055311 (Summen av L1 og STFT), L1 Loss: 0.586568 (Amplitudetap), STFT Loss: 1.407929 (Spektraltap)
2025-01-18 15:46:08,887 - INFO - [Batch] Epoch: 21/40, Batch: 20/25, Combined Loss: 0.055505 (Summen av L1 og STFT), L1 Loss: 0.465568 (Amplitudetap), STFT Loss: 1.465630 (Spektraltap)
2025-01-18 15:47:05,095 - INFO - [Batch] Epoch: 21/40, Batch: 21/25, Combined Loss: 0.121812 (Summen av L1 og STFT), L1 Loss: 1.375620 (Amplitudetap), STFT Loss: 3.064799 (Spektraltap)
2025-01-18 15:47:05,095 - INFO - Epoch [21/40], Batch [21/25], Gradient Norm: 0.0729
2025-01-18 15:48:01,397 - INFO - [Batch] Epoch: 21/40, Batch: 22/25, Combined Loss: 0.078357 (Summen av L1 og STFT), L1 Loss: 0.974628 (Amplitudetap), STFT Loss: 1.933024 (Spektraltap)
2025-01-18 15:48:55,935 - INFO - [Batch] Epoch: 21/40, Batch: 23/25, Combined Loss: 0.040480 (Summen av L1 og STFT), L1 Loss: 0.404297 (Amplitudetap), STFT Loss: 1.041118 (Spektraltap)
2025-01-18 15:49:46,088 - INFO - [Batch] Epoch: 21/40, Batch: 24/25, Combined Loss: 0.072939 (Summen av L1 og STFT), L1 Loss: 0.825749 (Amplitudetap), STFT Loss: 1.834275 (Spektraltap)
2025-01-18 15:50:43,979 - INFO - [Batch] Epoch: 21/40, Batch: 25/25, Combined Loss: 0.130866 (Summen av L1 og STFT), L1 Loss: 1.673960 (Amplitudetap), STFT Loss: 3.208566 (Spektraltap)
2025-01-18 15:50:43,995 - INFO - Epoch [21/40], Batch [25/25], Gradient Norm: 0.0535
2025-01-18 15:50:44,026 - INFO - Current Learning Rate 1.25e-08
2025-01-18 15:50:44,026 - INFO - [Epoch Improvement] Epoch 21: Loss improved by 0.00% from previous epoch.
2025-01-18 15:50:44,026 - INFO - [Epoch Summary] Avg Training Loss: 0.084294
2025-01-18 15:50:44,026 - INFO - [Train] No improvement in loss for epoch 21 with loss: 0.08429438970983029. Best loss remains 0.084276. Trigger_times: 2
2025-01-18 15:59:06,246 - INFO - [Validation] Epoch 21/40, [Validation] Avg Validation Loss:  1.078927
2025-01-18 15:59:06,246 - INFO - [Epoch Summary] Epoch: 21/40, Avg Combined Loss: 0.084294, Avg L1 Loss: 0.980648, Avg STFT Loss: 2.108686
2025-01-18 15:59:06,246 - INFO - Previous epoch loss: 0.08429438970983029
2025-01-18 15:59:06,246 - INFO - [Train] Epoch 22/40 started.
2025-01-18 15:59:42,663 - INFO - [Batch] Epoch: 22/40, Batch: 1/25, Combined Loss: 0.100429 (Summen av L1 og STFT), L1 Loss: 1.036188 (Amplitudetap), STFT Loss: 2.568777 (Spektraltap)
2025-01-18 16:00:34,494 - INFO - [Batch] Epoch: 22/40, Batch: 2/25, Combined Loss: 0.098433 (Summen av L1 og STFT), L1 Loss: 1.123595 (Amplitudetap), STFT Loss: 2.471452 (Spektraltap)
2025-01-18 16:01:20,715 - INFO - [Batch] Epoch: 22/40, Batch: 3/25, Combined Loss: 0.091125 (Summen av L1 og STFT), L1 Loss: 0.950848 (Amplitudetap), STFT Loss: 2.326232 (Spektraltap)
2025-01-18 16:02:09,359 - INFO - [Batch] Epoch: 22/40, Batch: 4/25, Combined Loss: 0.021446 (Summen av L1 og STFT), L1 Loss: 0.186183 (Amplitudetap), STFT Loss: 0.563581 (Spektraltap)
2025-01-18 16:02:59,381 - INFO - [Batch] Epoch: 22/40, Batch: 5/25, Combined Loss: 0.081014 (Summen av L1 og STFT), L1 Loss: 0.829170 (Amplitudetap), STFT Loss: 2.075076 (Spektraltap)
2025-01-18 16:03:41,770 - INFO - [Batch] Epoch: 22/40, Batch: 6/25, Combined Loss: 0.106617 (Summen av L1 og STFT), L1 Loss: 1.347354 (Amplitudetap), STFT Loss: 2.621078 (Spektraltap)
2025-01-18 16:04:23,756 - INFO - [Batch] Epoch: 22/40, Batch: 7/25, Combined Loss: 0.123182 (Summen av L1 og STFT), L1 Loss: 1.433606 (Amplitudetap), STFT Loss: 3.081043 (Spektraltap)
2025-01-18 16:05:15,865 - INFO - [Batch] Epoch: 22/40, Batch: 8/25, Combined Loss: 0.218443 (Summen av L1 og STFT), L1 Loss: 2.704175 (Amplitudetap), STFT Loss: 5.394371 (Spektraltap)
2025-01-18 16:06:13,119 - INFO - [Batch] Epoch: 22/40, Batch: 9/25, Combined Loss: 0.140672 (Summen av L1 og STFT), L1 Loss: 1.609645 (Amplitudetap), STFT Loss: 3.530320 (Spektraltap)
2025-01-18 16:06:58,154 - INFO - [Batch] Epoch: 22/40, Batch: 10/25, Combined Loss: 0.041823 (Summen av L1 og STFT), L1 Loss: 0.580025 (Amplitudetap), STFT Loss: 1.006099 (Spektraltap)
2025-01-18 16:07:52,405 - INFO - [Batch] Epoch: 22/40, Batch: 11/25, Combined Loss: 0.083199 (Summen av L1 og STFT), L1 Loss: 0.938408 (Amplitudetap), STFT Loss: 2.093793 (Spektraltap)
2025-01-18 16:08:52,609 - INFO - [Batch] Epoch: 22/40, Batch: 12/25, Combined Loss: 0.069576 (Summen av L1 og STFT), L1 Loss: 0.743044 (Amplitudetap), STFT Loss: 1.768836 (Spektraltap)
2025-01-18 16:09:32,081 - INFO - [Batch] Epoch: 22/40, Batch: 13/25, Combined Loss: 0.030165 (Summen av L1 og STFT), L1 Loss: 0.247746 (Amplitudetap), STFT Loss: 0.798769 (Spektraltap)
2025-01-18 16:10:38,030 - INFO - [Batch] Epoch: 22/40, Batch: 14/25, Combined Loss: 0.113683 (Summen av L1 og STFT), L1 Loss: 1.356170 (Amplitudetap), STFT Loss: 2.829283 (Spektraltap)
2025-01-18 16:11:18,431 - INFO - [Batch] Epoch: 22/40, Batch: 15/25, Combined Loss: 0.098231 (Summen av L1 og STFT), L1 Loss: 1.354782 (Amplitudetap), STFT Loss: 2.366309 (Spektraltap)
2025-01-18 16:12:06,942 - INFO - [Batch] Epoch: 22/40, Batch: 16/25, Combined Loss: 0.072066 (Summen av L1 og STFT), L1 Loss: 0.737499 (Amplitudetap), STFT Loss: 1.845921 (Spektraltap)
2025-01-18 16:12:55,634 - INFO - [Batch] Epoch: 22/40, Batch: 17/25, Combined Loss: 0.071163 (Summen av L1 og STFT), L1 Loss: 0.859202 (Amplitudetap), STFT Loss: 1.766654 (Spektraltap)
2025-01-18 16:13:46,489 - INFO - [Batch] Epoch: 22/40, Batch: 18/25, Combined Loss: 0.027710 (Summen av L1 og STFT), L1 Loss: 0.262015 (Amplitudetap), STFT Loss: 0.719015 (Spektraltap)
2025-01-18 16:14:40,166 - INFO - [Batch] Epoch: 22/40, Batch: 19/25, Combined Loss: 0.074710 (Summen av L1 og STFT), L1 Loss: 1.017729 (Amplitudetap), STFT Loss: 1.805142 (Spektraltap)
2025-01-18 16:15:30,498 - INFO - [Batch] Epoch: 22/40, Batch: 20/25, Combined Loss: 0.124739 (Summen av L1 og STFT), L1 Loss: 1.490473 (Amplitudetap), STFT Loss: 3.103400 (Spektraltap)
2025-01-18 16:16:16,545 - INFO - [Batch] Epoch: 22/40, Batch: 21/25, Combined Loss: 0.067425 (Summen av L1 og STFT), L1 Loss: 0.565921 (Amplitudetap), STFT Loss: 1.780215 (Spektraltap)
2025-01-18 16:16:16,545 - INFO - Epoch [22/40], Batch [21/25], Gradient Norm: 0.0689
2025-01-18 16:17:13,265 - INFO - [Batch] Epoch: 22/40, Batch: 22/25, Combined Loss: 0.061241 (Summen av L1 og STFT), L1 Loss: 0.715667 (Amplitudetap), STFT Loss: 1.530514 (Spektraltap)
2025-01-18 16:18:18,388 - INFO - [Batch] Epoch: 22/40, Batch: 23/25, Combined Loss: 0.063346 (Summen av L1 og STFT), L1 Loss: 0.753036 (Amplitudetap), STFT Loss: 1.577636 (Spektraltap)
2025-01-18 16:19:17,038 - INFO - [Batch] Epoch: 22/40, Batch: 24/25, Combined Loss: 0.013075 (Summen av L1 og STFT), L1 Loss: 0.092604 (Amplitudetap), STFT Loss: 0.352575 (Spektraltap)
2025-01-18 16:20:05,423 - INFO - [Batch] Epoch: 22/40, Batch: 25/25, Combined Loss: 0.113801 (Summen av L1 og STFT), L1 Loss: 1.581299 (Amplitudetap), STFT Loss: 2.736326 (Spektraltap)
2025-01-18 16:20:05,439 - INFO - Epoch [22/40], Batch [25/25], Gradient Norm: 0.0786
2025-01-18 16:20:05,439 - INFO - Current Learning Rate 1.25e-08
2025-01-18 16:20:05,439 - INFO - [Epoch Improvement] Epoch 22: Loss improved by 0.00% from previous epoch.
2025-01-18 16:20:05,439 - INFO - [Epoch Summary] Avg Training Loss: 0.084293
2025-01-18 16:20:05,439 - INFO - [Train] No improvement in loss for epoch 22 with loss: 0.08429258272051811. Best loss remains 0.084276. Trigger_times: 3
2025-01-18 16:28:51,233 - INFO - [Validation] Epoch 22/40, [Validation] Avg Validation Loss:  1.078929
2025-01-18 16:28:51,233 - INFO - [Epoch Summary] Epoch: 22/40, Avg Combined Loss: 0.084293, Avg L1 Loss: 0.980648, Avg STFT Loss: 2.108678
2025-01-18 16:28:51,249 - INFO - Previous epoch loss: 0.08429258272051811
2025-01-18 16:28:51,249 - INFO - [Train] Epoch 23/40 started.
2025-01-18 16:29:40,135 - INFO - [Batch] Epoch: 23/40, Batch: 1/25, Combined Loss: 0.060130 (Summen av L1 og STFT), L1 Loss: 0.614116 (Amplitudetap), STFT Loss: 1.540716 (Spektraltap)
2025-01-18 16:30:38,238 - INFO - [Batch] Epoch: 23/40, Batch: 2/25, Combined Loss: 0.118114 (Summen av L1 og STFT), L1 Loss: 1.588586 (Amplitudetap), STFT Loss: 2.862598 (Spektraltap)
2025-01-18 16:31:32,161 - INFO - [Batch] Epoch: 23/40, Batch: 3/25, Combined Loss: 0.011987 (Summen av L1 og STFT), L1 Loss: 0.122244 (Amplitudetap), STFT Loss: 0.307232 (Spektraltap)
2025-01-18 16:32:23,507 - INFO - [Batch] Epoch: 23/40, Batch: 4/25, Combined Loss: 0.070907 (Summen av L1 og STFT), L1 Loss: 0.727273 (Amplitudetap), STFT Loss: 1.815518 (Spektraltap)
2025-01-18 16:33:07,047 - INFO - [Batch] Epoch: 23/40, Batch: 5/25, Combined Loss: 0.038629 (Summen av L1 og STFT), L1 Loss: 0.358618 (Amplitudetap), STFT Loss: 1.005179 (Spektraltap)
2025-01-18 16:33:51,887 - INFO - [Batch] Epoch: 23/40, Batch: 6/25, Combined Loss: 0.105462 (Summen av L1 og STFT), L1 Loss: 1.189081 (Amplitudetap), STFT Loss: 2.654268 (Spektraltap)
2025-01-18 16:34:38,824 - INFO - [Batch] Epoch: 23/40, Batch: 7/25, Combined Loss: 0.124085 (Summen av L1 og STFT), L1 Loss: 1.410174 (Amplitudetap), STFT Loss: 3.118188 (Spektraltap)
2025-01-18 16:35:25,638 - INFO - [Batch] Epoch: 23/40, Batch: 8/25, Combined Loss: 0.116659 (Summen av L1 og STFT), L1 Loss: 1.228090 (Amplitudetap), STFT Loss: 2.973441 (Spektraltap)
2025-01-18 16:36:25,430 - INFO - [Batch] Epoch: 23/40, Batch: 9/25, Combined Loss: 0.153333 (Summen av L1 og STFT), L1 Loss: 1.825618 (Amplitudetap), STFT Loss: 3.817576 (Spektraltap)
2025-01-18 16:37:21,665 - INFO - [Batch] Epoch: 23/40, Batch: 10/25, Combined Loss: 0.090056 (Summen av L1 og STFT), L1 Loss: 1.144843 (Amplitudetap), STFT Loss: 2.211021 (Spektraltap)
2025-01-18 16:38:11,248 - INFO - [Batch] Epoch: 23/40, Batch: 11/25, Combined Loss: 0.121391 (Summen av L1 og STFT), L1 Loss: 1.526063 (Amplitudetap), STFT Loss: 2.987692 (Spektraltap)
2025-01-18 16:39:02,851 - INFO - [Batch] Epoch: 23/40, Batch: 12/25, Combined Loss: 0.127392 (Summen av L1 og STFT), L1 Loss: 1.475567 (Amplitudetap), STFT Loss: 3.189378 (Spektraltap)
2025-01-18 16:39:44,889 - INFO - [Batch] Epoch: 23/40, Batch: 13/25, Combined Loss: 0.043674 (Summen av L1 og STFT), L1 Loss: 0.519810 (Amplitudetap), STFT Loss: 1.087456 (Spektraltap)
2025-01-18 16:40:43,712 - INFO - [Batch] Epoch: 23/40, Batch: 14/25, Combined Loss: 0.095066 (Summen av L1 og STFT), L1 Loss: 1.175784 (Amplitudetap), STFT Loss: 2.348068 (Spektraltap)
2025-01-18 16:41:26,747 - INFO - [Batch] Epoch: 23/40, Batch: 15/25, Combined Loss: 0.115822 (Summen av L1 og STFT), L1 Loss: 1.360797 (Amplitudetap), STFT Loss: 2.891450 (Spektraltap)
2025-01-18 16:42:16,091 - INFO - [Batch] Epoch: 23/40, Batch: 16/25, Combined Loss: 0.024046 (Summen av L1 og STFT), L1 Loss: 0.185256 (Amplitudetap), STFT Loss: 0.641998 (Spektraltap)
2025-01-18 16:43:05,896 - INFO - [Batch] Epoch: 23/40, Batch: 17/25, Combined Loss: 0.145562 (Summen av L1 og STFT), L1 Loss: 1.728450 (Amplitudetap), STFT Loss: 3.626082 (Spektraltap)
2025-01-18 16:43:50,842 - INFO - [Batch] Epoch: 23/40, Batch: 18/25, Combined Loss: 0.023977 (Summen av L1 og STFT), L1 Loss: 0.202927 (Amplitudetap), STFT Loss: 0.632339 (Spektraltap)
2025-01-18 16:44:45,546 - INFO - [Batch] Epoch: 23/40, Batch: 19/25, Combined Loss: 0.064109 (Summen av L1 og STFT), L1 Loss: 0.736271 (Amplitudetap), STFT Loss: 1.607734 (Spektraltap)
2025-01-18 16:45:34,813 - INFO - [Batch] Epoch: 23/40, Batch: 20/25, Combined Loss: 0.045812 (Summen av L1 og STFT), L1 Loss: 0.423101 (Amplitudetap), STFT Loss: 1.193035 (Spektraltap)
2025-01-18 16:46:14,103 - INFO - [Batch] Epoch: 23/40, Batch: 21/25, Combined Loss: 0.067891 (Summen av L1 og STFT), L1 Loss: 0.813920 (Amplitudetap), STFT Loss: 1.687909 (Spektraltap)
2025-01-18 16:46:14,103 - INFO - Epoch [23/40], Batch [21/25], Gradient Norm: 0.0528
2025-01-18 16:47:11,358 - INFO - [Batch] Epoch: 23/40, Batch: 22/25, Combined Loss: 0.134367 (Summen av L1 og STFT), L1 Loss: 1.809529 (Amplitudetap), STFT Loss: 3.255493 (Spektraltap)
2025-01-18 16:48:08,474 - INFO - [Batch] Epoch: 23/40, Batch: 23/25, Combined Loss: 0.014099 (Summen av L1 og STFT), L1 Loss: 0.117374 (Amplitudetap), STFT Loss: 0.372658 (Spektraltap)
2025-01-18 16:49:06,178 - INFO - [Batch] Epoch: 23/40, Batch: 24/25, Combined Loss: 0.132164 (Summen av L1 og STFT), L1 Loss: 1.523298 (Amplitudetap), STFT Loss: 3.312084 (Spektraltap)
2025-01-18 16:49:55,411 - INFO - [Batch] Epoch: 23/40, Batch: 25/25, Combined Loss: 0.062457 (Summen av L1 og STFT), L1 Loss: 0.703999 (Amplitudetap), STFT Loss: 1.571993 (Spektraltap)
2025-01-18 16:49:55,427 - INFO - Epoch [23/40], Batch [25/25], Gradient Norm: 0.0581
2025-01-18 16:49:55,458 - INFO - Current Learning Rate 1.25e-08
2025-01-18 16:49:55,458 - INFO - [Epoch Improvement] Epoch 23: Loss improved by 0.00% from previous epoch.
2025-01-18 16:49:55,458 - INFO - [Epoch Summary] Avg Training Loss: 0.084288
2025-01-18 16:49:55,458 - INFO - [Train] No improvement in loss for epoch 23 with loss: 0.08428763914853335. Best loss remains 0.084276. Trigger_times: 4
2025-01-18 16:58:18,023 - INFO - [Validation] Epoch 23/40, [Validation] Avg Validation Loss:  1.078921
2025-01-18 16:58:18,023 - INFO - [Epoch Summary] Epoch: 23/40, Avg Combined Loss: 0.084288, Avg L1 Loss: 0.980639, Avg STFT Loss: 2.108667
2025-01-18 16:58:18,023 - INFO - Previous epoch loss: 0.08428763914853335
2025-01-18 16:58:18,023 - INFO - [Train] Epoch 24/40 started.
2025-01-18 16:59:05,102 - INFO - [Batch] Epoch: 24/40, Batch: 1/25, Combined Loss: 0.083589 (Summen av L1 og STFT), L1 Loss: 0.839932 (Amplitudetap), STFT Loss: 2.147692 (Spektraltap)
2025-01-18 16:59:54,734 - INFO - [Batch] Epoch: 24/40, Batch: 2/25, Combined Loss: 0.087814 (Summen av L1 og STFT), L1 Loss: 0.905182 (Amplitudetap), STFT Loss: 2.246474 (Spektraltap)
2025-01-18 17:00:47,922 - INFO - [Batch] Epoch: 24/40, Batch: 3/25, Combined Loss: 0.028563 (Summen av L1 og STFT), L1 Loss: 0.234341 (Amplitudetap), STFT Loss: 0.756451 (Spektraltap)
2025-01-18 17:01:41,381 - INFO - [Batch] Epoch: 24/40, Batch: 4/25, Combined Loss: 0.116020 (Summen av L1 og STFT), L1 Loss: 1.435353 (Amplitudetap), STFT Loss: 2.865456 (Spektraltap)
2025-01-18 17:02:25,878 - INFO - [Batch] Epoch: 24/40, Batch: 5/25, Combined Loss: 0.074313 (Summen av L1 og STFT), L1 Loss: 0.899082 (Amplitudetap), STFT Loss: 1.844075 (Spektraltap)
2025-01-18 17:03:26,494 - INFO - [Batch] Epoch: 24/40, Batch: 6/25, Combined Loss: 0.175974 (Summen av L1 og STFT), L1 Loss: 2.095345 (Amplitudetap), STFT Loss: 4.381219 (Spektraltap)
2025-01-18 17:04:17,344 - INFO - [Batch] Epoch: 24/40, Batch: 7/25, Combined Loss: 0.047842 (Summen av L1 og STFT), L1 Loss: 0.668976 (Amplitudetap), STFT Loss: 1.148560 (Spektraltap)
2025-01-18 17:05:08,401 - INFO - [Batch] Epoch: 24/40, Batch: 8/25, Combined Loss: 0.115069 (Summen av L1 og STFT), L1 Loss: 1.253782 (Amplitudetap), STFT Loss: 2.914732 (Spektraltap)
2025-01-18 17:06:01,367 - INFO - [Batch] Epoch: 24/40, Batch: 9/25, Combined Loss: 0.068457 (Summen av L1 og STFT), L1 Loss: 0.779912 (Amplitudetap), STFT Loss: 1.719474 (Spektraltap)
2025-01-18 17:06:46,595 - INFO - [Batch] Epoch: 24/40, Batch: 10/25, Combined Loss: 0.040292 (Summen av L1 og STFT), L1 Loss: 0.396270 (Amplitudetap), STFT Loss: 1.038926 (Spektraltap)
2025-01-18 17:07:33,348 - INFO - [Batch] Epoch: 24/40, Batch: 11/25, Combined Loss: 0.081224 (Summen av L1 og STFT), L1 Loss: 0.907420 (Amplitudetap), STFT Loss: 2.047826 (Spektraltap)
2025-01-18 17:08:22,216 - INFO - [Batch] Epoch: 24/40, Batch: 12/25, Combined Loss: 0.160266 (Summen av L1 og STFT), L1 Loss: 1.909651 (Amplitudetap), STFT Loss: 3.989547 (Spektraltap)
2025-01-18 17:09:22,876 - INFO - [Batch] Epoch: 24/40, Batch: 13/25, Combined Loss: 0.069274 (Summen av L1 og STFT), L1 Loss: 0.790684 (Amplitudetap), STFT Loss: 1.739351 (Spektraltap)
2025-01-18 17:10:07,354 - INFO - [Batch] Epoch: 24/40, Batch: 14/25, Combined Loss: 0.038011 (Summen av L1 og STFT), L1 Loss: 0.377775 (Amplitudetap), STFT Loss: 0.978423 (Spektraltap)
2025-01-18 17:10:43,033 - INFO - [Batch] Epoch: 24/40, Batch: 15/25, Combined Loss: 0.034782 (Summen av L1 og STFT), L1 Loss: 0.323747 (Amplitudetap), STFT Loss: 0.904706 (Spektraltap)
2025-01-18 17:11:32,636 - INFO - [Batch] Epoch: 24/40, Batch: 16/25, Combined Loss: 0.092913 (Summen av L1 og STFT), L1 Loss: 1.222815 (Amplitudetap), STFT Loss: 2.263333 (Spektraltap)
2025-01-18 17:12:28,334 - INFO - [Batch] Epoch: 24/40, Batch: 17/25, Combined Loss: 0.085524 (Summen av L1 og STFT), L1 Loss: 1.047486 (Amplitudetap), STFT Loss: 2.116798 (Spektraltap)
2025-01-18 17:13:24,142 - INFO - [Batch] Epoch: 24/40, Batch: 18/25, Combined Loss: 0.090598 (Summen av L1 og STFT), L1 Loss: 0.988562 (Amplitudetap), STFT Loss: 2.294256 (Spektraltap)
2025-01-18 17:14:05,516 - INFO - [Batch] Epoch: 24/40, Batch: 19/25, Combined Loss: 0.084315 (Summen av L1 og STFT), L1 Loss: 0.990335 (Amplitudetap), STFT Loss: 2.105011 (Spektraltap)
2025-01-18 17:14:46,241 - INFO - [Batch] Epoch: 24/40, Batch: 20/25, Combined Loss: 0.118902 (Summen av L1 og STFT), L1 Loss: 1.182172 (Amplitudetap), STFT Loss: 3.060405 (Spektraltap)
2025-01-18 17:15:37,165 - INFO - [Batch] Epoch: 24/40, Batch: 21/25, Combined Loss: 0.074805 (Summen av L1 og STFT), L1 Loss: 0.841089 (Amplitudetap), STFT Loss: 1.883672 (Spektraltap)
2025-01-18 17:15:37,165 - INFO - Epoch [24/40], Batch [21/25], Gradient Norm: 0.0320
2025-01-18 17:16:34,467 - INFO - [Batch] Epoch: 24/40, Batch: 22/25, Combined Loss: 0.093476 (Summen av L1 og STFT), L1 Loss: 1.138014 (Amplitudetap), STFT Loss: 2.316549 (Spektraltap)
2025-01-18 17:17:24,668 - INFO - [Batch] Epoch: 24/40, Batch: 23/25, Combined Loss: 0.052455 (Summen av L1 og STFT), L1 Loss: 0.602999 (Amplitudetap), STFT Loss: 1.315220 (Spektraltap)
2025-01-18 17:18:14,883 - INFO - [Batch] Epoch: 24/40, Batch: 24/25, Combined Loss: 0.016767 (Summen av L1 og STFT), L1 Loss: 0.125255 (Amplitudetap), STFT Loss: 0.449327 (Spektraltap)
2025-01-18 17:19:04,231 - INFO - [Batch] Epoch: 24/40, Batch: 25/25, Combined Loss: 0.175598 (Summen av L1 og STFT), L1 Loss: 2.556676 (Amplitudetap), STFT Loss: 4.172213 (Spektraltap)
2025-01-18 17:19:04,246 - INFO - Epoch [24/40], Batch [25/25], Gradient Norm: 0.0593
2025-01-18 17:19:04,246 - INFO - Current Learning Rate 1.25e-08
2025-01-18 17:19:04,246 - INFO - [Epoch Improvement] Epoch 24: Loss improved by 0.00% from previous epoch.
2025-01-18 17:19:04,246 - INFO - [Epoch Summary] Avg Training Loss: 0.084274
2025-01-18 17:19:04,359 - INFO - [Train] New best model saved at C:\Users\didri\Desktop\UNet Models\UNet_vocal_isolation_model\Model_weights\CheckPoints\best_model_epoch-23.pth with loss 0.084274
2025-01-18 17:27:47,312 - INFO - [Validation] Epoch 24/40, [Validation] Avg Validation Loss:  1.078930
2025-01-18 17:27:47,312 - INFO - [Epoch Summary] Epoch: 24/40, Avg Combined Loss: 0.084274, Avg L1 Loss: 0.980633, Avg STFT Loss: 2.108639
2025-01-18 17:27:47,312 - INFO - Previous epoch loss: 0.08427360646426678
2025-01-18 17:27:47,327 - INFO - [Train] Epoch 25/40 started.
2025-01-18 17:28:40,647 - INFO - [Batch] Epoch: 25/40, Batch: 1/25, Combined Loss: 0.142990 (Summen av L1 og STFT), L1 Loss: 1.666777 (Amplitudetap), STFT Loss: 3.575366 (Spektraltap)
2025-01-18 17:29:29,566 - INFO - [Batch] Epoch: 25/40, Batch: 2/25, Combined Loss: 0.039086 (Summen av L1 og STFT), L1 Loss: 0.250002 (Amplitudetap), STFT Loss: 1.065430 (Spektraltap)
2025-01-18 17:30:15,134 - INFO - [Batch] Epoch: 25/40, Batch: 3/25, Combined Loss: 0.080433 (Summen av L1 og STFT), L1 Loss: 1.138138 (Amplitudetap), STFT Loss: 1.925208 (Spektraltap)
2025-01-18 17:30:54,231 - INFO - [Batch] Epoch: 25/40, Batch: 4/25, Combined Loss: 0.034186 (Summen av L1 og STFT), L1 Loss: 0.355656 (Amplitudetap), STFT Loss: 0.873145 (Spektraltap)
2025-01-18 17:31:49,588 - INFO - [Batch] Epoch: 25/40, Batch: 5/25, Combined Loss: 0.092363 (Summen av L1 og STFT), L1 Loss: 1.198272 (Amplitudetap), STFT Loss: 2.257349 (Spektraltap)
2025-01-18 17:32:44,665 - INFO - [Batch] Epoch: 25/40, Batch: 6/25, Combined Loss: 0.140326 (Summen av L1 og STFT), L1 Loss: 1.677359 (Amplitudetap), STFT Loss: 3.490903 (Spektraltap)
2025-01-18 17:33:30,951 - INFO - [Batch] Epoch: 25/40, Batch: 7/25, Combined Loss: 0.084172 (Summen av L1 og STFT), L1 Loss: 0.892460 (Amplitudetap), STFT Loss: 2.142672 (Spektraltap)
2025-01-18 17:34:21,023 - INFO - [Batch] Epoch: 25/40, Batch: 8/25, Combined Loss: 0.075699 (Summen av L1 og STFT), L1 Loss: 0.867637 (Amplitudetap), STFT Loss: 1.899138 (Spektraltap)
2025-01-18 17:35:08,928 - INFO - [Batch] Epoch: 25/40, Batch: 9/25, Combined Loss: 0.129693 (Summen av L1 og STFT), L1 Loss: 1.529773 (Amplitudetap), STFT Loss: 3.235169 (Spektraltap)
2025-01-18 17:36:03,558 - INFO - [Batch] Epoch: 25/40, Batch: 10/25, Combined Loss: 0.076913 (Summen av L1 og STFT), L1 Loss: 0.850216 (Amplitudetap), STFT Loss: 1.943004 (Spektraltap)
2025-01-18 17:36:56,267 - INFO - [Batch] Epoch: 25/40, Batch: 11/25, Combined Loss: 0.068284 (Summen av L1 og STFT), L1 Loss: 0.765191 (Amplitudetap), STFT Loss: 1.720593 (Spektraltap)
2025-01-18 17:37:36,143 - INFO - [Batch] Epoch: 25/40, Batch: 12/25, Combined Loss: 0.072320 (Summen av L1 og STFT), L1 Loss: 0.958399 (Amplitudetap), STFT Loss: 1.758873 (Spektraltap)
2025-01-18 17:38:30,946 - INFO - [Batch] Epoch: 25/40, Batch: 13/25, Combined Loss: 0.031588 (Summen av L1 og STFT), L1 Loss: 0.272384 (Amplitudetap), STFT Loss: 0.830902 (Spektraltap)
2025-01-18 17:39:28,781 - INFO - [Batch] Epoch: 25/40, Batch: 14/25, Combined Loss: 0.129132 (Summen av L1 og STFT), L1 Loss: 1.532169 (Amplitudetap), STFT Loss: 3.217314 (Spektraltap)
2025-01-18 17:40:20,041 - INFO - [Batch] Epoch: 25/40, Batch: 15/25, Combined Loss: 0.115812 (Summen av L1 og STFT), L1 Loss: 1.420282 (Amplitudetap), STFT Loss: 2.865682 (Spektraltap)
2025-01-18 17:41:18,364 - INFO - [Batch] Epoch: 25/40, Batch: 16/25, Combined Loss: 0.145202 (Summen av L1 og STFT), L1 Loss: 1.761851 (Amplitudetap), STFT Loss: 3.600989 (Spektraltap)
2025-01-18 17:41:57,821 - INFO - [Batch] Epoch: 25/40, Batch: 17/25, Combined Loss: 0.152772 (Summen av L1 og STFT), L1 Loss: 1.773375 (Amplitudetap), STFT Loss: 3.823152 (Spektraltap)
2025-01-18 17:42:49,747 - INFO - [Batch] Epoch: 25/40, Batch: 18/25, Combined Loss: 0.034678 (Summen av L1 og STFT), L1 Loss: 0.272005 (Amplitudetap), STFT Loss: 0.923768 (Spektraltap)
2025-01-18 17:43:34,541 - INFO - [Batch] Epoch: 25/40, Batch: 19/25, Combined Loss: 0.021184 (Summen av L1 og STFT), L1 Loss: 0.126642 (Amplitudetap), STFT Loss: 0.581231 (Spektraltap)
2025-01-18 17:44:22,009 - INFO - [Batch] Epoch: 25/40, Batch: 20/25, Combined Loss: 0.113054 (Summen av L1 og STFT), L1 Loss: 1.457815 (Amplitudetap), STFT Loss: 2.766853 (Spektraltap)
2025-01-18 17:45:23,288 - INFO - [Batch] Epoch: 25/40, Batch: 21/25, Combined Loss: 0.105651 (Summen av L1 og STFT), L1 Loss: 1.379182 (Amplitudetap), STFT Loss: 2.578456 (Spektraltap)
2025-01-18 17:45:23,304 - INFO - Epoch [25/40], Batch [21/25], Gradient Norm: 0.0461
2025-01-18 17:46:19,337 - INFO - [Batch] Epoch: 25/40, Batch: 22/25, Combined Loss: 0.025780 (Summen av L1 og STFT), L1 Loss: 0.259383 (Amplitudetap), STFT Loss: 0.662240 (Spektraltap)
2025-01-18 17:47:09,963 - INFO - [Batch] Epoch: 25/40, Batch: 23/25, Combined Loss: 0.030681 (Summen av L1 og STFT), L1 Loss: 0.253999 (Amplitudetap), STFT Loss: 0.811585 (Spektraltap)
2025-01-18 17:47:44,300 - INFO - [Batch] Epoch: 25/40, Batch: 24/25, Combined Loss: 0.090492 (Summen av L1 og STFT), L1 Loss: 1.015895 (Amplitudetap), STFT Loss: 2.279376 (Spektraltap)
2025-01-18 17:48:44,670 - INFO - [Batch] Epoch: 25/40, Batch: 25/25, Combined Loss: 0.074684 (Summen av L1 og STFT), L1 Loss: 0.838195 (Amplitudetap), STFT Loss: 1.881298 (Spektraltap)
2025-01-18 17:48:44,686 - INFO - Epoch [25/40], Batch [25/25], Gradient Norm: 0.0278
2025-01-18 17:48:44,686 - INFO - Current Learning Rate 1.25e-08
2025-01-18 17:48:44,686 - INFO - [Epoch Improvement] Epoch 25: Loss improved by 0.00% from previous epoch.
2025-01-18 17:48:44,686 - INFO - [Epoch Summary] Avg Training Loss: 0.084287
2025-01-18 17:48:44,686 - INFO - [Train] No improvement in loss for epoch 25 with loss: 0.08428705371916294. Best loss remains 0.084274. Trigger_times: 0
2025-01-18 17:57:25,070 - INFO - [Validation] Epoch 25/40, [Validation] Avg Validation Loss:  1.078887
2025-01-18 17:57:25,070 - INFO - [Memory After Epoch 25] Timestamp: 2025-01-18 17:57:25
2025-01-18 17:57:25,070 - INFO - [Memory After Epoch 25] GPU 0: NVIDIA GeForce RTX 3060, Total=12.88 GB, Allocated=0.51 GB, Cached=23.26 GB
2025-01-18 17:57:25,070 - INFO - [Memory After Epoch 25] CPU Memory Usage: ~0.57 GB
2025-01-18 17:57:25,070 - INFO - [Epoch Summary] Epoch: 25/40, Avg Combined Loss: 0.084287, Avg L1 Loss: 0.980629, Avg STFT Loss: 2.108629
2025-01-18 17:57:25,070 - INFO - Previous epoch loss: 0.08428705371916294
2025-01-18 17:57:25,070 - INFO - [Train] Epoch 26/40 started.
2025-01-18 17:58:07,553 - INFO - [Batch] Epoch: 26/40, Batch: 1/25, Combined Loss: 0.195615 (Summen av L1 og STFT), L1 Loss: 2.215168 (Amplitudetap), STFT Loss: 4.919099 (Spektraltap)
2025-01-18 17:58:55,768 - INFO - [Batch] Epoch: 26/40, Batch: 2/25, Combined Loss: 0.071973 (Summen av L1 og STFT), L1 Loss: 0.817070 (Amplitudetap), STFT Loss: 1.809014 (Spektraltap)
2025-01-18 17:59:42,112 - INFO - [Batch] Epoch: 26/40, Batch: 3/25, Combined Loss: 0.179685 (Summen av L1 og STFT), L1 Loss: 2.145135 (Amplitudetap), STFT Loss: 4.471194 (Spektraltap)
2025-01-18 18:00:43,705 - INFO - [Batch] Epoch: 26/40, Batch: 4/25, Combined Loss: 0.119942 (Summen av L1 og STFT), L1 Loss: 1.392388 (Amplitudetap), STFT Loss: 3.001513 (Spektraltap)
2025-01-18 18:01:30,340 - INFO - [Batch] Epoch: 26/40, Batch: 5/25, Combined Loss: 0.208206 (Summen av L1 og STFT), L1 Loss: 2.678643 (Amplitudetap), STFT Loss: 5.098183 (Spektraltap)
2025-01-18 18:02:30,703 - INFO - [Batch] Epoch: 26/40, Batch: 6/25, Combined Loss: 0.106435 (Summen av L1 og STFT), L1 Loss: 1.497382 (Amplitudetap), STFT Loss: 2.551308 (Spektraltap)
2025-01-18 18:03:27,985 - INFO - [Batch] Epoch: 26/40, Batch: 7/25, Combined Loss: 0.113055 (Summen av L1 og STFT), L1 Loss: 1.388230 (Amplitudetap), STFT Loss: 2.796682 (Spektraltap)
2025-01-18 18:04:22,944 - INFO - [Batch] Epoch: 26/40, Batch: 8/25, Combined Loss: 0.049096 (Summen av L1 og STFT), L1 Loss: 0.449020 (Amplitudetap), STFT Loss: 1.280446 (Spektraltap)
2025-01-18 18:05:10,659 - INFO - [Batch] Epoch: 26/40, Batch: 9/25, Combined Loss: 0.071753 (Summen av L1 og STFT), L1 Loss: 1.037442 (Amplitudetap), STFT Loss: 1.707959 (Spektraltap)
2025-01-18 18:06:08,518 - INFO - [Batch] Epoch: 26/40, Batch: 10/25, Combined Loss: 0.066326 (Summen av L1 og STFT), L1 Loss: 0.690107 (Amplitudetap), STFT Loss: 1.694029 (Spektraltap)
2025-01-18 18:06:50,101 - INFO - [Batch] Epoch: 26/40, Batch: 11/25, Combined Loss: 0.025249 (Summen av L1 og STFT), L1 Loss: 0.147792 (Amplitudetap), STFT Loss: 0.694123 (Spektraltap)
2025-01-18 18:07:42,886 - INFO - [Batch] Epoch: 26/40, Batch: 12/25, Combined Loss: 0.070210 (Summen av L1 og STFT), L1 Loss: 0.799240 (Amplitudetap), STFT Loss: 1.763779 (Spektraltap)
2025-01-18 18:08:27,174 - INFO - [Batch] Epoch: 26/40, Batch: 13/25, Combined Loss: 0.060833 (Summen av L1 og STFT), L1 Loss: 0.742025 (Amplitudetap), STFT Loss: 1.506970 (Spektraltap)
2025-01-18 18:09:20,715 - INFO - [Batch] Epoch: 26/40, Batch: 14/25, Combined Loss: 0.024249 (Summen av L1 og STFT), L1 Loss: 0.238096 (Amplitudetap), STFT Loss: 0.625415 (Spektraltap)
2025-01-18 18:10:12,797 - INFO - [Batch] Epoch: 26/40, Batch: 15/25, Combined Loss: 0.046263 (Summen av L1 og STFT), L1 Loss: 0.411787 (Amplitudetap), STFT Loss: 1.211421 (Spektraltap)
2025-01-18 18:11:11,726 - INFO - [Batch] Epoch: 26/40, Batch: 16/25, Combined Loss: 0.098895 (Summen av L1 og STFT), L1 Loss: 1.036974 (Amplitudetap), STFT Loss: 2.522434 (Spektraltap)
2025-01-18 18:11:58,264 - INFO - [Batch] Epoch: 26/40, Batch: 17/25, Combined Loss: 0.073903 (Summen av L1 og STFT), L1 Loss: 1.019629 (Amplitudetap), STFT Loss: 1.780111 (Spektraltap)
2025-01-18 18:12:46,809 - INFO - [Batch] Epoch: 26/40, Batch: 18/25, Combined Loss: 0.025637 (Summen av L1 og STFT), L1 Loss: 0.198103 (Amplitudetap), STFT Loss: 0.684211 (Spektraltap)
2025-01-18 18:13:35,050 - INFO - [Batch] Epoch: 26/40, Batch: 19/25, Combined Loss: 0.071530 (Summen av L1 og STFT), L1 Loss: 0.827389 (Amplitudetap), STFT Loss: 1.791294 (Spektraltap)
2025-01-18 18:14:18,888 - INFO - [Batch] Epoch: 26/40, Batch: 20/25, Combined Loss: 0.039465 (Summen av L1 og STFT), L1 Loss: 0.318941 (Amplitudetap), STFT Loss: 1.047273 (Spektraltap)
2025-01-18 18:15:06,651 - INFO - [Batch] Epoch: 26/40, Batch: 21/25, Combined Loss: 0.035179 (Summen av L1 og STFT), L1 Loss: 0.337024 (Amplitudetap), STFT Loss: 0.910932 (Spektraltap)
2025-01-18 18:15:06,667 - INFO - Epoch [26/40], Batch [21/25], Gradient Norm: 0.0357
2025-01-18 18:16:03,802 - INFO - [Batch] Epoch: 26/40, Batch: 22/25, Combined Loss: 0.153118 (Summen av L1 og STFT), L1 Loss: 1.877716 (Amplitudetap), STFT Loss: 3.788816 (Spektraltap)
2025-01-18 18:16:55,725 - INFO - [Batch] Epoch: 26/40, Batch: 23/25, Combined Loss: 0.131784 (Summen av L1 og STFT), L1 Loss: 1.570835 (Amplitudetap), STFT Loss: 3.280303 (Spektraltap)
2025-01-18 18:17:45,797 - INFO - [Batch] Epoch: 26/40, Batch: 24/25, Combined Loss: 0.050434 (Summen av L1 og STFT), L1 Loss: 0.493755 (Amplitudetap), STFT Loss: 1.301396 (Spektraltap)
2025-01-18 18:18:26,276 - INFO - [Batch] Epoch: 26/40, Batch: 25/25, Combined Loss: 0.018266 (Summen av L1 og STFT), L1 Loss: 0.188030 (Amplitudetap), STFT Loss: 0.467390 (Spektraltap)
2025-01-18 18:18:26,276 - INFO - Epoch [26/40], Batch [25/25], Gradient Norm: 0.0314
2025-01-18 18:18:26,276 - INFO - Current Learning Rate 1.25e-08
2025-01-18 18:18:26,276 - INFO - [Epoch Improvement] Epoch 26: Loss improved by 0.00% from previous epoch.
2025-01-18 18:18:26,276 - INFO - [Epoch Summary] Avg Training Loss: 0.084284
2025-01-18 18:18:26,276 - INFO - [Train] No improvement in loss for epoch 26 with loss: 0.0842839690297842. Best loss remains 0.084274. Trigger_times: 1
2025-01-18 18:27:14,006 - INFO - [Validation] Epoch 26/40, [Validation] Avg Validation Loss:  1.078892
2025-01-18 18:27:14,006 - INFO - [Epoch Summary] Epoch: 26/40, Avg Combined Loss: 0.084284, Avg L1 Loss: 0.980632, Avg STFT Loss: 2.108613
2025-01-18 18:27:14,006 - INFO - Previous epoch loss: 0.0842839690297842
2025-01-18 18:27:14,006 - INFO - [Train] Epoch 27/40 started.
2025-01-18 18:28:06,507 - INFO - [Batch] Epoch: 27/40, Batch: 1/25, Combined Loss: 0.134869 (Summen av L1 og STFT), L1 Loss: 1.701607 (Amplitudetap), STFT Loss: 3.316820 (Spektraltap)
2025-01-18 18:28:57,689 - INFO - [Batch] Epoch: 27/40, Batch: 2/25, Combined Loss: 0.068885 (Summen av L1 og STFT), L1 Loss: 0.812065 (Amplitudetap), STFT Loss: 1.718520 (Spektraltap)
2025-01-18 18:29:56,152 - INFO - [Batch] Epoch: 27/40, Batch: 3/25, Combined Loss: 0.083013 (Summen av L1 og STFT), L1 Loss: 0.964493 (Amplitudetap), STFT Loss: 2.077041 (Spektraltap)
2025-01-18 18:30:57,545 - INFO - [Batch] Epoch: 27/40, Batch: 4/25, Combined Loss: 0.063559 (Summen av L1 og STFT), L1 Loss: 0.815823 (Amplitudetap), STFT Loss: 1.557128 (Spektraltap)
2025-01-18 18:31:48,889 - INFO - [Batch] Epoch: 27/40, Batch: 5/25, Combined Loss: 0.047359 (Summen av L1 og STFT), L1 Loss: 0.516795 (Amplitudetap), STFT Loss: 1.199272 (Spektraltap)
2025-01-18 18:32:30,155 - INFO - [Batch] Epoch: 27/40, Batch: 6/25, Combined Loss: 0.019096 (Summen av L1 og STFT), L1 Loss: 0.108269 (Amplitudetap), STFT Loss: 0.526488 (Spektraltap)
2025-01-18 18:33:23,966 - INFO - [Batch] Epoch: 27/40, Batch: 7/25, Combined Loss: 0.086860 (Summen av L1 og STFT), L1 Loss: 0.961674 (Amplitudetap), STFT Loss: 2.193648 (Spektraltap)
2025-01-18 18:34:14,814 - INFO - [Batch] Epoch: 27/40, Batch: 8/25, Combined Loss: 0.101396 (Summen av L1 og STFT), L1 Loss: 1.283969 (Amplitudetap), STFT Loss: 2.491612 (Spektraltap)
2025-01-18 18:34:50,467 - INFO - [Batch] Epoch: 27/40, Batch: 9/25, Combined Loss: 0.020008 (Summen av L1 og STFT), L1 Loss: 0.104076 (Amplitudetap), STFT Loss: 0.555645 (Spektraltap)
2025-01-18 18:35:30,317 - INFO - [Batch] Epoch: 27/40, Batch: 10/25, Combined Loss: 0.069968 (Summen av L1 og STFT), L1 Loss: 0.691790 (Amplitudetap), STFT Loss: 1.802544 (Spektraltap)
2025-01-18 18:36:15,784 - INFO - [Batch] Epoch: 27/40, Batch: 11/25, Combined Loss: 0.101605 (Summen av L1 og STFT), L1 Loss: 1.092365 (Amplitudetap), STFT Loss: 2.579995 (Spektraltap)
2025-01-18 18:37:00,061 - INFO - [Batch] Epoch: 27/40, Batch: 12/25, Combined Loss: 0.084311 (Summen av L1 og STFT), L1 Loss: 1.129352 (Amplitudetap), STFT Loss: 2.045309 (Spektraltap)
2025-01-18 18:37:44,388 - INFO - [Batch] Epoch: 27/40, Batch: 13/25, Combined Loss: 0.131450 (Summen av L1 og STFT), L1 Loss: 1.388190 (Amplitudetap), STFT Loss: 3.348560 (Spektraltap)
2025-01-18 18:38:35,754 - INFO - [Batch] Epoch: 27/40, Batch: 14/25, Combined Loss: 0.204189 (Summen av L1 og STFT), L1 Loss: 2.442626 (Amplitudetap), STFT Loss: 5.078825 (Spektraltap)
2025-01-18 18:39:31,731 - INFO - [Batch] Epoch: 27/40, Batch: 15/25, Combined Loss: 0.064455 (Summen av L1 og STFT), L1 Loss: 0.732491 (Amplitudetap), STFT Loss: 1.619735 (Spektraltap)
2025-01-18 18:40:21,577 - INFO - [Batch] Epoch: 27/40, Batch: 16/25, Combined Loss: 0.065656 (Summen av L1 og STFT), L1 Loss: 0.693369 (Amplitudetap), STFT Loss: 1.672518 (Spektraltap)
2025-01-18 18:41:17,369 - INFO - [Batch] Epoch: 27/40, Batch: 17/25, Combined Loss: 0.084038 (Summen av L1 og STFT), L1 Loss: 0.859205 (Amplitudetap), STFT Loss: 2.152910 (Spektraltap)
2025-01-18 18:42:06,336 - INFO - [Batch] Epoch: 27/40, Batch: 18/25, Combined Loss: 0.040171 (Summen av L1 og STFT), L1 Loss: 0.434336 (Amplitudetap), STFT Loss: 1.018999 (Spektraltap)
2025-01-18 18:42:52,407 - INFO - [Batch] Epoch: 27/40, Batch: 19/25, Combined Loss: 0.071769 (Summen av L1 og STFT), L1 Loss: 0.879650 (Amplitudetap), STFT Loss: 1.776087 (Spektraltap)
2025-01-18 18:43:36,528 - INFO - [Batch] Epoch: 27/40, Batch: 20/25, Combined Loss: 0.081266 (Summen av L1 og STFT), L1 Loss: 0.885240 (Amplitudetap), STFT Loss: 2.058602 (Spektraltap)
2025-01-18 18:44:28,309 - INFO - [Batch] Epoch: 27/40, Batch: 21/25, Combined Loss: 0.096369 (Summen av L1 og STFT), L1 Loss: 1.043088 (Amplitudetap), STFT Loss: 2.444025 (Spektraltap)
2025-01-18 18:44:28,324 - INFO - Epoch [27/40], Batch [21/25], Gradient Norm: 0.0826
2025-01-18 18:45:21,278 - INFO - [Batch] Epoch: 27/40, Batch: 22/25, Combined Loss: 0.102899 (Summen av L1 og STFT), L1 Loss: 1.430871 (Amplitudetap), STFT Loss: 2.473745 (Spektraltap)
2025-01-18 18:46:18,596 - INFO - [Batch] Epoch: 27/40, Batch: 23/25, Combined Loss: 0.106599 (Summen av L1 og STFT), L1 Loss: 1.304538 (Amplitudetap), STFT Loss: 2.638870 (Spektraltap)
2025-01-18 18:47:04,838 - INFO - [Batch] Epoch: 27/40, Batch: 24/25, Combined Loss: 0.102253 (Summen av L1 og STFT), L1 Loss: 1.357461 (Amplitudetap), STFT Loss: 2.485813 (Spektraltap)
2025-01-18 18:48:06,561 - INFO - [Batch] Epoch: 27/40, Batch: 25/25, Combined Loss: 0.075069 (Summen av L1 og STFT), L1 Loss: 0.881197 (Amplitudetap), STFT Loss: 1.874426 (Spektraltap)
2025-01-18 18:48:06,561 - INFO - Epoch [27/40], Batch [25/25], Gradient Norm: 0.0432
2025-01-18 18:48:06,561 - INFO - Current Learning Rate 1.25e-08
2025-01-18 18:48:06,561 - INFO - [Epoch Improvement] Epoch 27: Loss improved by 0.00% from previous epoch.
2025-01-18 18:48:06,561 - INFO - [Epoch Summary] Avg Training Loss: 0.084284
2025-01-18 18:48:06,561 - INFO - [Train] No improvement in loss for epoch 27 with loss: 0.08428449600934983. Best loss remains 0.084274. Trigger_times: 2
2025-01-18 18:56:27,141 - INFO - [Validation] Epoch 27/40, [Validation] Avg Validation Loss:  1.078863
2025-01-18 18:56:27,141 - INFO - [Epoch Summary] Epoch: 27/40, Avg Combined Loss: 0.084284, Avg L1 Loss: 0.980630, Avg STFT Loss: 2.108601
2025-01-18 18:56:27,141 - INFO - Previous epoch loss: 0.08428449600934983
2025-01-18 18:56:27,141 - INFO - [Train] Epoch 28/40 started.
2025-01-18 18:57:22,172 - INFO - [Batch] Epoch: 28/40, Batch: 1/25, Combined Loss: 0.115452 (Summen av L1 og STFT), L1 Loss: 1.336885 (Amplitudetap), STFT Loss: 2.890598 (Spektraltap)
2025-01-18 18:58:12,792 - INFO - [Batch] Epoch: 28/40, Batch: 2/25, Combined Loss: 0.082833 (Summen av L1 og STFT), L1 Loss: 0.929779 (Amplitudetap), STFT Loss: 2.086510 (Spektraltap)
2025-01-18 18:59:07,396 - INFO - [Batch] Epoch: 28/40, Batch: 3/25, Combined Loss: 0.159503 (Summen av L1 og STFT), L1 Loss: 2.032601 (Amplitudetap), STFT Loss: 3.913978 (Spektraltap)
2025-01-18 18:59:59,988 - INFO - [Batch] Epoch: 28/40, Batch: 4/25, Combined Loss: 0.079141 (Summen av L1 og STFT), L1 Loss: 0.829657 (Amplitudetap), STFT Loss: 2.018669 (Spektraltap)
2025-01-18 19:00:57,754 - INFO - [Batch] Epoch: 28/40, Batch: 5/25, Combined Loss: 0.036902 (Summen av L1 og STFT), L1 Loss: 0.273240 (Amplitudetap), STFT Loss: 0.989972 (Spektraltap)
2025-01-18 19:01:48,449 - INFO - [Batch] Epoch: 28/40, Batch: 6/25, Combined Loss: 0.120384 (Summen av L1 og STFT), L1 Loss: 1.756996 (Amplitudetap), STFT Loss: 2.858508 (Spektraltap)
2025-01-18 19:02:24,345 - INFO - [Batch] Epoch: 28/40, Batch: 7/25, Combined Loss: 0.127383 (Summen av L1 og STFT), L1 Loss: 1.440192 (Amplitudetap), STFT Loss: 3.204263 (Spektraltap)
2025-01-18 19:03:09,060 - INFO - [Batch] Epoch: 28/40, Batch: 8/25, Combined Loss: 0.115399 (Summen av L1 og STFT), L1 Loss: 1.325976 (Amplitudetap), STFT Loss: 2.893691 (Spektraltap)
2025-01-18 19:03:46,451 - INFO - [Batch] Epoch: 28/40, Batch: 9/25, Combined Loss: 0.096185 (Summen av L1 og STFT), L1 Loss: 1.025986 (Amplitudetap), STFT Loss: 2.445832 (Spektraltap)
2025-01-18 19:04:39,316 - INFO - [Batch] Epoch: 28/40, Batch: 10/25, Combined Loss: 0.071341 (Summen av L1 og STFT), L1 Loss: 0.820906 (Amplitudetap), STFT Loss: 1.788422 (Spektraltap)
2025-01-18 19:05:29,523 - INFO - [Batch] Epoch: 28/40, Batch: 11/25, Combined Loss: 0.030895 (Summen av L1 og STFT), L1 Loss: 0.214870 (Amplitudetap), STFT Loss: 0.834772 (Spektraltap)
2025-01-18 19:06:20,183 - INFO - [Batch] Epoch: 28/40, Batch: 12/25, Combined Loss: 0.125118 (Summen av L1 og STFT), L1 Loss: 1.592408 (Amplitudetap), STFT Loss: 3.071072 (Spektraltap)
2025-01-18 19:07:08,203 - INFO - [Batch] Epoch: 28/40, Batch: 13/25, Combined Loss: 0.073873 (Summen av L1 og STFT), L1 Loss: 0.799943 (Amplitudetap), STFT Loss: 1.873363 (Spektraltap)
2025-01-18 19:07:54,444 - INFO - [Batch] Epoch: 28/40, Batch: 14/25, Combined Loss: 0.079944 (Summen av L1 og STFT), L1 Loss: 0.889340 (Amplitudetap), STFT Loss: 2.017186 (Spektraltap)
2025-01-18 19:08:45,810 - INFO - [Batch] Epoch: 28/40, Batch: 15/25, Combined Loss: 0.013003 (Summen av L1 og STFT), L1 Loss: 0.105530 (Amplitudetap), STFT Loss: 0.344854 (Spektraltap)
2025-01-18 19:09:36,855 - INFO - [Batch] Epoch: 28/40, Batch: 16/25, Combined Loss: 0.100297 (Summen av L1 og STFT), L1 Loss: 1.174962 (Amplitudetap), STFT Loss: 2.505352 (Spektraltap)
2025-01-18 19:10:43,399 - INFO - [Batch] Epoch: 28/40, Batch: 17/25, Combined Loss: 0.089738 (Summen av L1 og STFT), L1 Loss: 1.249890 (Amplitudetap), STFT Loss: 2.156468 (Spektraltap)
2025-01-18 19:11:33,672 - INFO - [Batch] Epoch: 28/40, Batch: 18/25, Combined Loss: 0.065607 (Summen av L1 og STFT), L1 Loss: 0.708680 (Amplitudetap), STFT Loss: 1.664492 (Spektraltap)
2025-01-18 19:12:19,077 - INFO - [Batch] Epoch: 28/40, Batch: 19/25, Combined Loss: 0.065967 (Summen av L1 og STFT), L1 Loss: 0.944507 (Amplitudetap), STFT Loss: 1.574232 (Spektraltap)
2025-01-18 19:13:10,414 - INFO - [Batch] Epoch: 28/40, Batch: 20/25, Combined Loss: 0.162336 (Summen av L1 og STFT), L1 Loss: 1.993980 (Amplitudetap), STFT Loss: 4.015502 (Spektraltap)
2025-01-18 19:14:10,069 - INFO - [Batch] Epoch: 28/40, Batch: 21/25, Combined Loss: 0.093906 (Summen av L1 og STFT), L1 Loss: 0.981736 (Amplitudetap), STFT Loss: 2.396445 (Spektraltap)
2025-01-18 19:14:10,069 - INFO - Epoch [28/40], Batch [21/25], Gradient Norm: 0.0508
2025-01-18 19:14:52,768 - INFO - [Batch] Epoch: 28/40, Batch: 22/25, Combined Loss: 0.062669 (Summen av L1 og STFT), L1 Loss: 0.618917 (Amplitudetap), STFT Loss: 1.614820 (Spektraltap)
2025-01-18 19:15:43,262 - INFO - [Batch] Epoch: 28/40, Batch: 23/25, Combined Loss: 0.083650 (Summen av L1 og STFT), L1 Loss: 0.927556 (Amplitudetap), STFT Loss: 2.111962 (Spektraltap)
2025-01-18 19:16:32,331 - INFO - [Batch] Epoch: 28/40, Batch: 24/25, Combined Loss: 0.029650 (Summen av L1 og STFT), L1 Loss: 0.255437 (Amplitudetap), STFT Loss: 0.780038 (Spektraltap)
2025-01-18 19:17:27,869 - INFO - [Batch] Epoch: 28/40, Batch: 25/25, Combined Loss: 0.025822 (Summen av L1 og STFT), L1 Loss: 0.279304 (Amplitudetap), STFT Loss: 0.654972 (Spektraltap)
2025-01-18 19:17:27,885 - INFO - Epoch [28/40], Batch [25/25], Gradient Norm: 0.0349
2025-01-18 19:17:27,912 - INFO - Current Learning Rate 1.25e-08
2025-01-18 19:17:27,912 - INFO - [Epoch Improvement] Epoch 28: Loss improved by 0.00% from previous epoch.
2025-01-18 19:17:27,912 - INFO - [Epoch Summary] Avg Training Loss: 0.084280
2025-01-18 19:17:27,912 - INFO - [Train] No improvement in loss for epoch 28 with loss: 0.0842799361795187. Best loss remains 0.084274. Trigger_times: 3
2025-01-18 19:25:57,136 - INFO - [Validation] Epoch 28/40, [Validation] Avg Validation Loss:  1.078879
2025-01-18 19:25:57,136 - INFO - [Epoch Summary] Epoch: 28/40, Avg Combined Loss: 0.084280, Avg L1 Loss: 0.980621, Avg STFT Loss: 2.108588
2025-01-18 19:25:57,136 - INFO - Previous epoch loss: 0.0842799361795187
2025-01-18 19:25:57,136 - INFO - [Train] Epoch 29/40 started.
2025-01-18 19:26:47,022 - INFO - [Batch] Epoch: 29/40, Batch: 1/25, Combined Loss: 0.057351 (Summen av L1 og STFT), L1 Loss: 0.517145 (Amplitudetap), STFT Loss: 1.498905 (Spektraltap)
2025-01-18 19:27:43,313 - INFO - [Batch] Epoch: 29/40, Batch: 2/25, Combined Loss: 0.116865 (Summen av L1 og STFT), L1 Loss: 1.360171 (Amplitudetap), STFT Loss: 2.923030 (Spektraltap)
2025-01-18 19:28:35,178 - INFO - [Batch] Epoch: 29/40, Batch: 3/25, Combined Loss: 0.149514 (Summen av L1 og STFT), L1 Loss: 1.738513 (Amplitudetap), STFT Loss: 3.740353 (Spektraltap)
2025-01-18 19:29:22,320 - INFO - [Batch] Epoch: 29/40, Batch: 4/25, Combined Loss: 0.125452 (Summen av L1 og STFT), L1 Loss: 1.724707 (Amplitudetap), STFT Loss: 3.024389 (Spektraltap)
2025-01-18 19:30:10,358 - INFO - [Batch] Epoch: 29/40, Batch: 5/25, Combined Loss: 0.021587 (Summen av L1 og STFT), L1 Loss: 0.348613 (Amplitudetap), STFT Loss: 0.498208 (Spektraltap)
2025-01-18 19:30:56,754 - INFO - [Batch] Epoch: 29/40, Batch: 6/25, Combined Loss: 0.151970 (Summen av L1 og STFT), L1 Loss: 1.688756 (Amplitudetap), STFT Loss: 3.835361 (Spektraltap)
2025-01-18 19:31:37,659 - INFO - [Batch] Epoch: 29/40, Batch: 7/25, Combined Loss: 0.115464 (Summen av L1 og STFT), L1 Loss: 1.376401 (Amplitudetap), STFT Loss: 2.874033 (Spektraltap)
2025-01-18 19:32:20,041 - INFO - [Batch] Epoch: 29/40, Batch: 8/25, Combined Loss: 0.088856 (Summen av L1 og STFT), L1 Loss: 0.944891 (Amplitudetap), STFT Loss: 2.260722 (Spektraltap)
2025-01-18 19:33:13,691 - INFO - [Batch] Epoch: 29/40, Batch: 9/25, Combined Loss: 0.028463 (Summen av L1 og STFT), L1 Loss: 0.296861 (Amplitudetap), STFT Loss: 0.726674 (Spektraltap)
2025-01-18 19:34:00,361 - INFO - [Batch] Epoch: 29/40, Batch: 10/25, Combined Loss: 0.048309 (Summen av L1 og STFT), L1 Loss: 0.468333 (Amplitudetap), STFT Loss: 1.248566 (Spektraltap)
2025-01-18 19:34:57,565 - INFO - [Batch] Epoch: 29/40, Batch: 11/25, Combined Loss: 0.106944 (Summen av L1 og STFT), L1 Loss: 1.192655 (Amplitudetap), STFT Loss: 2.697177 (Spektraltap)
2025-01-18 19:35:43,362 - INFO - [Batch] Epoch: 29/40, Batch: 12/25, Combined Loss: 0.111800 (Summen av L1 og STFT), L1 Loss: 1.309014 (Amplitudetap), STFT Loss: 2.793006 (Spektraltap)
2025-01-18 19:36:34,839 - INFO - [Batch] Epoch: 29/40, Batch: 13/25, Combined Loss: 0.093855 (Summen av L1 og STFT), L1 Loss: 0.957557 (Amplitudetap), STFT Loss: 2.405267 (Spektraltap)
2025-01-18 19:37:20,176 - INFO - [Batch] Epoch: 29/40, Batch: 14/25, Combined Loss: 0.058883 (Summen av L1 og STFT), L1 Loss: 0.792944 (Amplitudetap), STFT Loss: 1.426669 (Spektraltap)
2025-01-18 19:38:21,599 - INFO - [Batch] Epoch: 29/40, Batch: 15/25, Combined Loss: 0.126241 (Summen av L1 og STFT), L1 Loss: 1.433078 (Amplitudetap), STFT Loss: 3.173046 (Spektraltap)
2025-01-18 19:39:14,243 - INFO - [Batch] Epoch: 29/40, Batch: 16/25, Combined Loss: 0.127796 (Summen av L1 og STFT), L1 Loss: 1.537252 (Amplitudetap), STFT Loss: 3.175049 (Spektraltap)
2025-01-18 19:40:10,545 - INFO - [Batch] Epoch: 29/40, Batch: 17/25, Combined Loss: 0.066720 (Summen av L1 og STFT), L1 Loss: 0.888464 (Amplitudetap), STFT Loss: 1.620843 (Spektraltap)
2025-01-18 19:41:00,276 - INFO - [Batch] Epoch: 29/40, Batch: 18/25, Combined Loss: 0.075112 (Summen av L1 og STFT), L1 Loss: 0.977002 (Amplitudetap), STFT Loss: 1.834637 (Spektraltap)
2025-01-18 19:41:42,555 - INFO - [Batch] Epoch: 29/40, Batch: 19/25, Combined Loss: 0.113711 (Summen av L1 og STFT), L1 Loss: 1.232935 (Amplitudetap), STFT Loss: 2.882927 (Spektraltap)
2025-01-18 19:42:30,137 - INFO - [Batch] Epoch: 29/40, Batch: 20/25, Combined Loss: 0.025156 (Summen av L1 og STFT), L1 Loss: 0.223165 (Amplitudetap), STFT Loss: 0.659050 (Spektraltap)
2025-01-18 19:43:27,983 - INFO - [Batch] Epoch: 29/40, Batch: 21/25, Combined Loss: 0.102164 (Summen av L1 og STFT), L1 Loss: 1.284936 (Amplitudetap), STFT Loss: 2.514225 (Spektraltap)
2025-01-18 19:43:27,983 - INFO - Epoch [29/40], Batch [21/25], Gradient Norm: 0.0428
2025-01-18 19:44:28,208 - INFO - [Batch] Epoch: 29/40, Batch: 22/25, Combined Loss: 0.063560 (Summen av L1 og STFT), L1 Loss: 0.854983 (Amplitudetap), STFT Loss: 1.540381 (Spektraltap)
2025-01-18 19:45:24,249 - INFO - [Batch] Epoch: 29/40, Batch: 23/25, Combined Loss: 0.015120 (Summen av L1 og STFT), L1 Loss: 0.097221 (Amplitudetap), STFT Loss: 0.411930 (Spektraltap)
2025-01-18 19:46:16,827 - INFO - [Batch] Epoch: 29/40, Batch: 24/25, Combined Loss: 0.064862 (Summen av L1 og STFT), L1 Loss: 0.738532 (Amplitudetap), STFT Loss: 1.629346 (Spektraltap)
2025-01-18 19:47:05,199 - INFO - [Batch] Epoch: 29/40, Batch: 25/25, Combined Loss: 0.050771 (Summen av L1 og STFT), L1 Loss: 0.533473 (Amplitudetap), STFT Loss: 1.294502 (Spektraltap)
2025-01-18 19:47:05,199 - INFO - Epoch [29/40], Batch [25/25], Gradient Norm: 0.0496
2025-01-18 19:47:05,215 - INFO - Current Learning Rate 1.25e-08
2025-01-18 19:47:05,215 - INFO - [Epoch Improvement] Epoch 29: Loss improved by 0.00% from previous epoch.
2025-01-18 19:47:05,215 - INFO - [Epoch Summary] Avg Training Loss: 0.084261
2025-01-18 19:47:05,324 - INFO - [Train] New best model saved at C:\Users\didri\Desktop\UNet Models\UNet_vocal_isolation_model\Model_weights\CheckPoints\best_model_epoch-28.pth with loss 0.084261
2025-01-18 19:55:46,300 - INFO - [Validation] Epoch 29/40, [Validation] Avg Validation Loss:  1.078886
2025-01-18 19:55:46,300 - INFO - [Epoch Summary] Epoch: 29/40, Avg Combined Loss: 0.084261, Avg L1 Loss: 0.980624, Avg STFT Loss: 2.108552
2025-01-18 19:55:46,300 - INFO - Previous epoch loss: 0.08426112554967403
2025-01-18 19:55:46,300 - INFO - [Train] Epoch 30/40 started.
2025-01-18 19:56:43,447 - INFO - [Batch] Epoch: 30/40, Batch: 1/25, Combined Loss: 0.030598 (Summen av L1 og STFT), L1 Loss: 0.287590 (Amplitudetap), STFT Loss: 0.794693 (Spektraltap)
2025-01-18 19:57:40,812 - INFO - [Batch] Epoch: 30/40, Batch: 2/25, Combined Loss: 0.167518 (Summen av L1 og STFT), L1 Loss: 2.215565 (Amplitudetap), STFT Loss: 4.076019 (Spektraltap)
2025-01-18 19:58:24,593 - INFO - [Batch] Epoch: 30/40, Batch: 3/25, Combined Loss: 0.089710 (Summen av L1 og STFT), L1 Loss: 1.166656 (Amplitudetap), STFT Loss: 2.191319 (Spektraltap)
2025-01-18 19:59:09,031 - INFO - [Batch] Epoch: 30/40, Batch: 4/25, Combined Loss: 0.088579 (Summen av L1 og STFT), L1 Loss: 0.986057 (Amplitudetap), STFT Loss: 2.234773 (Spektraltap)
2025-01-18 20:00:02,016 - INFO - [Batch] Epoch: 30/40, Batch: 5/25, Combined Loss: 0.099643 (Summen av L1 og STFT), L1 Loss: 1.050835 (Amplitudetap), STFT Loss: 2.538934 (Spektraltap)
2025-01-18 20:00:40,605 - INFO - [Batch] Epoch: 30/40, Batch: 6/25, Combined Loss: 0.091617 (Summen av L1 og STFT), L1 Loss: 1.083580 (Amplitudetap), STFT Loss: 2.284128 (Spektraltap)
2025-01-18 20:01:27,783 - INFO - [Batch] Epoch: 30/40, Batch: 7/25, Combined Loss: 0.043273 (Summen av L1 og STFT), L1 Loss: 0.346375 (Amplitudetap), STFT Loss: 1.149730 (Spektraltap)
2025-01-18 20:02:22,406 - INFO - [Batch] Epoch: 30/40, Batch: 8/25, Combined Loss: 0.173411 (Summen av L1 og STFT), L1 Loss: 2.019374 (Amplitudetap), STFT Loss: 4.336872 (Spektraltap)
2025-01-18 20:03:12,987 - INFO - [Batch] Epoch: 30/40, Batch: 9/25, Combined Loss: 0.109198 (Summen av L1 og STFT), L1 Loss: 1.191378 (Amplitudetap), STFT Loss: 2.765346 (Spektraltap)
2025-01-18 20:04:01,310 - INFO - [Batch] Epoch: 30/40, Batch: 10/25, Combined Loss: 0.095163 (Summen av L1 og STFT), L1 Loss: 1.003823 (Amplitudetap), STFT Loss: 2.424677 (Spektraltap)
2025-01-18 20:04:49,814 - INFO - [Batch] Epoch: 30/40, Batch: 11/25, Combined Loss: 0.089524 (Summen av L1 og STFT), L1 Loss: 1.225887 (Amplitudetap), STFT Loss: 2.160329 (Spektraltap)
2025-01-18 20:05:43,963 - INFO - [Batch] Epoch: 30/40, Batch: 12/25, Combined Loss: 0.079680 (Summen av L1 og STFT), L1 Loss: 0.926077 (Amplitudetap), STFT Loss: 1.993510 (Spektraltap)
2025-01-18 20:06:41,244 - INFO - [Batch] Epoch: 30/40, Batch: 13/25, Combined Loss: 0.115529 (Summen av L1 og STFT), L1 Loss: 1.499205 (Amplitudetap), STFT Loss: 2.823367 (Spektraltap)
2025-01-18 20:07:42,451 - INFO - [Batch] Epoch: 30/40, Batch: 14/25, Combined Loss: 0.060974 (Summen av L1 og STFT), L1 Loss: 0.654500 (Amplitudetap), STFT Loss: 1.548715 (Spektraltap)
2025-01-18 20:08:23,504 - INFO - [Batch] Epoch: 30/40, Batch: 15/25, Combined Loss: 0.057217 (Summen av L1 og STFT), L1 Loss: 0.604346 (Amplitudetap), STFT Loss: 1.457498 (Spektraltap)
2025-01-18 20:09:16,971 - INFO - [Batch] Epoch: 30/40, Batch: 16/25, Combined Loss: 0.070168 (Summen av L1 og STFT), L1 Loss: 0.765329 (Amplitudetap), STFT Loss: 1.777049 (Spektraltap)
2025-01-18 20:10:09,088 - INFO - [Batch] Epoch: 30/40, Batch: 17/25, Combined Loss: 0.090317 (Summen av L1 og STFT), L1 Loss: 1.012903 (Amplitudetap), STFT Loss: 2.275414 (Spektraltap)
2025-01-18 20:10:52,157 - INFO - [Batch] Epoch: 30/40, Batch: 18/25, Combined Loss: 0.031452 (Summen av L1 og STFT), L1 Loss: 0.271941 (Amplitudetap), STFT Loss: 0.827021 (Spektraltap)
2025-01-18 20:11:54,651 - INFO - [Batch] Epoch: 30/40, Batch: 19/25, Combined Loss: 0.149021 (Summen av L1 og STFT), L1 Loss: 1.851490 (Amplitudetap), STFT Loss: 3.677130 (Spektraltap)
2025-01-18 20:12:38,733 - INFO - [Batch] Epoch: 30/40, Batch: 20/25, Combined Loss: 0.031567 (Summen av L1 og STFT), L1 Loss: 0.273225 (Amplitudetap), STFT Loss: 0.829910 (Spektraltap)
2025-01-18 20:13:26,121 - INFO - [Batch] Epoch: 30/40, Batch: 21/25, Combined Loss: 0.030259 (Summen av L1 og STFT), L1 Loss: 0.293416 (Amplitudetap), STFT Loss: 0.782028 (Spektraltap)
2025-01-18 20:13:26,121 - INFO - Epoch [30/40], Batch [21/25], Gradient Norm: 0.0373
2025-01-18 20:14:22,688 - INFO - [Batch] Epoch: 30/40, Batch: 22/25, Combined Loss: 0.034237 (Summen av L1 og STFT), L1 Loss: 0.271233 (Amplitudetap), STFT Loss: 0.910877 (Spektraltap)
2025-01-18 20:15:09,616 - INFO - [Batch] Epoch: 30/40, Batch: 23/25, Combined Loss: 0.125393 (Summen av L1 og STFT), L1 Loss: 1.609064 (Amplitudetap), STFT Loss: 3.072203 (Spektraltap)
2025-01-18 20:15:53,399 - INFO - [Batch] Epoch: 30/40, Batch: 24/25, Combined Loss: 0.066061 (Summen av L1 og STFT), L1 Loss: 0.722119 (Amplitudetap), STFT Loss: 1.672358 (Spektraltap)
2025-01-18 20:16:43,642 - INFO - [Batch] Epoch: 30/40, Batch: 25/25, Combined Loss: 0.086853 (Summen av L1 og STFT), L1 Loss: 1.182751 (Amplitudetap), STFT Loss: 2.098687 (Spektraltap)
2025-01-18 20:16:43,667 - INFO - Epoch [30/40], Batch [25/25], Gradient Norm: 0.0389
2025-01-18 20:16:43,699 - INFO - Current Learning Rate 1.25e-08
2025-01-18 20:16:43,699 - INFO - [Epoch Improvement] Epoch 30: Loss improved by 0.00% from previous epoch.
2025-01-18 20:16:43,699 - INFO - [Epoch Summary] Avg Training Loss: 0.084279
2025-01-18 20:16:43,699 - INFO - [Train] No improvement in loss for epoch 30 with loss: 0.08427852995693684. Best loss remains 0.084261. Trigger_times: 0
2025-01-18 20:25:11,482 - INFO - [Validation] Epoch 30/40, [Validation] Avg Validation Loss:  1.078886
2025-01-18 20:25:11,482 - INFO - [Memory After Epoch 30] Timestamp: 2025-01-18 20:25:11
2025-01-18 20:25:11,482 - INFO - [Memory After Epoch 30] GPU 0: NVIDIA GeForce RTX 3060, Total=12.88 GB, Allocated=0.51 GB, Cached=23.26 GB
2025-01-18 20:25:11,482 - INFO - [Memory After Epoch 30] CPU Memory Usage: ~0.57 GB
2025-01-18 20:25:11,482 - INFO - [Epoch Summary] Epoch: 30/40, Avg Combined Loss: 0.084279, Avg L1 Loss: 0.980623, Avg STFT Loss: 2.108537
2025-01-18 20:25:11,482 - INFO - Previous epoch loss: 0.08427852995693684
2025-01-18 20:25:11,482 - INFO - [Train] Epoch 31/40 started.
2025-01-18 20:25:56,628 - INFO - [Batch] Epoch: 31/40, Batch: 1/25, Combined Loss: 0.021001 (Summen av L1 og STFT), L1 Loss: 0.150532 (Amplitudetap), STFT Loss: 0.565505 (Spektraltap)
2025-01-18 20:26:46,785 - INFO - [Batch] Epoch: 31/40, Batch: 2/25, Combined Loss: 0.086353 (Summen av L1 og STFT), L1 Loss: 1.180746 (Amplitudetap), STFT Loss: 2.084564 (Spektraltap)
2025-01-18 20:27:35,682 - INFO - [Batch] Epoch: 31/40, Batch: 3/25, Combined Loss: 0.051696 (Summen av L1 og STFT), L1 Loss: 0.482516 (Amplitudetap), STFT Loss: 1.344085 (Spektraltap)
2025-01-18 20:28:29,247 - INFO - [Batch] Epoch: 31/40, Batch: 4/25, Combined Loss: 0.110767 (Summen av L1 og STFT), L1 Loss: 1.324160 (Amplitudetap), STFT Loss: 2.755499 (Spektraltap)
2025-01-18 20:29:20,656 - INFO - [Batch] Epoch: 31/40, Batch: 5/25, Combined Loss: 0.027875 (Summen av L1 og STFT), L1 Loss: 0.187103 (Amplitudetap), STFT Loss: 0.756051 (Spektraltap)
2025-01-18 20:30:10,386 - INFO - [Batch] Epoch: 31/40, Batch: 6/25, Combined Loss: 0.228122 (Summen av L1 og STFT), L1 Loss: 2.719171 (Amplitudetap), STFT Loss: 5.678310 (Spektraltap)
2025-01-18 20:31:09,076 - INFO - [Batch] Epoch: 31/40, Batch: 7/25, Combined Loss: 0.044338 (Summen av L1 og STFT), L1 Loss: 0.400477 (Amplitudetap), STFT Loss: 1.158514 (Spektraltap)
2025-01-18 20:32:06,109 - INFO - [Batch] Epoch: 31/40, Batch: 8/25, Combined Loss: 0.156098 (Summen av L1 og STFT), L1 Loss: 1.923132 (Amplitudetap), STFT Loss: 3.858753 (Spektraltap)
2025-01-18 20:32:51,768 - INFO - [Batch] Epoch: 31/40, Batch: 9/25, Combined Loss: 0.061690 (Summen av L1 og STFT), L1 Loss: 0.644017 (Amplitudetap), STFT Loss: 1.574704 (Spektraltap)
2025-01-18 20:33:36,976 - INFO - [Batch] Epoch: 31/40, Batch: 10/25, Combined Loss: 0.196548 (Summen av L1 og STFT), L1 Loss: 2.363762 (Amplitudetap), STFT Loss: 4.883405 (Spektraltap)
2025-01-18 20:34:27,996 - INFO - [Batch] Epoch: 31/40, Batch: 11/25, Combined Loss: 0.020941 (Summen av L1 og STFT), L1 Loss: 0.207791 (Amplitudetap), STFT Loss: 0.539172 (Spektraltap)
2025-01-18 20:35:11,764 - INFO - [Batch] Epoch: 31/40, Batch: 12/25, Combined Loss: 0.056152 (Summen av L1 og STFT), L1 Loss: 0.810923 (Amplitudetap), STFT Loss: 1.337019 (Spektraltap)
2025-01-18 20:36:11,339 - INFO - [Batch] Epoch: 31/40, Batch: 13/25, Combined Loss: 0.068639 (Summen av L1 og STFT), L1 Loss: 0.803148 (Amplitudetap), STFT Loss: 1.714966 (Spektraltap)
2025-01-18 20:37:02,683 - INFO - [Batch] Epoch: 31/40, Batch: 14/25, Combined Loss: 0.087895 (Summen av L1 og STFT), L1 Loss: 0.953754 (Amplitudetap), STFT Loss: 2.228097 (Spektraltap)
2025-01-18 20:37:44,735 - INFO - [Batch] Epoch: 31/40, Batch: 15/25, Combined Loss: 0.087749 (Summen av L1 og STFT), L1 Loss: 1.044546 (Amplitudetap), STFT Loss: 2.184815 (Spektraltap)
2025-01-18 20:38:42,785 - INFO - [Batch] Epoch: 31/40, Batch: 16/25, Combined Loss: 0.182253 (Summen av L1 og STFT), L1 Loss: 2.428947 (Amplitudetap), STFT Loss: 4.426618 (Spektraltap)
2025-01-18 20:39:37,373 - INFO - [Batch] Epoch: 31/40, Batch: 17/25, Combined Loss: 0.070145 (Summen av L1 og STFT), L1 Loss: 0.793834 (Amplitudetap), STFT Loss: 1.764128 (Spektraltap)
2025-01-18 20:40:23,124 - INFO - [Batch] Epoch: 31/40, Batch: 18/25, Combined Loss: 0.039864 (Summen av L1 og STFT), L1 Loss: 0.368337 (Amplitudetap), STFT Loss: 1.038072 (Spektraltap)
2025-01-18 20:41:11,125 - INFO - [Batch] Epoch: 31/40, Batch: 19/25, Combined Loss: 0.098948 (Summen av L1 og STFT), L1 Loss: 1.093930 (Amplitudetap), STFT Loss: 2.499606 (Spektraltap)
2025-01-18 20:41:58,825 - INFO - [Batch] Epoch: 31/40, Batch: 20/25, Combined Loss: 0.033771 (Summen av L1 og STFT), L1 Loss: 0.344914 (Amplitudetap), STFT Loss: 0.865323 (Spektraltap)
2025-01-18 20:42:44,717 - INFO - [Batch] Epoch: 31/40, Batch: 21/25, Combined Loss: 0.079281 (Summen av L1 og STFT), L1 Loss: 1.001406 (Amplitudetap), STFT Loss: 1.949249 (Spektraltap)
2025-01-18 20:42:44,717 - INFO - Epoch [31/40], Batch [21/25], Gradient Norm: 0.0670
2025-01-18 20:43:38,638 - INFO - [Batch] Epoch: 31/40, Batch: 22/25, Combined Loss: 0.044644 (Summen av L1 og STFT), L1 Loss: 0.390074 (Amplitudetap), STFT Loss: 1.172157 (Spektraltap)
2025-01-18 20:44:27,893 - INFO - [Batch] Epoch: 31/40, Batch: 23/25, Combined Loss: 0.144118 (Summen av L1 og STFT), L1 Loss: 1.727279 (Amplitudetap), STFT Loss: 3.583267 (Spektraltap)
2025-01-18 20:45:18,429 - INFO - [Batch] Epoch: 31/40, Batch: 24/25, Combined Loss: 0.034112 (Summen av L1 og STFT), L1 Loss: 0.306719 (Amplitudetap), STFT Loss: 0.891923 (Spektraltap)
2025-01-18 20:46:06,616 - INFO - [Batch] Epoch: 31/40, Batch: 25/25, Combined Loss: 0.073819 (Summen av L1 og STFT), L1 Loss: 0.860974 (Amplitudetap), STFT Loss: 1.845593 (Spektraltap)
2025-01-18 20:46:06,616 - INFO - Epoch [31/40], Batch [25/25], Gradient Norm: 0.0370
2025-01-18 20:46:06,632 - INFO - Current Learning Rate 1.25e-08
2025-01-18 20:46:06,632 - INFO - [Epoch Improvement] Epoch 31: Loss improved by 0.00% from previous epoch.
2025-01-18 20:46:06,632 - INFO - [Epoch Summary] Avg Training Loss: 0.084273
2025-01-18 20:46:06,632 - INFO - [Train] No improvement in loss for epoch 31 with loss: 0.08427282720804215. Best loss remains 0.084261. Trigger_times: 1
2025-01-18 20:54:46,505 - INFO - [Validation] Epoch 31/40, [Validation] Avg Validation Loss:  1.078887
2025-01-18 20:54:46,505 - INFO - [Epoch Summary] Epoch: 31/40, Avg Combined Loss: 0.084273, Avg L1 Loss: 0.980619, Avg STFT Loss: 2.108519
2025-01-18 20:54:46,505 - INFO - Previous epoch loss: 0.08427282720804215
2025-01-18 20:54:46,505 - INFO - [Train] Epoch 32/40 started.
2025-01-18 20:55:31,077 - INFO - [Batch] Epoch: 32/40, Batch: 1/25, Combined Loss: 0.081402 (Summen av L1 og STFT), L1 Loss: 1.011492 (Amplitudetap), STFT Loss: 2.008558 (Spektraltap)
2025-01-18 20:56:10,672 - INFO - [Batch] Epoch: 32/40, Batch: 2/25, Combined Loss: 0.054931 (Summen av L1 og STFT), L1 Loss: 0.759526 (Amplitudetap), STFT Loss: 1.322426 (Spektraltap)
2025-01-18 20:56:54,439 - INFO - [Batch] Epoch: 32/40, Batch: 3/25, Combined Loss: 0.069375 (Summen av L1 og STFT), L1 Loss: 0.778753 (Amplitudetap), STFT Loss: 1.747494 (Spektraltap)
2025-01-18 20:57:41,226 - INFO - [Batch] Epoch: 32/40, Batch: 4/25, Combined Loss: 0.078487 (Summen av L1 og STFT), L1 Loss: 0.950393 (Amplitudetap), STFT Loss: 1.947307 (Spektraltap)
2025-01-18 20:58:32,608 - INFO - [Batch] Epoch: 32/40, Batch: 5/25, Combined Loss: 0.068369 (Summen av L1 og STFT), L1 Loss: 0.785102 (Amplitudetap), STFT Loss: 1.714589 (Spektraltap)
2025-01-18 20:59:30,749 - INFO - [Batch] Epoch: 32/40, Batch: 6/25, Combined Loss: 0.120910 (Summen av L1 og STFT), L1 Loss: 1.646733 (Amplitudetap), STFT Loss: 2.921567 (Spektraltap)
2025-01-18 21:00:12,387 - INFO - [Batch] Epoch: 32/40, Batch: 7/25, Combined Loss: 0.048030 (Summen av L1 og STFT), L1 Loss: 0.610553 (Amplitudetap), STFT Loss: 1.179225 (Spektraltap)
2025-01-18 21:01:13,409 - INFO - [Batch] Epoch: 32/40, Batch: 8/25, Combined Loss: 0.220583 (Summen av L1 og STFT), L1 Loss: 2.655010 (Amplitudetap), STFT Loss: 5.479617 (Spektraltap)
2025-01-18 21:02:14,477 - INFO - [Batch] Epoch: 32/40, Batch: 9/25, Combined Loss: 0.179554 (Summen av L1 og STFT), L1 Loss: 2.221511 (Amplitudetap), STFT Loss: 4.434554 (Spektraltap)
2025-01-18 21:03:03,364 - INFO - [Batch] Epoch: 32/40, Batch: 10/25, Combined Loss: 0.039396 (Summen av L1 og STFT), L1 Loss: 0.437381 (Amplitudetap), STFT Loss: 0.994445 (Spektraltap)
2025-01-18 21:03:52,062 - INFO - [Batch] Epoch: 32/40, Batch: 11/25, Combined Loss: 0.040623 (Summen av L1 og STFT), L1 Loss: 0.440767 (Amplitudetap), STFT Loss: 1.029795 (Spektraltap)
2025-01-18 21:04:42,246 - INFO - [Batch] Epoch: 32/40, Batch: 12/25, Combined Loss: 0.034798 (Summen av L1 og STFT), L1 Loss: 0.198532 (Amplitudetap), STFT Loss: 0.958850 (Spektraltap)
2025-01-18 21:05:29,120 - INFO - [Batch] Epoch: 32/40, Batch: 13/25, Combined Loss: 0.073586 (Summen av L1 og STFT), L1 Loss: 0.787293 (Amplitudetap), STFT Loss: 1.870180 (Spektraltap)
2025-01-18 21:06:15,519 - INFO - [Batch] Epoch: 32/40, Batch: 14/25, Combined Loss: 0.124221 (Summen av L1 og STFT), L1 Loss: 1.462981 (Amplitudetap), STFT Loss: 3.099635 (Spektraltap)
2025-01-18 21:07:01,052 - INFO - [Batch] Epoch: 32/40, Batch: 15/25, Combined Loss: 0.069689 (Summen av L1 og STFT), L1 Loss: 0.983578 (Amplitudetap), STFT Loss: 1.669127 (Spektraltap)
2025-01-18 21:07:55,397 - INFO - [Batch] Epoch: 32/40, Batch: 16/25, Combined Loss: 0.033344 (Summen av L1 og STFT), L1 Loss: 0.180583 (Amplitudetap), STFT Loss: 0.922918 (Spektraltap)
2025-01-18 21:08:37,037 - INFO - [Batch] Epoch: 32/40, Batch: 17/25, Combined Loss: 0.110909 (Summen av L1 og STFT), L1 Loss: 1.284540 (Amplitudetap), STFT Loss: 2.776761 (Spektraltap)
2025-01-18 21:09:18,148 - INFO - [Batch] Epoch: 32/40, Batch: 18/25, Combined Loss: 0.077731 (Summen av L1 og STFT), L1 Loss: 0.753986 (Amplitudetap), STFT Loss: 2.008793 (Spektraltap)
2025-01-18 21:10:16,877 - INFO - [Batch] Epoch: 32/40, Batch: 19/25, Combined Loss: 0.011174 (Summen av L1 og STFT), L1 Loss: 0.114788 (Amplitudetap), STFT Loss: 0.286018 (Spektraltap)
2025-01-18 21:11:13,298 - INFO - [Batch] Epoch: 32/40, Batch: 20/25, Combined Loss: 0.036888 (Summen av L1 og STFT), L1 Loss: 0.366229 (Amplitudetap), STFT Loss: 0.949687 (Spektraltap)
2025-01-18 21:12:07,784 - INFO - [Batch] Epoch: 32/40, Batch: 21/25, Combined Loss: 0.142977 (Summen av L1 og STFT), L1 Loss: 1.714625 (Amplitudetap), STFT Loss: 3.554458 (Spektraltap)
2025-01-18 21:12:07,784 - INFO - Epoch [32/40], Batch [21/25], Gradient Norm: 0.0841
2025-01-18 21:13:02,954 - INFO - [Batch] Epoch: 32/40, Batch: 22/25, Combined Loss: 0.101568 (Summen av L1 og STFT), L1 Loss: 1.151721 (Amplitudetap), STFT Loss: 2.553452 (Spektraltap)
2025-01-18 21:13:56,901 - INFO - [Batch] Epoch: 32/40, Batch: 23/25, Combined Loss: 0.126564 (Summen av L1 og STFT), L1 Loss: 1.496707 (Amplitudetap), STFT Loss: 3.155485 (Spektraltap)
2025-01-18 21:14:54,905 - INFO - [Batch] Epoch: 32/40, Batch: 24/25, Combined Loss: 0.041607 (Summen av L1 og STFT), L1 Loss: 0.380228 (Amplitudetap), STFT Loss: 1.085254 (Spektraltap)
2025-01-18 21:15:47,515 - INFO - [Batch] Epoch: 32/40, Batch: 25/25, Combined Loss: 0.120239 (Summen av L1 og STFT), L1 Loss: 1.350136 (Amplitudetap), STFT Loss: 3.028525 (Spektraltap)
2025-01-18 21:15:47,515 - INFO - Epoch [32/40], Batch [25/25], Gradient Norm: 0.0626
2025-01-18 21:15:47,531 - INFO - Current Learning Rate 1.25e-08
2025-01-18 21:15:47,531 - INFO - [Epoch Improvement] Epoch 32: Loss improved by 0.00% from previous epoch.
2025-01-18 21:15:47,531 - INFO - [Epoch Summary] Avg Training Loss: 0.084278
2025-01-18 21:15:47,531 - INFO - [Train] No improvement in loss for epoch 32 with loss: 0.0842781887575984. Best loss remains 0.084261. Trigger_times: 2
2025-01-18 21:23:56,425 - INFO - [Validation] Epoch 32/40, [Validation] Avg Validation Loss:  1.078853
2025-01-18 21:23:56,425 - INFO - [Epoch Summary] Epoch: 32/40, Avg Combined Loss: 0.084278, Avg L1 Loss: 0.980628, Avg STFT Loss: 2.108501
2025-01-18 21:23:56,425 - INFO - Previous epoch loss: 0.0842781887575984
2025-01-18 21:23:56,425 - INFO - [Train] Epoch 33/40 started.
2025-01-18 21:24:43,210 - INFO - [Batch] Epoch: 33/40, Batch: 1/25, Combined Loss: 0.074938 (Summen av L1 og STFT), L1 Loss: 0.906682 (Amplitudetap), STFT Loss: 1.859570 (Spektraltap)
2025-01-18 21:25:37,417 - INFO - [Batch] Epoch: 33/40, Batch: 2/25, Combined Loss: 0.221608 (Summen av L1 og STFT), L1 Loss: 2.667072 (Amplitudetap), STFT Loss: 5.505208 (Spektraltap)
2025-01-18 21:26:38,789 - INFO - [Batch] Epoch: 33/40, Batch: 3/25, Combined Loss: 0.074714 (Summen av L1 og STFT), L1 Loss: 0.914468 (Amplitudetap), STFT Loss: 1.849519 (Spektraltap)
2025-01-18 21:27:25,817 - INFO - [Batch] Epoch: 33/40, Batch: 4/25, Combined Loss: 0.080848 (Summen av L1 og STFT), L1 Loss: 0.856814 (Amplitudetap), STFT Loss: 2.058241 (Spektraltap)
2025-01-18 21:28:21,372 - INFO - [Batch] Epoch: 33/40, Batch: 5/25, Combined Loss: 0.059344 (Summen av L1 og STFT), L1 Loss: 0.708720 (Amplitudetap), STFT Loss: 1.476582 (Spektraltap)
2025-01-18 21:29:05,089 - INFO - [Batch] Epoch: 33/40, Batch: 6/25, Combined Loss: 0.112725 (Summen av L1 og STFT), L1 Loss: 1.335683 (Amplitudetap), STFT Loss: 2.809311 (Spektraltap)
2025-01-18 21:29:49,677 - INFO - [Batch] Epoch: 33/40, Batch: 7/25, Combined Loss: 0.093242 (Summen av L1 og STFT), L1 Loss: 1.237393 (Amplitudetap), STFT Loss: 2.266948 (Spektraltap)
2025-01-18 21:30:45,845 - INFO - [Batch] Epoch: 33/40, Batch: 8/25, Combined Loss: 0.038719 (Summen av L1 og STFT), L1 Loss: 0.517741 (Amplitudetap), STFT Loss: 0.939678 (Spektraltap)
2025-01-18 21:31:34,216 - INFO - [Batch] Epoch: 33/40, Batch: 9/25, Combined Loss: 0.116211 (Summen av L1 og STFT), L1 Loss: 1.316896 (Amplitudetap), STFT Loss: 2.921948 (Spektraltap)
2025-01-18 21:32:22,672 - INFO - [Batch] Epoch: 33/40, Batch: 10/25, Combined Loss: 0.112438 (Summen av L1 og STFT), L1 Loss: 1.315950 (Amplitudetap), STFT Loss: 2.809175 (Spektraltap)
2025-01-18 21:33:05,170 - INFO - [Batch] Epoch: 33/40, Batch: 11/25, Combined Loss: 0.042147 (Summen av L1 og STFT), L1 Loss: 0.422221 (Amplitudetap), STFT Loss: 1.083453 (Spektraltap)
2025-01-18 21:34:00,522 - INFO - [Batch] Epoch: 33/40, Batch: 12/25, Combined Loss: 0.082947 (Summen av L1 og STFT), L1 Loss: 0.947650 (Amplitudetap), STFT Loss: 2.082286 (Spektraltap)
2025-01-18 21:34:54,496 - INFO - [Batch] Epoch: 33/40, Batch: 13/25, Combined Loss: 0.045619 (Summen av L1 og STFT), L1 Loss: 0.416788 (Amplitudetap), STFT Loss: 1.189950 (Spektraltap)
2025-01-18 21:35:48,364 - INFO - [Batch] Epoch: 33/40, Batch: 14/25, Combined Loss: 0.027709 (Summen av L1 og STFT), L1 Loss: 0.170935 (Amplitudetap), STFT Loss: 0.758014 (Spektraltap)
2025-01-18 21:36:41,281 - INFO - [Batch] Epoch: 33/40, Batch: 15/25, Combined Loss: 0.036241 (Summen av L1 og STFT), L1 Loss: 0.331142 (Amplitudetap), STFT Loss: 0.945303 (Spektraltap)
2025-01-18 21:37:35,008 - INFO - [Batch] Epoch: 33/40, Batch: 16/25, Combined Loss: 0.137669 (Summen av L1 og STFT), L1 Loss: 1.689815 (Amplitudetap), STFT Loss: 3.405863 (Spektraltap)
2025-01-18 21:38:26,352 - INFO - [Batch] Epoch: 33/40, Batch: 17/25, Combined Loss: 0.080135 (Summen av L1 og STFT), L1 Loss: 1.006877 (Amplitudetap), STFT Loss: 1.972537 (Spektraltap)
2025-01-18 21:39:13,014 - INFO - [Batch] Epoch: 33/40, Batch: 18/25, Combined Loss: 0.034427 (Summen av L1 og STFT), L1 Loss: 0.337727 (Amplitudetap), STFT Loss: 0.888074 (Spektraltap)
2025-01-18 21:40:01,016 - INFO - [Batch] Epoch: 33/40, Batch: 19/25, Combined Loss: 0.093921 (Summen av L1 og STFT), L1 Loss: 1.067209 (Amplitudetap), STFT Loss: 2.360242 (Spektraltap)
2025-01-18 21:41:01,255 - INFO - [Batch] Epoch: 33/40, Batch: 20/25, Combined Loss: 0.068956 (Summen av L1 og STFT), L1 Loss: 0.844285 (Amplitudetap), STFT Loss: 1.706839 (Spektraltap)
2025-01-18 21:41:46,234 - INFO - [Batch] Epoch: 33/40, Batch: 21/25, Combined Loss: 0.066566 (Summen av L1 og STFT), L1 Loss: 0.787336 (Amplitudetap), STFT Loss: 1.659549 (Spektraltap)
2025-01-18 21:41:46,249 - INFO - Epoch [33/40], Batch [21/25], Gradient Norm: 0.0495
2025-01-18 21:42:25,756 - INFO - [Batch] Epoch: 33/40, Batch: 22/25, Combined Loss: 0.074214 (Summen av L1 og STFT), L1 Loss: 0.998434 (Amplitudetap), STFT Loss: 1.798509 (Spektraltap)
2025-01-18 21:43:18,743 - INFO - [Batch] Epoch: 33/40, Batch: 23/25, Combined Loss: 0.117491 (Summen av L1 og STFT), L1 Loss: 1.389731 (Amplitudetap), STFT Loss: 2.929123 (Spektraltap)
2025-01-18 21:44:20,848 - INFO - [Batch] Epoch: 33/40, Batch: 24/25, Combined Loss: 0.152821 (Summen av L1 og STFT), L1 Loss: 1.751688 (Amplitudetap), STFT Loss: 3.833907 (Spektraltap)
2025-01-18 21:45:06,267 - INFO - [Batch] Epoch: 33/40, Batch: 25/25, Combined Loss: 0.061389 (Summen av L1 og STFT), L1 Loss: 0.574743 (Amplitudetap), STFT Loss: 1.595353 (Spektraltap)
2025-01-18 21:45:06,283 - INFO - Epoch [33/40], Batch [25/25], Gradient Norm: 0.1031
2025-01-18 21:45:06,321 - INFO - Current Learning Rate 1.25e-08
2025-01-18 21:45:06,321 - INFO - [Epoch Improvement] Epoch 33: Loss improved by 0.00% from previous epoch.
2025-01-18 21:45:06,321 - INFO - [Epoch Summary] Avg Training Loss: 0.084282
2025-01-18 21:45:06,321 - INFO - [Train] No improvement in loss for epoch 33 with loss: 0.0842815787345171. Best loss remains 0.084261. Trigger_times: 3
2025-01-18 21:53:29,734 - INFO - [Validation] Epoch 33/40, [Validation] Avg Validation Loss:  1.078838
2025-01-18 21:53:29,734 - INFO - [Epoch Summary] Epoch: 33/40, Avg Combined Loss: 0.084282, Avg L1 Loss: 0.980626, Avg STFT Loss: 2.108492
2025-01-18 21:53:29,734 - INFO - Previous epoch loss: 0.0842815787345171
2025-01-18 21:53:29,734 - INFO - [Train] Epoch 34/40 started.
2025-01-18 21:54:22,750 - INFO - [Batch] Epoch: 34/40, Batch: 1/25, Combined Loss: 0.084249 (Summen av L1 og STFT), L1 Loss: 0.923989 (Amplitudetap), STFT Loss: 2.131489 (Spektraltap)
2025-01-18 21:55:11,082 - INFO - [Batch] Epoch: 34/40, Batch: 2/25, Combined Loss: 0.126691 (Summen av L1 og STFT), L1 Loss: 1.677572 (Amplitudetap), STFT Loss: 3.081767 (Spektraltap)
2025-01-18 21:55:58,546 - INFO - [Batch] Epoch: 34/40, Batch: 3/25, Combined Loss: 0.041057 (Summen av L1 og STFT), L1 Loss: 0.441990 (Amplitudetap), STFT Loss: 1.042286 (Spektraltap)
2025-01-18 21:56:43,591 - INFO - [Batch] Epoch: 34/40, Batch: 4/25, Combined Loss: 0.070126 (Summen av L1 og STFT), L1 Loss: 0.832133 (Amplitudetap), STFT Loss: 1.747141 (Spektraltap)
2025-01-18 21:57:30,381 - INFO - [Batch] Epoch: 34/40, Batch: 5/25, Combined Loss: 0.027090 (Summen av L1 og STFT), L1 Loss: 0.210468 (Amplitudetap), STFT Loss: 0.722512 (Spektraltap)
2025-01-18 21:58:27,503 - INFO - [Batch] Epoch: 34/40, Batch: 6/25, Combined Loss: 0.113961 (Summen av L1 og STFT), L1 Loss: 1.360552 (Amplitudetap), STFT Loss: 2.835746 (Spektraltap)
2025-01-18 21:59:16,019 - INFO - [Batch] Epoch: 34/40, Batch: 7/25, Combined Loss: 0.075467 (Summen av L1 og STFT), L1 Loss: 0.927344 (Amplitudetap), STFT Loss: 1.866582 (Spektraltap)
2025-01-18 22:00:17,592 - INFO - [Batch] Epoch: 34/40, Batch: 8/25, Combined Loss: 0.042061 (Summen av L1 og STFT), L1 Loss: 0.393608 (Amplitudetap), STFT Loss: 1.093153 (Spektraltap)
2025-01-18 22:01:06,296 - INFO - [Batch] Epoch: 34/40, Batch: 9/25, Combined Loss: 0.106799 (Summen av L1 og STFT), L1 Loss: 1.236852 (Amplitudetap), STFT Loss: 2.673894 (Spektraltap)
2025-01-18 22:02:04,915 - INFO - [Batch] Epoch: 34/40, Batch: 10/25, Combined Loss: 0.011850 (Summen av L1 og STFT), L1 Loss: 0.116780 (Amplitudetap), STFT Loss: 0.305459 (Spektraltap)
2025-01-18 22:02:54,705 - INFO - [Batch] Epoch: 34/40, Batch: 11/25, Combined Loss: 0.021400 (Summen av L1 og STFT), L1 Loss: 0.367552 (Amplitudetap), STFT Loss: 0.484481 (Spektraltap)
2025-01-18 22:03:47,809 - INFO - [Batch] Epoch: 34/40, Batch: 12/25, Combined Loss: 0.079048 (Summen av L1 og STFT), L1 Loss: 0.851181 (Amplitudetap), STFT Loss: 2.006657 (Spektraltap)
2025-01-18 22:04:27,401 - INFO - [Batch] Epoch: 34/40, Batch: 13/25, Combined Loss: 0.097040 (Summen av L1 og STFT), L1 Loss: 1.231859 (Amplitudetap), STFT Loss: 2.383264 (Spektraltap)
2025-01-18 22:05:24,022 - INFO - [Batch] Epoch: 34/40, Batch: 14/25, Combined Loss: 0.075730 (Summen av L1 og STFT), L1 Loss: 0.864125 (Amplitudetap), STFT Loss: 1.901565 (Spektraltap)
2025-01-18 22:06:12,826 - INFO - [Batch] Epoch: 34/40, Batch: 15/25, Combined Loss: 0.174657 (Summen av L1 og STFT), L1 Loss: 2.004569 (Amplitudetap), STFT Loss: 4.380607 (Spektraltap)
2025-01-18 22:07:08,138 - INFO - [Batch] Epoch: 34/40, Batch: 16/25, Combined Loss: 0.121916 (Summen av L1 og STFT), L1 Loss: 1.318020 (Amplitudetap), STFT Loss: 3.092606 (Spektraltap)
2025-01-18 22:07:37,651 - INFO - [Batch] Epoch: 34/40, Batch: 17/25, Combined Loss: 0.022976 (Summen av L1 og STFT), L1 Loss: 0.189699 (Amplitudetap), STFT Loss: 0.607983 (Spektraltap)
2025-01-18 22:08:31,752 - INFO - [Batch] Epoch: 34/40, Batch: 18/25, Combined Loss: 0.067262 (Summen av L1 og STFT), L1 Loss: 0.750930 (Amplitudetap), STFT Loss: 1.696039 (Spektraltap)
2025-01-18 22:09:23,316 - INFO - [Batch] Epoch: 34/40, Batch: 19/25, Combined Loss: 0.155616 (Summen av L1 og STFT), L1 Loss: 1.843731 (Amplitudetap), STFT Loss: 3.878299 (Spektraltap)
2025-01-18 22:10:07,779 - INFO - [Batch] Epoch: 34/40, Batch: 20/25, Combined Loss: 0.128307 (Summen av L1 og STFT), L1 Loss: 1.536282 (Amplitudetap), STFT Loss: 3.190802 (Spektraltap)
2025-01-18 22:11:00,615 - INFO - [Batch] Epoch: 34/40, Batch: 21/25, Combined Loss: 0.087275 (Summen av L1 og STFT), L1 Loss: 0.981340 (Amplitudetap), STFT Loss: 2.197685 (Spektraltap)
2025-01-18 22:11:00,615 - INFO - Epoch [34/40], Batch [21/25], Gradient Norm: 0.0462
2025-01-18 22:11:57,159 - INFO - [Batch] Epoch: 34/40, Batch: 22/25, Combined Loss: 0.090310 (Summen av L1 og STFT), L1 Loss: 0.895641 (Amplitudetap), STFT Loss: 2.325444 (Spektraltap)
2025-01-18 22:12:45,928 - INFO - [Batch] Epoch: 34/40, Batch: 23/25, Combined Loss: 0.119229 (Summen av L1 og STFT), L1 Loss: 1.378060 (Amplitudetap), STFT Loss: 2.986265 (Spektraltap)
2025-01-18 22:13:40,766 - INFO - [Batch] Epoch: 34/40, Batch: 24/25, Combined Loss: 0.121049 (Summen av L1 og STFT), L1 Loss: 1.716704 (Amplitudetap), STFT Loss: 2.895739 (Spektraltap)
2025-01-18 22:14:26,117 - INFO - [Batch] Epoch: 34/40, Batch: 25/25, Combined Loss: 0.045615 (Summen av L1 og STFT), L1 Loss: 0.456422 (Amplitudetap), STFT Loss: 1.172841 (Spektraltap)
2025-01-18 22:14:26,133 - INFO - Epoch [34/40], Batch [25/25], Gradient Norm: 0.0790
2025-01-18 22:14:26,164 - INFO - Current Learning Rate 1.25e-08
2025-01-18 22:14:26,164 - INFO - [Epoch Improvement] Epoch 34: Loss improved by 0.00% from previous epoch.
2025-01-18 22:14:26,164 - INFO - [Epoch Summary] Avg Training Loss: 0.084271
2025-01-18 22:14:26,164 - INFO - [Train] No improvement in loss for epoch 34 with loss: 0.08427130363881588. Best loss remains 0.084261. Trigger_times: 4
2025-01-18 22:22:51,757 - INFO - [Validation] Epoch 34/40, [Validation] Avg Validation Loss:  1.078837
2025-01-18 22:22:51,757 - INFO - [Epoch Summary] Epoch: 34/40, Avg Combined Loss: 0.084271, Avg L1 Loss: 0.980616, Avg STFT Loss: 2.108478
2025-01-18 22:22:51,757 - INFO - Previous epoch loss: 0.08427130363881588
2025-01-18 22:22:51,757 - INFO - [Train] Epoch 35/40 started.
2025-01-18 22:23:42,918 - INFO - [Batch] Epoch: 35/40, Batch: 1/25, Combined Loss: 0.092172 (Summen av L1 og STFT), L1 Loss: 1.088027 (Amplitudetap), STFT Loss: 2.298850 (Spektraltap)
2025-01-18 22:24:31,598 - INFO - [Batch] Epoch: 35/40, Batch: 2/25, Combined Loss: 0.025959 (Summen av L1 og STFT), L1 Loss: 0.194016 (Amplitudetap), STFT Loss: 0.695625 (Spektraltap)
2025-01-18 22:25:20,703 - INFO - [Batch] Epoch: 35/40, Batch: 3/25, Combined Loss: 0.097393 (Summen av L1 og STFT), L1 Loss: 1.097614 (Amplitudetap), STFT Loss: 2.451374 (Spektraltap)
2025-01-18 22:26:09,251 - INFO - [Batch] Epoch: 35/40, Batch: 4/25, Combined Loss: 0.116873 (Summen av L1 og STFT), L1 Loss: 1.326463 (Amplitudetap), STFT Loss: 2.937711 (Spektraltap)
2025-01-18 22:27:00,994 - INFO - [Batch] Epoch: 35/40, Batch: 5/25, Combined Loss: 0.029290 (Summen av L1 og STFT), L1 Loss: 0.283080 (Amplitudetap), STFT Loss: 0.757390 (Spektraltap)
2025-01-18 22:27:49,400 - INFO - [Batch] Epoch: 35/40, Batch: 6/25, Combined Loss: 0.113516 (Summen av L1 og STFT), L1 Loss: 1.417757 (Amplitudetap), STFT Loss: 2.797883 (Spektraltap)
2025-01-18 22:28:33,057 - INFO - [Batch] Epoch: 35/40, Batch: 7/25, Combined Loss: 0.127570 (Summen av L1 og STFT), L1 Loss: 1.725660 (Amplitudetap), STFT Loss: 3.087536 (Spektraltap)
2025-01-18 22:29:28,468 - INFO - [Batch] Epoch: 35/40, Batch: 8/25, Combined Loss: 0.141311 (Summen av L1 og STFT), L1 Loss: 1.691555 (Amplitudetap), STFT Loss: 3.514374 (Spektraltap)
2025-01-18 22:30:14,453 - INFO - [Batch] Epoch: 35/40, Batch: 9/25, Combined Loss: 0.068920 (Summen av L1 og STFT), L1 Loss: 0.773205 (Amplitudetap), STFT Loss: 1.736234 (Spektraltap)
2025-01-18 22:30:55,008 - INFO - [Batch] Epoch: 35/40, Batch: 10/25, Combined Loss: 0.027623 (Summen av L1 og STFT), L1 Loss: 0.203859 (Amplitudetap), STFT Loss: 0.741322 (Spektraltap)
2025-01-18 22:31:47,079 - INFO - [Batch] Epoch: 35/40, Batch: 11/25, Combined Loss: 0.081472 (Summen av L1 og STFT), L1 Loss: 0.939023 (Amplitudetap), STFT Loss: 2.041736 (Spektraltap)
2025-01-18 22:32:30,791 - INFO - [Batch] Epoch: 35/40, Batch: 12/25, Combined Loss: 0.117971 (Summen av L1 og STFT), L1 Loss: 1.361723 (Amplitudetap), STFT Loss: 2.955521 (Spektraltap)
2025-01-18 22:33:15,768 - INFO - [Batch] Epoch: 35/40, Batch: 13/25, Combined Loss: 0.055383 (Summen av L1 og STFT), L1 Loss: 0.557813 (Amplitudetap), STFT Loss: 1.422421 (Spektraltap)
2025-01-18 22:34:24,917 - INFO - [Batch] Epoch: 35/40, Batch: 14/25, Combined Loss: 0.010317 (Summen av L1 og STFT), L1 Loss: 0.089283 (Amplitudetap), STFT Loss: 0.271246 (Spektraltap)
2025-01-18 22:35:18,727 - INFO - [Batch] Epoch: 35/40, Batch: 15/25, Combined Loss: 0.174872 (Summen av L1 og STFT), L1 Loss: 2.195096 (Amplitudetap), STFT Loss: 4.305409 (Spektraltap)
2025-01-18 22:36:09,503 - INFO - [Batch] Epoch: 35/40, Batch: 16/25, Combined Loss: 0.025466 (Summen av L1 og STFT), L1 Loss: 0.415263 (Amplitudetap), STFT Loss: 0.586016 (Spektraltap)
2025-01-18 22:36:58,197 - INFO - [Batch] Epoch: 35/40, Batch: 17/25, Combined Loss: 0.035584 (Summen av L1 og STFT), L1 Loss: 0.330110 (Amplitudetap), STFT Loss: 0.926055 (Spektraltap)
2025-01-18 22:37:57,654 - INFO - [Batch] Epoch: 35/40, Batch: 18/25, Combined Loss: 0.082024 (Summen av L1 og STFT), L1 Loss: 0.902983 (Amplitudetap), STFT Loss: 2.073720 (Spektraltap)
2025-01-18 22:38:55,961 - INFO - [Batch] Epoch: 35/40, Batch: 19/25, Combined Loss: 0.107054 (Summen av L1 og STFT), L1 Loss: 1.238000 (Amplitudetap), STFT Loss: 2.681058 (Spektraltap)
2025-01-18 22:39:43,565 - INFO - [Batch] Epoch: 35/40, Batch: 20/25, Combined Loss: 0.086780 (Summen av L1 og STFT), L1 Loss: 1.106679 (Amplitudetap), STFT Loss: 2.129105 (Spektraltap)
2025-01-18 22:40:35,535 - INFO - [Batch] Epoch: 35/40, Batch: 21/25, Combined Loss: 0.086663 (Summen av L1 og STFT), L1 Loss: 0.900155 (Amplitudetap), STFT Loss: 2.214103 (Spektraltap)
2025-01-18 22:40:35,535 - INFO - Epoch [35/40], Batch [21/25], Gradient Norm: 0.0605
2025-01-18 22:41:31,250 - INFO - [Batch] Epoch: 35/40, Batch: 22/25, Combined Loss: 0.075426 (Summen av L1 og STFT), L1 Loss: 1.016218 (Amplitudetap), STFT Loss: 1.827271 (Spektraltap)
2025-01-18 22:42:35,590 - INFO - [Batch] Epoch: 35/40, Batch: 23/25, Combined Loss: 0.088187 (Summen av L1 og STFT), L1 Loss: 1.002300 (Amplitudetap), STFT Loss: 2.216038 (Spektraltap)
2025-01-18 22:43:25,016 - INFO - [Batch] Epoch: 35/40, Batch: 24/25, Combined Loss: 0.164424 (Summen av L1 og STFT), L1 Loss: 1.843917 (Amplitudetap), STFT Loss: 4.142477 (Spektraltap)
2025-01-18 22:44:15,601 - INFO - [Batch] Epoch: 35/40, Batch: 25/25, Combined Loss: 0.074761 (Summen av L1 og STFT), L1 Loss: 0.808442 (Amplitudetap), STFT Loss: 1.896343 (Spektraltap)
2025-01-18 22:44:15,616 - INFO - Epoch [35/40], Batch [25/25], Gradient Norm: 0.0214
2025-01-18 22:44:15,616 - INFO - Current Learning Rate 1.25e-08
2025-01-18 22:44:15,616 - INFO - [Epoch Improvement] Epoch 35: Loss improved by 0.00% from previous epoch.
2025-01-18 22:44:15,616 - INFO - [Epoch Summary] Avg Training Loss: 0.084280
2025-01-18 22:44:15,616 - INFO - [Train] No improvement in loss for epoch 35 with loss: 0.08428046729415656. Best loss remains 0.084261. Trigger_times: 5
2025-01-18 22:53:13,457 - INFO - [Validation] Epoch 35/40, [Validation] Avg Validation Loss:  1.078819
2025-01-18 22:53:13,457 - INFO - [Memory After Epoch 35] Timestamp: 2025-01-18 22:53:13
2025-01-18 22:53:13,457 - INFO - [Memory After Epoch 35] GPU 0: NVIDIA GeForce RTX 3060, Total=12.88 GB, Allocated=0.51 GB, Cached=23.26 GB
2025-01-18 22:53:13,457 - INFO - [Memory After Epoch 35] CPU Memory Usage: ~0.55 GB
2025-01-18 22:53:13,457 - INFO - [Epoch Summary] Epoch: 35/40, Avg Combined Loss: 0.084280, Avg L1 Loss: 0.980608, Avg STFT Loss: 2.108472
2025-01-18 22:53:13,457 - INFO - Previous epoch loss: 0.08428046729415656
2025-01-18 22:53:13,457 - INFO - [Train] Epoch 36/40 started.
2025-01-18 22:54:04,181 - INFO - [Batch] Epoch: 36/40, Batch: 1/25, Combined Loss: 0.106631 (Summen av L1 og STFT), L1 Loss: 1.236033 (Amplitudetap), STFT Loss: 2.669215 (Spektraltap)
2025-01-18 22:54:59,971 - INFO - [Batch] Epoch: 36/40, Batch: 2/25, Combined Loss: 0.077858 (Summen av L1 og STFT), L1 Loss: 0.894909 (Amplitudetap), STFT Loss: 1.952216 (Spektraltap)
2025-01-18 22:55:54,613 - INFO - [Batch] Epoch: 36/40, Batch: 3/25, Combined Loss: 0.077446 (Summen av L1 og STFT), L1 Loss: 0.915489 (Amplitudetap), STFT Loss: 1.931034 (Spektraltap)
2025-01-18 22:56:42,734 - INFO - [Batch] Epoch: 36/40, Batch: 4/25, Combined Loss: 0.064526 (Summen av L1 og STFT), L1 Loss: 0.725751 (Amplitudetap), STFT Loss: 1.624732 (Spektraltap)
2025-01-18 22:57:29,012 - INFO - [Batch] Epoch: 36/40, Batch: 5/25, Combined Loss: 0.050674 (Summen av L1 og STFT), L1 Loss: 0.656576 (Amplitudetap), STFT Loss: 1.238837 (Spektraltap)
2025-01-18 22:58:19,595 - INFO - [Batch] Epoch: 36/40, Batch: 6/25, Combined Loss: 0.027062 (Summen av L1 og STFT), L1 Loss: 0.208393 (Amplitudetap), STFT Loss: 0.722554 (Spektraltap)
2025-01-18 22:58:59,687 - INFO - [Batch] Epoch: 36/40, Batch: 7/25, Combined Loss: 0.049379 (Summen av L1 og STFT), L1 Loss: 0.512497 (Amplitudetap), STFT Loss: 1.261719 (Spektraltap)
2025-01-18 22:59:52,220 - INFO - [Batch] Epoch: 36/40, Batch: 8/25, Combined Loss: 0.035560 (Summen av L1 og STFT), L1 Loss: 0.369053 (Amplitudetap), STFT Loss: 0.908627 (Spektraltap)
2025-01-18 23:00:32,923 - INFO - [Batch] Epoch: 36/40, Batch: 9/25, Combined Loss: 0.075313 (Summen av L1 og STFT), L1 Loss: 0.847787 (Amplitudetap), STFT Loss: 1.896040 (Spektraltap)
2025-01-18 23:01:26,394 - INFO - [Batch] Epoch: 36/40, Batch: 10/25, Combined Loss: 0.106934 (Summen av L1 og STFT), L1 Loss: 1.234916 (Amplitudetap), STFT Loss: 2.678764 (Spektraltap)
2025-01-18 23:02:19,892 - INFO - [Batch] Epoch: 36/40, Batch: 11/25, Combined Loss: 0.061419 (Summen av L1 og STFT), L1 Loss: 0.701722 (Amplitudetap), STFT Loss: 1.541821 (Spektraltap)
2025-01-18 23:03:08,129 - INFO - [Batch] Epoch: 36/40, Batch: 12/25, Combined Loss: 0.078962 (Summen av L1 og STFT), L1 Loss: 0.839380 (Amplitudetap), STFT Loss: 2.009128 (Spektraltap)
2025-01-18 23:04:03,335 - INFO - [Batch] Epoch: 36/40, Batch: 13/25, Combined Loss: 0.130848 (Summen av L1 og STFT), L1 Loss: 1.556760 (Amplitudetap), STFT Loss: 3.258259 (Spektraltap)
2025-01-18 23:04:54,517 - INFO - [Batch] Epoch: 36/40, Batch: 14/25, Combined Loss: 0.136305 (Summen av L1 og STFT), L1 Loss: 1.491422 (Amplitudetap), STFT Loss: 3.449972 (Spektraltap)
2025-01-18 23:05:41,760 - INFO - [Batch] Epoch: 36/40, Batch: 15/25, Combined Loss: 0.129740 (Summen av L1 og STFT), L1 Loss: 1.642597 (Amplitudetap), STFT Loss: 3.188244 (Spektraltap)
2025-01-18 23:06:36,850 - INFO - [Batch] Epoch: 36/40, Batch: 16/25, Combined Loss: 0.078744 (Summen av L1 og STFT), L1 Loss: 0.936108 (Amplitudetap), STFT Loss: 1.961142 (Spektraltap)
2025-01-18 23:07:17,945 - INFO - [Batch] Epoch: 36/40, Batch: 17/25, Combined Loss: 0.067160 (Summen av L1 og STFT), L1 Loss: 0.686773 (Amplitudetap), STFT Loss: 1.720476 (Spektraltap)
2025-01-18 23:08:05,196 - INFO - [Batch] Epoch: 36/40, Batch: 18/25, Combined Loss: 0.097578 (Summen av L1 og STFT), L1 Loss: 0.952805 (Amplitudetap), STFT Loss: 2.519001 (Spektraltap)
2025-01-18 23:08:59,787 - INFO - [Batch] Epoch: 36/40, Batch: 19/25, Combined Loss: 0.097581 (Summen av L1 og STFT), L1 Loss: 1.196532 (Amplitudetap), STFT Loss: 2.414621 (Spektraltap)
2025-01-18 23:09:49,381 - INFO - [Batch] Epoch: 36/40, Batch: 20/25, Combined Loss: 0.040318 (Summen av L1 og STFT), L1 Loss: 0.632624 (Amplitudetap), STFT Loss: 0.938418 (Spektraltap)
2025-01-18 23:10:49,759 - INFO - [Batch] Epoch: 36/40, Batch: 21/25, Combined Loss: 0.129216 (Summen av L1 og STFT), L1 Loss: 1.550525 (Amplitudetap), STFT Loss: 3.211982 (Spektraltap)
2025-01-18 23:10:49,775 - INFO - Epoch [36/40], Batch [21/25], Gradient Norm: 0.0712
2025-01-18 23:11:50,015 - INFO - [Batch] Epoch: 36/40, Batch: 22/25, Combined Loss: 0.159240 (Summen av L1 og STFT), L1 Loss: 2.051043 (Amplitudetap), STFT Loss: 3.898176 (Spektraltap)
2025-01-18 23:12:40,864 - INFO - [Batch] Epoch: 36/40, Batch: 23/25, Combined Loss: 0.042278 (Summen av L1 og STFT), L1 Loss: 0.351570 (Amplitudetap), STFT Loss: 1.117671 (Spektraltap)
2025-01-18 23:13:28,519 - INFO - [Batch] Epoch: 36/40, Batch: 24/25, Combined Loss: 0.170288 (Summen av L1 og STFT), L1 Loss: 2.220310 (Amplitudetap), STFT Loss: 4.157070 (Spektraltap)
2025-01-18 23:14:12,744 - INFO - [Batch] Epoch: 36/40, Batch: 25/25, Combined Loss: 0.015380 (Summen av L1 og STFT), L1 Loss: 0.098187 (Amplitudetap), STFT Loss: 0.419320 (Spektraltap)
2025-01-18 23:14:12,759 - INFO - Epoch [36/40], Batch [25/25], Gradient Norm: 0.0781
2025-01-18 23:14:12,790 - INFO - Current Learning Rate 1.25e-08
2025-01-18 23:14:12,790 - INFO - [Epoch Improvement] Epoch 36: Loss improved by 0.00% from previous epoch.
2025-01-18 23:14:12,790 - INFO - [Epoch Summary] Avg Training Loss: 0.084258
2025-01-18 23:14:12,948 - INFO - [Train] New best model saved at C:\Users\didri\Desktop\UNet Models\UNet_vocal_isolation_model\Model_weights\CheckPoints\best_model_epoch-35.pth with loss 0.084258
2025-01-18 23:22:42,693 - INFO - [Validation] Epoch 36/40, [Validation] Avg Validation Loss:  1.078816
2025-01-18 23:22:42,693 - INFO - [Epoch Summary] Epoch: 36/40, Avg Combined Loss: 0.084258, Avg L1 Loss: 0.980602, Avg STFT Loss: 2.108447
2025-01-18 23:22:42,693 - INFO - Previous epoch loss: 0.08425763055682182



#FINE TUNING####
2025-01-21 01:10:35,176 - INFO - Found 100 files in 'C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Datasets\Dataset_Audio_Folders\musdb18\train' (subset='train')
2025-01-21 01:10:35,176 - INFO - Dataset initialized with 100 valid files.
2025-01-21 01:10:35,176 - INFO - Found 50 files in 'C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Datasets\Dataset_Audio_Folders\musdb18\test' (subset='test')
2025-01-21 01:10:35,177 - INFO - Dataset initialized with 50 valid files.
2025-01-21 01:10:35,180 - INFO - Training dataset size: 150
2025-01-21 01:10:35,180 - INFO - Validation dataset size: 100
2025-01-21 01:10:35,181 - INFO - Data loaders created successfully.
2025-01-21 01:10:35,181 - ERROR - Fine-tuning failed: name 'pretrained_model_path' is not defined
2025-01-21 01:11:43,949 - INFO - Found 100 files in 'C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Datasets\Dataset_Audio_Folders\musdb18\train' (subset='train')
2025-01-21 01:11:43,949 - INFO - Dataset initialized with 100 valid files.
2025-01-21 01:11:43,949 - INFO - Found 50 files in 'C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Datasets\Dataset_Audio_Folders\musdb18\test' (subset='test')
2025-01-21 01:11:43,950 - INFO - Dataset initialized with 50 valid files.
2025-01-21 01:11:43,954 - INFO - Training dataset size: 150
2025-01-21 01:11:43,954 - INFO - Validation dataset size: 100
2025-01-21 01:11:43,955 - INFO - Data loaders created successfully.
2025-01-21 01:11:43,955 - ERROR - Fine-tuning failed: name 'pretrained_model_path' is not defined
2025-01-21 01:11:47,032 - INFO - Found 100 files in 'C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Datasets\Dataset_Audio_Folders\musdb18\train' (subset='train')
2025-01-21 01:11:47,032 - INFO - Dataset initialized with 100 valid files.
2025-01-21 01:11:47,032 - INFO - Found 50 files in 'C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Datasets\Dataset_Audio_Folders\musdb18\test' (subset='test')
2025-01-21 01:11:47,032 - INFO - Dataset initialized with 50 valid files.
2025-01-21 01:11:47,047 - INFO - Training dataset size: 150
2025-01-21 01:11:47,047 - INFO - Validation dataset size: 100
2025-01-21 01:11:47,047 - INFO - Data loaders created successfully.
2025-01-21 01:11:47,047 - ERROR - Fine-tuning failed: name 'pretrained_model_path' is not defined
2025-01-21 01:11:50,055 - INFO - Found 100 files in 'C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Datasets\Dataset_Audio_Folders\musdb18\train' (subset='train')
2025-01-21 01:11:50,055 - INFO - Dataset initialized with 100 valid files.
2025-01-21 01:11:50,055 - INFO - Found 50 files in 'C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Datasets\Dataset_Audio_Folders\musdb18\test' (subset='test')
2025-01-21 01:11:50,055 - INFO - Dataset initialized with 50 valid files.
2025-01-21 01:11:50,058 - INFO - Training dataset size: 150
2025-01-21 01:11:50,058 - INFO - Validation dataset size: 100
2025-01-21 01:11:50,059 - INFO - Data loaders created successfully.
2025-01-21 01:11:50,059 - ERROR - Fine-tuning failed: name 'pretrained_model_path' is not defined
2025-01-21 01:12:10,092 - INFO - Found 100 files in 'C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Datasets\Dataset_Audio_Folders\musdb18\train' (subset='train')
2025-01-21 01:12:10,092 - INFO - Dataset initialized with 100 valid files.
2025-01-21 01:12:10,092 - INFO - Found 50 files in 'C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Datasets\Dataset_Audio_Folders\musdb18\test' (subset='test')
2025-01-21 01:12:10,092 - INFO - Dataset initialized with 50 valid files.
2025-01-21 01:12:10,092 - INFO - Training dataset size: 150
2025-01-21 01:12:10,092 - INFO - Validation dataset size: 100
2025-01-21 01:12:10,098 - INFO - Data loaders created successfully.
2025-01-21 01:12:10,117 - INFO - Fine-tuning --> Using device: cuda
2025-01-21 01:12:10,117 - INFO - Initializing model...
2025-01-21 01:12:10,346 - INFO - Single GPU detected or using CPU.
2025-01-21 01:12:10,474 - INFO - Fine-tuning --> Pretrained model loaded from: C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Model_Weights\CheckPoints\best_model_epoch-18.pth
2025-01-21 01:12:10,474 - INFO - Encoder layers frozen for fine-tuning.
2025-01-21 01:12:10,474 - INFO - Fine-tuning --> Encoder layers frozen.
2025-01-21 01:12:10,474 - INFO - Loading VGGish model for perceptual loss...
2025-01-21 01:12:11,601 - INFO - VGGish model loaded and set to evaluation mode.
2025-01-21 01:12:12,816 - INFO - Fine-tuning --> Starting fine-tuning: Epochs=6, LR=0.001
2025-01-21 01:12:12,816 - INFO - Visualization directory set to: C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Model_Weights\Fine_tuned\visualizations
2025-01-21 01:12:12,816 - INFO - Starting Epoch 1/6
2025-01-21 01:13:38,809 - INFO - Epoch [1], Batch [1] -> Combined Loss: 448.644470, Mask Loss: 0.170667, Total Loss: 897.118286
2025-01-21 01:14:02,047 - INFO - Epoch [1], Batch [2] -> Combined Loss: 454.875977, Mask Loss: 0.803548, Total Loss: 908.948425
2025-01-21 01:14:25,142 - INFO - Epoch [1], Batch [3] -> Combined Loss: 505.031616, Mask Loss: 0.850596, Total Loss: 1009.212646
2025-01-21 01:14:48,937 - INFO - Epoch [1], Batch [4] -> Combined Loss: 606.763489, Mask Loss: 1.422818, Total Loss: 1212.104126
2025-01-21 01:15:11,635 - INFO - Epoch [1], Batch [5] -> Combined Loss: 493.491852, Mask Loss: 0.538161, Total Loss: 986.445557
2025-01-21 01:15:33,536 - INFO - Epoch [1], Batch [6] -> Combined Loss: 520.665710, Mask Loss: 1.247418, Total Loss: 1040.083984
2025-01-21 01:15:56,696 - INFO - Epoch [1], Batch [7] -> Combined Loss: 617.164673, Mask Loss: 0.559067, Total Loss: 1233.770264
2025-01-21 01:16:18,911 - INFO - Epoch [1], Batch [8] -> Combined Loss: 561.888611, Mask Loss: 0.723102, Total Loss: 1123.054077
2025-01-21 01:16:41,828 - INFO - Epoch [1], Batch [9] -> Combined Loss: 517.351196, Mask Loss: 0.251998, Total Loss: 1034.450439
2025-01-21 01:17:04,081 - INFO - Epoch [1], Batch [10] -> Combined Loss: 558.521179, Mask Loss: 0.584475, Total Loss: 1116.457886
2025-01-21 01:17:26,650 - INFO - Epoch [1], Batch [11] -> Combined Loss: 471.115814, Mask Loss: 0.150079, Total Loss: 942.081543
2025-01-21 01:17:50,524 - INFO - Epoch [1], Batch [12] -> Combined Loss: 649.159851, Mask Loss: 1.303226, Total Loss: 1297.016479
2025-01-21 01:18:11,949 - INFO - Epoch [1], Batch [13] -> Combined Loss: 709.083069, Mask Loss: 1.302532, Total Loss: 1416.863647
2025-01-21 01:18:37,674 - INFO - Epoch [1], Batch [14] -> Combined Loss: 544.765137, Mask Loss: 0.871829, Total Loss: 1088.658447
2025-01-21 01:19:00,716 - INFO - Epoch [1], Batch [15] -> Combined Loss: 584.300964, Mask Loss: 0.854271, Total Loss: 1167.747681
2025-01-21 01:19:22,965 - INFO - Epoch [1], Batch [16] -> Combined Loss: 605.979919, Mask Loss: 0.785486, Total Loss: 1211.174316
2025-01-21 01:19:44,893 - INFO - Epoch [1], Batch [17] -> Combined Loss: 618.980164, Mask Loss: 1.497242, Total Loss: 1236.463135
2025-01-21 01:20:05,598 - INFO - Epoch [1], Batch [18] -> Combined Loss: 613.968018, Mask Loss: 1.194067, Total Loss: 1226.741943
2025-01-21 01:20:27,590 - INFO - Epoch [1], Batch [19] -> Combined Loss: 664.912292, Mask Loss: 1.252285, Total Loss: 1328.572266
2025-01-21 01:20:50,178 - INFO - Epoch [1], Batch [20] -> Combined Loss: 487.136475, Mask Loss: 0.161706, Total Loss: 974.111267
2025-01-21 01:21:12,414 - INFO - Epoch [1], Batch [21] -> Combined Loss: 491.917969, Mask Loss: 0.166412, Total Loss: 983.669556
2025-01-21 01:21:33,138 - INFO - Epoch [1], Batch [22] -> Combined Loss: 540.998901, Mask Loss: 0.683075, Total Loss: 1081.314697
2025-01-21 01:21:53,992 - INFO - Epoch [1], Batch [23] -> Combined Loss: 538.291992, Mask Loss: 0.796808, Total Loss: 1075.787231
2025-01-21 01:22:16,045 - INFO - Epoch [1], Batch [24] -> Combined Loss: 534.083923, Mask Loss: 0.200260, Total Loss: 1067.967529
2025-01-21 01:22:38,644 - INFO - Epoch [1], Batch [25] -> Combined Loss: 484.264832, Mask Loss: 0.461904, Total Loss: 968.067749
2025-01-21 01:23:00,582 - INFO - Epoch [1], Batch [26] -> Combined Loss: 487.084381, Mask Loss: 0.202458, Total Loss: 973.966309
2025-01-21 01:23:22,205 - INFO - Epoch [1], Batch [27] -> Combined Loss: 546.689453, Mask Loss: 0.687225, Total Loss: 1092.691650
2025-01-21 01:23:42,343 - INFO - Epoch [1], Batch [28] -> Combined Loss: 548.118164, Mask Loss: 0.720616, Total Loss: 1095.515747
2025-01-21 01:24:02,902 - INFO - Epoch [1], Batch [29] -> Combined Loss: 506.521759, Mask Loss: 0.174840, Total Loss: 1012.868652
2025-01-21 01:24:25,330 - INFO - Epoch [1], Batch [30] -> Combined Loss: 566.146423, Mask Loss: 1.223318, Total Loss: 1131.069580
2025-01-21 01:24:26,268 - INFO - Fine-Tuning --> Epoch [1/6] Average Training Loss: 549.263942
2025-01-21 01:25:50,137 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 01:25:53,773 - INFO - Saved waveform visualization at C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Model_Weights\Fine_tuned\visualizations\epoch_1_sample_2.png
2025-01-21 01:25:56,931 - INFO - Saved waveform visualization at C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Model_Weights\Fine_tuned\visualizations\epoch_1_sample_3.png
2025-01-21 01:26:00,081 - INFO - Saved waveform visualization at C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Model_Weights\Fine_tuned\visualizations\epoch_1_sample_4.png
2025-01-21 01:26:34,603 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 01:26:34,603 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 01:26:34,603 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 01:27:07,959 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 01:27:07,959 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 01:27:07,959 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 01:27:07,974 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 01:27:07,976 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 01:27:41,264 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 01:27:41,280 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 01:27:41,618 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 01:28:15,922 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 01:28:16,135 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 01:28:16,135 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 01:28:16,135 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 01:29:23,837 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 01:29:24,061 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 01:29:24,061 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 01:30:33,281 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 01:30:33,281 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 01:30:33,281 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 01:31:06,599 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 01:31:06,599 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 01:31:06,599 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 01:31:06,599 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 01:31:43,808 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 01:32:17,886 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 01:32:17,886 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 01:32:18,099 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 01:32:53,983 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 01:32:53,983 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 01:32:53,987 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 01:32:54,195 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 01:33:28,323 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 01:33:28,323 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 01:33:28,323 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 01:33:28,543 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 01:34:04,095 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 01:34:04,095 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 01:34:04,095 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 01:34:04,095 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 01:34:38,094 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 01:34:38,094 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 01:34:38,094 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 01:34:38,313 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 01:35:13,755 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 01:35:13,770 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 01:35:13,968 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 01:35:48,979 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 01:35:48,979 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 01:36:24,716 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 01:36:25,184 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 01:36:59,678 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 01:36:59,678 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 01:36:59,678 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 01:36:59,678 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 01:37:00,615 - INFO - Validation Metrics - SDR: -28.5055, SIR: inf, SAR: -28.5055
2025-01-21 01:37:00,615 - INFO - Fine-Tuning --> Epoch [1/6] Average Validation Loss: 418.544826
2025-01-21 01:37:00,615 - INFO - Starting Epoch 2/6
2025-01-21 01:38:20,883 - INFO - Epoch [2], Batch [1] -> Combined Loss: 549.368164, Mask Loss: 1.342518, Total Loss: 1097.393799
2025-01-21 01:38:45,701 - INFO - Epoch [2], Batch [2] -> Combined Loss: 561.698486, Mask Loss: 0.809430, Total Loss: 1122.587524
2025-01-21 01:39:09,193 - INFO - Epoch [2], Batch [3] -> Combined Loss: 537.731018, Mask Loss: 0.712167, Total Loss: 1074.749878
2025-01-21 01:39:31,791 - INFO - Epoch [2], Batch [4] -> Combined Loss: 507.526550, Mask Loss: 0.424014, Total Loss: 1014.629089
2025-01-21 01:39:53,812 - INFO - Epoch [2], Batch [5] -> Combined Loss: 529.023193, Mask Loss: 0.652311, Total Loss: 1057.394043
2025-01-21 01:40:16,646 - INFO - Epoch [2], Batch [6] -> Combined Loss: 566.586670, Mask Loss: 1.115233, Total Loss: 1132.058105
2025-01-21 01:40:38,295 - INFO - Epoch [2], Batch [7] -> Combined Loss: 534.275757, Mask Loss: 0.214704, Total Loss: 1068.336792
2025-01-21 01:41:00,586 - INFO - Epoch [2], Batch [8] -> Combined Loss: 616.671204, Mask Loss: 0.289313, Total Loss: 1233.053101
2025-01-21 01:41:22,094 - INFO - Epoch [2], Batch [9] -> Combined Loss: 590.163513, Mask Loss: 1.313873, Total Loss: 1179.013184
2025-01-21 01:41:45,194 - INFO - Epoch [2], Batch [10] -> Combined Loss: 618.820984, Mask Loss: 1.071632, Total Loss: 1236.570312
2025-01-21 01:42:06,696 - INFO - Epoch [2], Batch [11] -> Combined Loss: 494.612061, Mask Loss: 0.762233, Total Loss: 988.461914
2025-01-21 01:42:29,128 - INFO - Epoch [2], Batch [12] -> Combined Loss: 589.183228, Mask Loss: 1.175455, Total Loss: 1177.191040
2025-01-21 01:42:50,778 - INFO - Epoch [2], Batch [13] -> Combined Loss: 599.727173, Mask Loss: 0.789946, Total Loss: 1198.664429
2025-01-21 01:43:13,534 - INFO - Epoch [2], Batch [14] -> Combined Loss: 427.160736, Mask Loss: 0.071349, Total Loss: 854.250122
2025-01-21 01:43:33,885 - INFO - Epoch [2], Batch [15] -> Combined Loss: 573.754761, Mask Loss: 0.387296, Total Loss: 1147.122192
2025-01-21 01:43:54,664 - INFO - Epoch [2], Batch [16] -> Combined Loss: 563.687622, Mask Loss: 0.616816, Total Loss: 1126.758423
2025-01-21 01:44:18,778 - INFO - Epoch [2], Batch [17] -> Combined Loss: 400.979401, Mask Loss: 0.100655, Total Loss: 801.858154
2025-01-21 01:44:40,693 - INFO - Epoch [2], Batch [18] -> Combined Loss: 470.041809, Mask Loss: 0.115350, Total Loss: 939.968262
2025-01-21 01:45:00,917 - INFO - Epoch [2], Batch [19] -> Combined Loss: 638.840637, Mask Loss: 0.274640, Total Loss: 1277.406616
2025-01-21 01:45:21,660 - INFO - Epoch [2], Batch [20] -> Combined Loss: 655.989502, Mask Loss: 1.043451, Total Loss: 1310.935547
2025-01-21 01:45:45,595 - INFO - Epoch [2], Batch [21] -> Combined Loss: 623.051025, Mask Loss: 1.293650, Total Loss: 1244.808350
2025-01-21 01:46:07,478 - INFO - Epoch [2], Batch [22] -> Combined Loss: 562.169861, Mask Loss: 1.228734, Total Loss: 1123.110962
2025-01-21 01:46:28,178 - INFO - Epoch [2], Batch [23] -> Combined Loss: 547.244995, Mask Loss: 0.913866, Total Loss: 1093.576172
2025-01-21 01:46:49,656 - INFO - Epoch [2], Batch [24] -> Combined Loss: 717.436768, Mask Loss: 0.479879, Total Loss: 1434.393677
2025-01-21 01:47:09,955 - INFO - Epoch [2], Batch [25] -> Combined Loss: 667.254089, Mask Loss: 1.331633, Total Loss: 1333.176514
2025-01-21 01:47:30,773 - INFO - Epoch [2], Batch [26] -> Combined Loss: 565.467468, Mask Loss: 0.864697, Total Loss: 1130.070190
2025-01-21 01:47:54,738 - INFO - Epoch [2], Batch [27] -> Combined Loss: 435.745667, Mask Loss: 0.201944, Total Loss: 871.289368
2025-01-21 01:48:16,642 - INFO - Epoch [2], Batch [28] -> Combined Loss: 446.030457, Mask Loss: 0.451961, Total Loss: 891.608948
2025-01-21 01:48:37,273 - INFO - Epoch [2], Batch [29] -> Combined Loss: 495.132050, Mask Loss: 0.677194, Total Loss: 989.586914
2025-01-21 01:48:58,781 - INFO - Epoch [2], Batch [30] -> Combined Loss: 555.330688, Mask Loss: 1.032778, Total Loss: 1109.628540
2025-01-21 01:48:59,762 - INFO - Fine-Tuning --> Epoch [2/6] Average Training Loss: 554.690185
2025-01-21 01:50:22,296 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 01:50:25,391 - INFO - Saved waveform visualization at C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Model_Weights\Fine_tuned\visualizations\epoch_2_sample_2.png
2025-01-21 01:50:28,606 - INFO - Saved waveform visualization at C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Model_Weights\Fine_tuned\visualizations\epoch_2_sample_3.png
2025-01-21 01:50:31,771 - INFO - Saved waveform visualization at C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Model_Weights\Fine_tuned\visualizations\epoch_2_sample_4.png
2025-01-21 01:51:05,494 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 01:51:05,494 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 01:51:05,494 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 01:51:39,966 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 01:51:39,966 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 01:51:39,966 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 01:51:39,966 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 01:51:39,982 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 01:52:13,636 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 01:52:13,636 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 01:52:13,868 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 01:52:47,817 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 01:52:48,030 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 01:52:48,030 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 01:52:48,030 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 01:53:56,968 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 01:53:57,176 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 01:53:57,176 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 01:55:06,793 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 01:55:06,793 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 01:55:06,793 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 01:55:40,764 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 01:55:40,764 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 01:55:40,764 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 01:55:40,764 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 01:56:18,731 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 01:56:53,073 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 01:56:53,073 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 01:56:53,286 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 01:57:29,084 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 01:57:29,084 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 01:57:29,084 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 01:57:29,293 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 01:58:03,440 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 01:58:03,440 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 01:58:03,440 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 01:58:03,785 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 01:58:39,494 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 01:58:39,494 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 01:58:39,494 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 01:58:39,494 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 01:59:13,767 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 01:59:13,767 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 01:59:13,767 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 01:59:14,020 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 01:59:49,745 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 01:59:49,760 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 01:59:49,983 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:00:24,946 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:00:24,946 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:01:00,689 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 02:01:01,098 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:01:35,860 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:01:35,860 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:01:35,860 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:01:35,860 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:01:36,807 - INFO - Validation Metrics - SDR: -28.6340, SIR: inf, SAR: -28.6340
2025-01-21 02:01:36,807 - INFO - Fine-Tuning --> Epoch [2/6] Average Validation Loss: 423.597919
2025-01-21 02:01:36,807 - INFO - Starting Epoch 3/6
2025-01-21 02:03:07,245 - INFO - Epoch [3], Batch [1] -> Combined Loss: 365.568176, Mask Loss: 0.080577, Total Loss: 731.055786
2025-01-21 02:03:30,968 - INFO - Epoch [3], Batch [2] -> Combined Loss: 532.369751, Mask Loss: 0.809617, Total Loss: 1063.929932
2025-01-21 02:03:54,560 - INFO - Epoch [3], Batch [3] -> Combined Loss: 498.645264, Mask Loss: 0.582466, Total Loss: 996.708069
2025-01-21 02:04:15,779 - INFO - Epoch [3], Batch [4] -> Combined Loss: 637.966492, Mask Loss: 0.904144, Total Loss: 1275.028809
2025-01-21 02:04:38,765 - INFO - Epoch [3], Batch [5] -> Combined Loss: 655.189392, Mask Loss: 2.219375, Total Loss: 1308.159424
2025-01-21 02:05:02,254 - INFO - Epoch [3], Batch [6] -> Combined Loss: 597.902222, Mask Loss: 0.787027, Total Loss: 1195.017456
2025-01-21 02:05:26,561 - INFO - Epoch [3], Batch [7] -> Combined Loss: 502.602631, Mask Loss: 0.232411, Total Loss: 1004.972839
2025-01-21 02:05:49,387 - INFO - Epoch [3], Batch [8] -> Combined Loss: 479.788544, Mask Loss: 0.816623, Total Loss: 958.760437
2025-01-21 02:06:11,080 - INFO - Epoch [3], Batch [9] -> Combined Loss: 614.545898, Mask Loss: 0.302018, Total Loss: 1228.789795
2025-01-21 02:06:33,650 - INFO - Epoch [3], Batch [10] -> Combined Loss: 668.297424, Mask Loss: 0.890779, Total Loss: 1335.704102
2025-01-21 02:06:55,586 - INFO - Epoch [3], Batch [11] -> Combined Loss: 619.162415, Mask Loss: 1.163919, Total Loss: 1237.160889
2025-01-21 02:07:17,931 - INFO - Epoch [3], Batch [12] -> Combined Loss: 570.017883, Mask Loss: 0.816959, Total Loss: 1139.218750
2025-01-21 02:07:39,750 - INFO - Epoch [3], Batch [13] -> Combined Loss: 426.790222, Mask Loss: 0.084136, Total Loss: 853.496338
2025-01-21 02:08:02,282 - INFO - Epoch [3], Batch [14] -> Combined Loss: 545.497864, Mask Loss: 0.182892, Total Loss: 1090.812866
2025-01-21 02:08:22,970 - INFO - Epoch [3], Batch [15] -> Combined Loss: 649.866455, Mask Loss: 1.383556, Total Loss: 1298.349365
2025-01-21 02:08:44,292 - INFO - Epoch [3], Batch [16] -> Combined Loss: 472.440613, Mask Loss: 0.266510, Total Loss: 944.614746
2025-01-21 02:09:04,956 - INFO - Epoch [3], Batch [17] -> Combined Loss: 607.380249, Mask Loss: 0.738430, Total Loss: 1214.022095
2025-01-21 02:09:26,313 - INFO - Epoch [3], Batch [18] -> Combined Loss: 469.503448, Mask Loss: 0.102063, Total Loss: 938.904846
2025-01-21 02:09:46,607 - INFO - Epoch [3], Batch [19] -> Combined Loss: 547.584229, Mask Loss: 0.310132, Total Loss: 1094.858276
2025-01-21 02:10:07,308 - INFO - Epoch [3], Batch [20] -> Combined Loss: 467.098145, Mask Loss: 0.345250, Total Loss: 933.851013
2025-01-21 02:10:31,207 - INFO - Epoch [3], Batch [21] -> Combined Loss: 495.940857, Mask Loss: 0.552430, Total Loss: 991.329285
2025-01-21 02:10:53,090 - INFO - Epoch [3], Batch [22] -> Combined Loss: 525.205872, Mask Loss: 0.408799, Total Loss: 1050.002930
2025-01-21 02:11:13,738 - INFO - Epoch [3], Batch [23] -> Combined Loss: 483.092560, Mask Loss: 0.094957, Total Loss: 966.090149
2025-01-21 02:11:35,132 - INFO - Epoch [3], Batch [24] -> Combined Loss: 597.366150, Mask Loss: 0.377580, Total Loss: 1194.354736
2025-01-21 02:11:55,707 - INFO - Epoch [3], Batch [25] -> Combined Loss: 520.067261, Mask Loss: 0.692438, Total Loss: 1039.442139
2025-01-21 02:12:17,056 - INFO - Epoch [3], Batch [26] -> Combined Loss: 577.450439, Mask Loss: 1.411182, Total Loss: 1153.489746
2025-01-21 02:12:39,166 - INFO - Epoch [3], Batch [27] -> Combined Loss: 485.357056, Mask Loss: 0.192820, Total Loss: 970.521301
2025-01-21 02:13:01,730 - INFO - Epoch [3], Batch [28] -> Combined Loss: 716.545288, Mask Loss: 1.954823, Total Loss: 1431.135742
2025-01-21 02:13:23,807 - INFO - Epoch [3], Batch [29] -> Combined Loss: 657.579407, Mask Loss: 1.169595, Total Loss: 1313.989258
2025-01-21 02:13:45,985 - INFO - Epoch [3], Batch [30] -> Combined Loss: 622.252319, Mask Loss: 1.850117, Total Loss: 1242.654541
2025-01-21 02:13:46,965 - INFO - Fine-Tuning --> Epoch [3/6] Average Training Loss: 553.635817
2025-01-21 02:15:09,914 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:15:13,021 - INFO - Saved waveform visualization at C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Model_Weights\Fine_tuned\visualizations\epoch_3_sample_2.png
2025-01-21 02:15:16,118 - INFO - Saved waveform visualization at C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Model_Weights\Fine_tuned\visualizations\epoch_3_sample_3.png
2025-01-21 02:15:19,247 - INFO - Saved waveform visualization at C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Model_Weights\Fine_tuned\visualizations\epoch_3_sample_4.png
2025-01-21 02:15:52,967 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:15:52,967 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:15:52,967 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:16:27,290 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 02:16:27,290 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:16:27,290 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:16:27,304 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:16:27,306 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:17:00,851 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 02:17:00,851 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:17:01,064 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:17:34,533 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 02:17:34,765 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:17:34,765 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:17:34,765 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:18:43,901 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 02:18:44,120 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:18:44,120 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:19:53,487 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:19:53,487 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:19:53,487 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:20:27,052 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:20:27,052 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:20:27,052 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:20:27,052 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:21:04,635 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:21:38,651 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 02:21:38,651 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:21:38,905 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:22:14,667 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 02:22:14,667 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:22:14,681 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:22:14,912 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:22:49,113 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 02:22:49,113 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:22:49,113 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:22:49,339 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:23:24,854 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:23:24,854 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:23:24,854 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:23:24,854 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:23:58,890 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 02:23:58,906 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:23:58,906 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:23:59,127 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:24:34,684 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 02:24:34,684 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:24:34,900 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:25:09,832 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:25:09,832 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:25:46,104 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 02:25:46,550 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:26:20,865 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:26:20,865 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:26:20,881 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:26:20,883 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:26:21,807 - INFO - Validation Metrics - SDR: -28.7768, SIR: inf, SAR: -28.7768
2025-01-21 02:26:21,807 - INFO - Fine-Tuning --> Epoch [3/6] Average Validation Loss: 425.344666
2025-01-21 02:26:21,807 - INFO - Starting Epoch 4/6
2025-01-21 02:27:50,707 - INFO - Epoch [4], Batch [1] -> Combined Loss: 679.048462, Mask Loss: 1.340677, Total Loss: 1356.756226
2025-01-21 02:28:14,206 - INFO - Epoch [4], Batch [2] -> Combined Loss: 486.022125, Mask Loss: 1.350450, Total Loss: 970.693787
2025-01-21 02:28:38,745 - INFO - Epoch [4], Batch [3] -> Combined Loss: 426.590271, Mask Loss: 0.302313, Total Loss: 852.878235
2025-01-21 02:29:00,527 - INFO - Epoch [4], Batch [4] -> Combined Loss: 468.006927, Mask Loss: 0.182894, Total Loss: 935.830933
2025-01-21 02:29:23,361 - INFO - Epoch [4], Batch [5] -> Combined Loss: 539.802307, Mask Loss: 0.733209, Total Loss: 1078.871460
2025-01-21 02:29:46,670 - INFO - Epoch [4], Batch [6] -> Combined Loss: 582.562134, Mask Loss: 0.919063, Total Loss: 1164.205200
2025-01-21 02:30:09,587 - INFO - Epoch [4], Batch [7] -> Combined Loss: 534.654968, Mask Loss: 0.266988, Total Loss: 1069.042969
2025-01-21 02:30:31,346 - INFO - Epoch [4], Batch [8] -> Combined Loss: 513.672302, Mask Loss: 0.241401, Total Loss: 1027.103149
2025-01-21 02:30:54,292 - INFO - Epoch [4], Batch [9] -> Combined Loss: 596.367126, Mask Loss: 1.200667, Total Loss: 1191.533569
2025-01-21 02:31:17,048 - INFO - Epoch [4], Batch [10] -> Combined Loss: 574.449463, Mask Loss: 0.947718, Total Loss: 1147.951172
2025-01-21 02:31:40,230 - INFO - Epoch [4], Batch [11] -> Combined Loss: 679.414917, Mask Loss: 1.414573, Total Loss: 1357.415283
2025-01-21 02:32:03,380 - INFO - Epoch [4], Batch [12] -> Combined Loss: 594.392395, Mask Loss: 0.725827, Total Loss: 1188.058960
2025-01-21 02:32:25,907 - INFO - Epoch [4], Batch [13] -> Combined Loss: 582.163940, Mask Loss: 1.311854, Total Loss: 1163.015991
2025-01-21 02:32:48,808 - INFO - Epoch [4], Batch [14] -> Combined Loss: 574.677429, Mask Loss: 0.830282, Total Loss: 1148.524536
2025-01-21 02:33:08,997 - INFO - Epoch [4], Batch [15] -> Combined Loss: 540.095642, Mask Loss: 0.135187, Total Loss: 1080.056152
2025-01-21 02:33:29,619 - INFO - Epoch [4], Batch [16] -> Combined Loss: 580.930969, Mask Loss: 1.129257, Total Loss: 1160.732666
2025-01-21 02:33:49,609 - INFO - Epoch [4], Batch [17] -> Combined Loss: 688.273376, Mask Loss: 0.816365, Total Loss: 1375.730347
2025-01-21 02:34:10,171 - INFO - Epoch [4], Batch [18] -> Combined Loss: 527.669312, Mask Loss: 0.179775, Total Loss: 1055.158813
2025-01-21 02:34:32,660 - INFO - Epoch [4], Batch [19] -> Combined Loss: 539.224304, Mask Loss: 1.087339, Total Loss: 1077.361328
2025-01-21 02:34:54,054 - INFO - Epoch [4], Batch [20] -> Combined Loss: 526.667786, Mask Loss: 0.249728, Total Loss: 1053.085815
2025-01-21 02:35:16,072 - INFO - Epoch [4], Batch [21] -> Combined Loss: 472.763733, Mask Loss: 0.158185, Total Loss: 945.369263
2025-01-21 02:35:37,817 - INFO - Epoch [4], Batch [22] -> Combined Loss: 533.688110, Mask Loss: 0.174849, Total Loss: 1067.201416
2025-01-21 02:35:58,350 - INFO - Epoch [4], Batch [23] -> Combined Loss: 577.349670, Mask Loss: 0.737344, Total Loss: 1153.962036
2025-01-21 02:36:19,757 - INFO - Epoch [4], Batch [24] -> Combined Loss: 489.109161, Mask Loss: 0.588978, Total Loss: 977.629333
2025-01-21 02:36:40,221 - INFO - Epoch [4], Batch [25] -> Combined Loss: 556.512451, Mask Loss: 0.797971, Total Loss: 1112.226929
2025-01-21 02:37:01,623 - INFO - Epoch [4], Batch [26] -> Combined Loss: 663.594482, Mask Loss: 1.325329, Total Loss: 1325.863647
2025-01-21 02:37:23,725 - INFO - Epoch [4], Batch [27] -> Combined Loss: 510.319031, Mask Loss: 0.595835, Total Loss: 1020.042236
2025-01-21 02:37:46,139 - INFO - Epoch [4], Batch [28] -> Combined Loss: 523.751160, Mask Loss: 1.055292, Total Loss: 1046.447021
2025-01-21 02:38:08,106 - INFO - Epoch [4], Batch [29] -> Combined Loss: 524.819092, Mask Loss: 0.751441, Total Loss: 1048.886719
2025-01-21 02:38:29,182 - INFO - Epoch [4], Batch [30] -> Combined Loss: 534.775574, Mask Loss: 0.269305, Total Loss: 1069.281860
2025-01-21 02:38:30,173 - INFO - Fine-Tuning --> Epoch [4/6] Average Training Loss: 554.045621
2025-01-21 02:39:54,632 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:39:57,748 - INFO - Saved waveform visualization at C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Model_Weights\Fine_tuned\visualizations\epoch_4_sample_2.png
2025-01-21 02:40:00,846 - INFO - Saved waveform visualization at C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Model_Weights\Fine_tuned\visualizations\epoch_4_sample_3.png
2025-01-21 02:40:03,922 - INFO - Saved waveform visualization at C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Model_Weights\Fine_tuned\visualizations\epoch_4_sample_4.png
2025-01-21 02:40:37,585 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:40:37,585 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:40:37,585 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:41:11,318 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 02:41:11,318 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:41:11,318 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:41:11,318 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:41:11,325 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:41:45,599 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 02:41:45,599 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:41:45,920 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:42:19,672 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 02:42:19,963 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:42:19,963 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:42:19,963 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:43:28,752 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 02:43:29,078 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:43:29,078 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:44:38,070 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:44:38,070 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:44:38,086 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:45:12,704 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:45:12,704 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:45:12,704 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:45:12,704 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:45:50,133 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:46:25,061 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 02:46:25,076 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:46:25,280 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:46:59,842 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 02:46:59,842 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:46:59,842 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:47:00,065 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:47:35,629 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 02:47:35,629 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:47:35,629 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:47:35,854 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:48:10,178 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:48:10,178 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:48:10,178 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:48:10,185 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:48:45,765 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 02:48:45,765 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:48:45,765 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:48:46,072 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:49:20,146 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 02:49:20,146 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:49:20,361 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:49:56,758 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:49:56,774 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:50:31,273 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 02:50:31,691 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:51:07,871 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 02:51:07,871 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 02:51:07,871 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 02:51:07,883 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 02:51:08,806 - INFO - Validation Metrics - SDR: -28.9367, SIR: inf, SAR: -28.9367
2025-01-21 02:51:08,806 - INFO - Fine-Tuning --> Epoch [4/6] Average Validation Loss: 421.550522
2025-01-21 02:51:08,806 - INFO - Starting Epoch 5/6
2025-01-21 02:52:31,786 - INFO - Epoch [5], Batch [1] -> Combined Loss: 573.823608, Mask Loss: 1.097930, Total Loss: 1146.549316
2025-01-21 02:52:57,256 - INFO - Epoch [5], Batch [2] -> Combined Loss: 683.213623, Mask Loss: 1.146777, Total Loss: 1365.280518
2025-01-21 02:53:20,419 - INFO - Epoch [5], Batch [3] -> Combined Loss: 438.882996, Mask Loss: 0.120419, Total Loss: 877.645569
2025-01-21 02:53:41,729 - INFO - Epoch [5], Batch [4] -> Combined Loss: 566.433350, Mask Loss: 0.720949, Total Loss: 1132.145752
2025-01-21 02:54:04,043 - INFO - Epoch [5], Batch [5] -> Combined Loss: 597.389832, Mask Loss: 1.145914, Total Loss: 1193.633789
2025-01-21 02:54:26,082 - INFO - Epoch [5], Batch [6] -> Combined Loss: 667.496582, Mask Loss: 0.904800, Total Loss: 1334.088379
2025-01-21 02:54:48,396 - INFO - Epoch [5], Batch [7] -> Combined Loss: 484.453705, Mask Loss: 0.214831, Total Loss: 968.692566
2025-01-21 02:55:09,857 - INFO - Epoch [5], Batch [8] -> Combined Loss: 554.457886, Mask Loss: 0.260643, Total Loss: 1108.655151
2025-01-21 02:55:31,604 - INFO - Epoch [5], Batch [9] -> Combined Loss: 530.853882, Mask Loss: 1.161840, Total Loss: 1060.545898
2025-01-21 02:55:57,001 - INFO - Epoch [5], Batch [10] -> Combined Loss: 501.695343, Mask Loss: 0.459603, Total Loss: 1002.931091
2025-01-21 02:56:19,291 - INFO - Epoch [5], Batch [11] -> Combined Loss: 533.260742, Mask Loss: 0.742219, Total Loss: 1065.779297
2025-01-21 02:56:42,660 - INFO - Epoch [5], Batch [12] -> Combined Loss: 534.800720, Mask Loss: 0.322473, Total Loss: 1069.278931
2025-01-21 02:57:06,336 - INFO - Epoch [5], Batch [13] -> Combined Loss: 679.226318, Mask Loss: 1.383119, Total Loss: 1357.069458
2025-01-21 02:57:29,463 - INFO - Epoch [5], Batch [14] -> Combined Loss: 399.758514, Mask Loss: 0.429449, Total Loss: 799.087585
2025-01-21 02:57:51,415 - INFO - Epoch [5], Batch [15] -> Combined Loss: 601.576965, Mask Loss: 1.461718, Total Loss: 1201.692261
2025-01-21 02:58:12,096 - INFO - Epoch [5], Batch [16] -> Combined Loss: 479.650360, Mask Loss: 0.195651, Total Loss: 959.105042
2025-01-21 02:58:33,577 - INFO - Epoch [5], Batch [17] -> Combined Loss: 571.016968, Mask Loss: 0.369392, Total Loss: 1141.664551
2025-01-21 02:58:55,769 - INFO - Epoch [5], Batch [18] -> Combined Loss: 713.325562, Mask Loss: 1.389274, Total Loss: 1425.261841
2025-01-21 02:59:17,731 - INFO - Epoch [5], Batch [19] -> Combined Loss: 536.708252, Mask Loss: 0.726761, Total Loss: 1072.689697
2025-01-21 02:59:38,332 - INFO - Epoch [5], Batch [20] -> Combined Loss: 589.621460, Mask Loss: 0.625045, Total Loss: 1178.617920
2025-01-21 02:59:59,745 - INFO - Epoch [5], Batch [21] -> Combined Loss: 506.697723, Mask Loss: 0.219711, Total Loss: 1013.175720
2025-01-21 03:00:20,453 - INFO - Epoch [5], Batch [22] -> Combined Loss: 630.291382, Mask Loss: 1.481544, Total Loss: 1259.101196
2025-01-21 03:00:41,896 - INFO - Epoch [5], Batch [23] -> Combined Loss: 577.675232, Mask Loss: 0.751377, Total Loss: 1154.599121
2025-01-21 03:01:02,668 - INFO - Epoch [5], Batch [24] -> Combined Loss: 505.623932, Mask Loss: 0.618113, Total Loss: 1010.629761
2025-01-21 03:01:24,130 - INFO - Epoch [5], Batch [25] -> Combined Loss: 449.662109, Mask Loss: 0.392570, Total Loss: 898.931641
2025-01-21 03:01:44,788 - INFO - Epoch [5], Batch [26] -> Combined Loss: 752.164062, Mask Loss: 1.929350, Total Loss: 1502.398804
2025-01-21 03:02:06,294 - INFO - Epoch [5], Batch [27] -> Combined Loss: 461.281525, Mask Loss: 0.120600, Total Loss: 922.442444
2025-01-21 03:02:28,537 - INFO - Epoch [5], Batch [28] -> Combined Loss: 493.882965, Mask Loss: 1.233606, Total Loss: 986.532349
2025-01-21 03:02:51,100 - INFO - Epoch [5], Batch [29] -> Combined Loss: 536.177856, Mask Loss: 0.197036, Total Loss: 1072.158691
2025-01-21 03:03:13,195 - INFO - Epoch [5], Batch [30] -> Combined Loss: 467.507782, Mask Loss: 0.091358, Total Loss: 934.924194
2025-01-21 03:03:14,213 - INFO - Fine-Tuning --> Epoch [5/6] Average Training Loss: 553.953708
2025-01-21 03:04:37,868 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 03:04:40,957 - INFO - Saved waveform visualization at C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Model_Weights\Fine_tuned\visualizations\epoch_5_sample_2.png
2025-01-21 03:04:44,081 - INFO - Saved waveform visualization at C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Model_Weights\Fine_tuned\visualizations\epoch_5_sample_3.png
2025-01-21 03:04:47,158 - INFO - Saved waveform visualization at C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Model_Weights\Fine_tuned\visualizations\epoch_5_sample_4.png
2025-01-21 03:05:25,258 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 03:05:25,258 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:05:25,258 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:06:00,369 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 03:06:00,369 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 03:06:00,369 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:06:00,386 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:06:00,388 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 03:06:38,491 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 03:06:38,491 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 03:06:38,706 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:07:13,998 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 03:07:14,304 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:07:14,304 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:07:14,304 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 03:08:28,380 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 03:08:28,593 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:08:28,593 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:09:42,987 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:09:42,987 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:09:42,987 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 03:10:21,003 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 03:10:21,003 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:10:21,003 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:10:21,003 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 03:10:58,512 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 03:11:35,831 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 03:11:35,831 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 03:11:36,028 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:12:12,593 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 03:12:12,593 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 03:12:12,593 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:12:12,801 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 03:12:50,031 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 03:12:50,031 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 03:12:50,048 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:12:50,313 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 03:13:26,764 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 03:13:26,780 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:13:26,780 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:13:26,780 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 03:14:04,164 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 03:14:04,164 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 03:14:04,164 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:14:04,372 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 03:14:40,749 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 03:14:40,749 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 03:14:40,967 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:15:18,910 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:15:18,910 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:15:55,543 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 03:15:55,986 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:16:34,386 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 03:16:34,386 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:16:34,402 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:16:34,402 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 03:16:35,314 - INFO - Validation Metrics - SDR: -28.9087, SIR: inf, SAR: -28.9087
2025-01-21 03:16:35,314 - INFO - Fine-Tuning --> Epoch [5/6] Average Validation Loss: 420.992406
2025-01-21 03:16:35,314 - INFO - Starting Epoch 6/6
2025-01-21 03:17:59,548 - INFO - Epoch [6], Batch [1] -> Combined Loss: 453.143341, Mask Loss: 0.194862, Total Loss: 906.091797
2025-01-21 03:18:24,950 - INFO - Epoch [6], Batch [2] -> Combined Loss: 683.889832, Mask Loss: 1.874472, Total Loss: 1365.905151
2025-01-21 03:18:47,258 - INFO - Epoch [6], Batch [3] -> Combined Loss: 463.571930, Mask Loss: 1.042266, Total Loss: 926.101624
2025-01-21 03:19:10,799 - INFO - Epoch [6], Batch [4] -> Combined Loss: 627.772766, Mask Loss: 0.972980, Total Loss: 1254.572510
2025-01-21 03:19:33,236 - INFO - Epoch [6], Batch [5] -> Combined Loss: 441.979645, Mask Loss: 0.167919, Total Loss: 883.791382
2025-01-21 03:19:56,363 - INFO - Epoch [6], Batch [6] -> Combined Loss: 590.374573, Mask Loss: 1.228170, Total Loss: 1179.520996
2025-01-21 03:20:19,481 - INFO - Epoch [6], Batch [7] -> Combined Loss: 585.046753, Mask Loss: 0.659189, Total Loss: 1169.434326
2025-01-21 03:20:42,788 - INFO - Epoch [6], Batch [8] -> Combined Loss: 686.143494, Mask Loss: 1.210404, Total Loss: 1371.076538
2025-01-21 03:21:06,199 - INFO - Epoch [6], Batch [9] -> Combined Loss: 610.973328, Mask Loss: 0.800031, Total Loss: 1221.146606
2025-01-21 03:21:29,292 - INFO - Epoch [6], Batch [10] -> Combined Loss: 608.973022, Mask Loss: 0.831466, Total Loss: 1217.114624
2025-01-21 03:21:50,635 - INFO - Epoch [6], Batch [11] -> Combined Loss: 566.213989, Mask Loss: 0.696159, Total Loss: 1131.731812
2025-01-21 03:22:10,865 - INFO - Epoch [6], Batch [12] -> Combined Loss: 486.339813, Mask Loss: 0.136156, Total Loss: 972.543457
2025-01-21 03:22:31,922 - INFO - Epoch [6], Batch [13] -> Combined Loss: 476.786407, Mask Loss: 0.291618, Total Loss: 953.281189
2025-01-21 03:22:56,743 - INFO - Epoch [6], Batch [14] -> Combined Loss: 493.058502, Mask Loss: 0.342875, Total Loss: 985.774109
2025-01-21 03:23:18,065 - INFO - Epoch [6], Batch [15] -> Combined Loss: 451.620880, Mask Loss: 0.143807, Total Loss: 903.097961
2025-01-21 03:23:38,606 - INFO - Epoch [6], Batch [16] -> Combined Loss: 532.044556, Mask Loss: 0.750195, Total Loss: 1063.338867
2025-01-21 03:24:00,576 - INFO - Epoch [6], Batch [17] -> Combined Loss: 655.234192, Mask Loss: 0.517912, Total Loss: 1309.950439
2025-01-21 03:24:22,559 - INFO - Epoch [6], Batch [18] -> Combined Loss: 488.751404, Mask Loss: 0.245606, Total Loss: 977.257202
2025-01-21 03:24:44,452 - INFO - Epoch [6], Batch [19] -> Combined Loss: 573.437622, Mask Loss: 0.531280, Total Loss: 1146.343994
2025-01-21 03:25:06,497 - INFO - Epoch [6], Batch [20] -> Combined Loss: 646.025330, Mask Loss: 0.912263, Total Loss: 1291.138428
2025-01-21 03:25:28,496 - INFO - Epoch [6], Batch [21] -> Combined Loss: 591.155273, Mask Loss: 0.867581, Total Loss: 1181.442993
2025-01-21 03:25:50,524 - INFO - Epoch [6], Batch [22] -> Combined Loss: 590.448914, Mask Loss: 0.714083, Total Loss: 1180.183716
2025-01-21 03:26:12,514 - INFO - Epoch [6], Batch [23] -> Combined Loss: 436.927887, Mask Loss: 0.102601, Total Loss: 873.753174
2025-01-21 03:26:32,702 - INFO - Epoch [6], Batch [24] -> Combined Loss: 599.812439, Mask Loss: 1.465338, Total Loss: 1198.159546
2025-01-21 03:26:53,391 - INFO - Epoch [6], Batch [25] -> Combined Loss: 610.576477, Mask Loss: 1.472093, Total Loss: 1219.680908
2025-01-21 03:27:16,044 - INFO - Epoch [6], Batch [26] -> Combined Loss: 551.890808, Mask Loss: 0.228145, Total Loss: 1103.553467
2025-01-21 03:27:38,128 - INFO - Epoch [6], Batch [27] -> Combined Loss: 537.514893, Mask Loss: 0.121358, Total Loss: 1074.908447
2025-01-21 03:27:58,664 - INFO - Epoch [6], Batch [28] -> Combined Loss: 451.498505, Mask Loss: 0.497350, Total Loss: 902.499634
2025-01-21 03:28:20,096 - INFO - Epoch [6], Batch [29] -> Combined Loss: 595.793518, Mask Loss: 1.742562, Total Loss: 1189.844482
2025-01-21 03:28:42,173 - INFO - Epoch [6], Batch [30] -> Combined Loss: 633.212708, Mask Loss: 1.283034, Total Loss: 1265.142334
2025-01-21 03:28:43,171 - INFO - Fine-Tuning --> Epoch [6/6] Average Training Loss: 557.340427
2025-01-21 03:30:08,221 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 03:30:11,332 - INFO - Saved waveform visualization at C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Model_Weights\Fine_tuned\visualizations\epoch_6_sample_2.png
2025-01-21 03:30:14,452 - INFO - Saved waveform visualization at C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Model_Weights\Fine_tuned\visualizations\epoch_6_sample_3.png
2025-01-21 03:30:17,708 - INFO - Saved waveform visualization at C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Model_Weights\Fine_tuned\visualizations\epoch_6_sample_4.png
2025-01-21 03:30:56,741 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 03:30:56,741 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:30:56,741 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:31:32,623 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 03:31:32,623 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 03:31:32,623 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:31:32,631 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:31:32,634 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 03:32:11,449 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 03:32:11,449 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 03:32:11,658 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:32:47,475 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 03:32:47,695 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:32:47,695 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:32:47,695 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 03:34:03,288 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 03:34:03,527 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:34:03,527 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:35:19,909 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:35:19,909 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:35:19,909 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 03:35:58,861 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 03:35:58,861 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:35:58,861 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:35:58,873 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 03:36:36,183 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 03:37:13,656 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 03:37:13,672 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 03:37:13,896 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:37:50,578 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 03:37:50,578 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 03:37:50,578 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:37:50,797 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 03:38:28,222 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 03:38:28,222 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 03:38:28,222 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:38:28,515 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 03:39:05,182 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 03:39:05,182 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:39:05,182 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:39:05,182 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 03:39:42,585 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 03:39:42,585 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 03:39:42,585 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:39:42,813 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 03:40:19,409 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 03:40:19,409 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 03:40:19,675 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:40:57,746 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:40:57,762 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:41:34,370 - INFO - Skipping evaluation for a silent reference in sample 1.
2025-01-21 03:41:34,799 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:42:12,690 - INFO - Skipping evaluation for a silent reference in sample 2.
2025-01-21 03:42:12,690 - INFO - Skipping evaluation for a silent reference in sample 3.
2025-01-21 03:42:12,690 - INFO - Skipping evaluation for a silent reference in sample 4.
2025-01-21 03:42:12,690 - INFO - Skipping evaluation for a silent reference in sample 5.
2025-01-21 03:42:13,648 - INFO - Validation Metrics - SDR: -29.5282, SIR: inf, SAR: -29.5282
2025-01-21 03:42:13,648 - INFO - Fine-Tuning --> Epoch [6/6] Average Validation Loss: 416.549532
2025-01-21 03:42:13,766 - INFO - Loss curves plotted.
2025-01-21 03:42:13,899 - INFO - Fine-tuned model saved at C:\Users\didri\Desktop\UNet-Models\Unet_model_Audio_Seperation\Model_Weights\Fine_tuned\model.pth
