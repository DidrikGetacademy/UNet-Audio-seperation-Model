2025-01-25 20:52:30,207 - INFO - [Train] Using device: cuda
2025-01-25 20:52:30,208 - INFO - [Memory Clearing memory before training] Timestamp: 2025-01-25 20:52:30
2025-01-25 20:52:30,208 - INFO - [Memory Clearing memory before training] GPU 0: NVIDIA GeForce RTX 3060, Total=12.88 GB, Allocated=0.00 GB, Cached=0.00 GB
2025-01-25 20:52:30,209 - INFO - [Memory Clearing memory before training] CPU Memory Usage: ~0.70 GB
2025-01-25 20:52:30,306 - INFO - [Memory Memory after clearing it...] Timestamp: 2025-01-25 20:52:30
2025-01-25 20:52:30,307 - INFO - [Memory Memory after clearing it...] GPU 0: NVIDIA GeForce RTX 3060, Total=12.88 GB, Allocated=0.00 GB, Cached=0.00 GB
2025-01-25 20:52:30,308 - INFO - [Memory Memory after clearing it...] CPU Memory Usage: ~0.70 GB
2025-01-25 20:52:41,381 - INFO - [Train] Attempting to load DeepSpeed checkpoint from /mnt/c/Users/didri/Desktop/UNet-Models/Unet_model_Audio_Seperation/Model_Weights/CheckPoints/checkpoint_epochsss_1 with tag 'global_step18'...
2025-01-25 20:52:43,294 - INFO - [Train] Successfully loaded DeepSpeed checkpoint from /mnt/c/Users/didri/Desktop/UNet-Models/Unet_model_Audio_Seperation/Model_Weights/CheckPoints/checkpoint_epochsss_1 with tag 'global_step18'.
2025-01-25 20:52:43,294 - INFO - [Memory Clearing memory before training] Timestamp: 2025-01-25 20:52:43
2025-01-25 20:52:43,295 - INFO - [Memory Clearing memory before training] GPU 0: NVIDIA GeForce RTX 3060, Total=12.88 GB, Allocated=1.00 GB, Cached=1.12 GB
2025-01-25 20:52:43,295 - INFO - [Memory Clearing memory before training] CPU Memory Usage: ~1.77 GB
2025-01-25 20:52:43,390 - INFO - [Memory Memory after clearing it...] Timestamp: 2025-01-25 20:52:43
2025-01-25 20:52:43,391 - INFO - [Memory Memory after clearing it...] GPU 0: NVIDIA GeForce RTX 3060, Total=12.88 GB, Allocated=1.00 GB, Cached=1.00 GB
2025-01-25 20:52:43,392 - INFO - [Memory Memory after clearing it...] CPU Memory Usage: ~1.77 GB
2025-01-25 20:52:43,393 - INFO - Total number of parameters: 708501
2025-01-25 20:52:43,393 - INFO - Trainable parameters: 708501
2025-01-25 20:52:43,394 - INFO - Model architecture:
UNet(
  (encoder): ModuleList(
    (0): Sequential(
      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (2): ReLU(inplace=True)
      (3): SEBlock(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (fc): Sequential(
          (0): Linear(in_features=64, out_features=4, bias=False)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=4, out_features=64, bias=False)
          (3): Sigmoid()
        )
      )
      (4): SpectralAttentionBlock(
        (avg_pool): AdaptiveAvgPool2d(output_size=(1, None))
        (fc): Sequential(
          (0): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1))
          (3): Sigmoid()
        )
      )
      (5): Dropout(p=0.4, inplace=False)
      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (8): ReLU(inplace=True)
    )
    (1): Sequential(
      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (2): ReLU(inplace=True)
      (3): SEBlock(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (fc): Sequential(
          (0): Linear(in_features=128, out_features=8, bias=False)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=8, out_features=128, bias=False)
          (3): Sigmoid()
        )
      )
      (4): SpectralAttentionBlock(
        (avg_pool): AdaptiveAvgPool2d(output_size=(1, None))
        (fc): Sequential(
          (0): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1))
          (3): Sigmoid()
        )
      )
      (5): Dropout(p=0.4, inplace=False)
      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (8): ReLU(inplace=True)
    )
    (2): Sequential(
      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (2): ReLU(inplace=True)
      (3): SEBlock(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (fc): Sequential(
          (0): Linear(in_features=256, out_features=16, bias=False)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=16, out_features=256, bias=False)
          (3): Sigmoid()
        )
      )
      (4): SpectralAttentionBlock(
        (avg_pool): AdaptiveAvgPool2d(output_size=(1, None))
        (fc): Sequential(
          (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1))
          (3): Sigmoid()
        )
      )
      (5): Dropout(p=0.4, inplace=False)
      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (8): ReLU(inplace=True)
    )
    (3): Sequential(
      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (2): ReLU(inplace=True)
      (3): SEBlock(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (fc): Sequential(
          (0): Linear(in_features=512, out_features=32, bias=False)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=32, out_features=512, bias=False)
          (3): Sigmoid()
        )
      )
      (4): SpectralAttentionBlock(
        (avg_pool): AdaptiveAvgPool2d(output_size=(1, None))
        (fc): Sequential(
          (0): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
          (3): Sigmoid()
        )
      )
      (5): Dropout(p=0.4, inplace=False)
      (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (8): ReLU(inplace=True)
    )
  )
  (bottleneck): Sequential(
    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (2): ReLU(inplace=True)
    (3): SEBlock(
      (avg_pool): AdaptiveAvgPool2d(output_size=1)
      (fc): Sequential(
        (0): Linear(in_features=1024, out_features=64, bias=False)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=1024, bias=False)
        (3): Sigmoid()
      )
    )
    (4): SpectralAttentionBlock(
      (avg_pool): AdaptiveAvgPool2d(output_size=(1, None))
      (fc): Sequential(
        (0): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
        (2): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1))
        (3): Sigmoid()
      )
    )
    (5): Dropout(p=0.4, inplace=False)
    (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (8): ReLU(inplace=True)
  )
  (decoder): ModuleList(
    (0): MultiScaleDecoderBlock(
      (up): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))
      (conv): Sequential(
        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=512, out_features=32, bias=False)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=32, out_features=512, bias=False)
            (3): Sigmoid()
          )
        )
        (4): SpectralAttentionBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=(1, None))
          (fc): Sequential(
            (0): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
      )
    )
    (1): AttentionBlock(
      (W_g): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      (W_x): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      (psi): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
      (relu): ReLU(inplace=True)
      (sigmoid): Sigmoid()
    )
    (2): MultiScaleDecoderBlock(
      (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))
      (conv): Sequential(
        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=256, out_features=16, bias=False)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=16, out_features=256, bias=False)
            (3): Sigmoid()
          )
        )
        (4): SpectralAttentionBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=(1, None))
          (fc): Sequential(
            (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
      )
    )
    (3): AttentionBlock(
      (W_g): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
      (W_x): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
      (psi): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))
      (relu): ReLU(inplace=True)
      (sigmoid): Sigmoid()
    )
    (4): MultiScaleDecoderBlock(
      (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))
      (conv): Sequential(
        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=128, out_features=8, bias=False)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=8, out_features=128, bias=False)
            (3): Sigmoid()
          )
        )
        (4): SpectralAttentionBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=(1, None))
          (fc): Sequential(
            (0): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
      )
    )
    (5): AttentionBlock(
      (W_g): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
      (W_x): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
      (psi): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
      (relu): ReLU(inplace=True)
      (sigmoid): Sigmoid()
    )
    (6): MultiScaleDecoderBlock(
      (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))
      (conv): Sequential(
        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=64, out_features=4, bias=False)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=4, out_features=64, bias=False)
            (3): Sigmoid()
          )
        )
        (4): SpectralAttentionBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=(1, None))
          (fc): Sequential(
            (0): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
      )
    )
    (7): AttentionBlock(
      (W_g): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
      (W_x): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
      (psi): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))
      (relu): ReLU(inplace=True)
      (sigmoid): Sigmoid()
    )
  )
  (final_conv): Sequential(
    (0): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
    (1): Sigmoid()
  )
)
2025-01-25 20:52:43,400 - INFO - Found 100 files in '/mnt/c/Users/didri/Desktop/UNet-Models/Unet_model_Audio_Seperation/Datasets/Dataset_Audio_Folders/musdb18/train' (subset='train')
2025-01-25 20:52:43,400 - INFO - Dataset initialized with 100 valid files.
2025-01-25 20:52:43,405 - INFO - Dataset initialized with inputfiles:  11 valid files. & targetfiles : 11 valid files.
2025-01-25 20:52:43,409 - INFO - Dataset initialized with inputfiles:  11 valid files. & targetfiles : 11 valid files.
2025-01-25 20:52:43,412 - INFO - Found 50 files in '/mnt/c/Users/didri/Desktop/UNet-Models/Unet_model_Audio_Seperation/Datasets/Dataset_Audio_Folders/musdb18/test' (subset='test')
2025-01-25 20:52:43,412 - INFO - Dataset initialized with 50 valid files.
2025-01-25 20:52:43,589 - INFO - Training dataset size: 161
2025-01-25 20:52:43,590 - INFO - Validation dataset size: 111
2025-01-25 20:52:43,590 - INFO - Training dataset: 40 batches
2025-01-25 20:52:43,590 - INFO - Validation dataset: 27 batches
2025-01-25 20:55:13,555 - INFO - [Train] Epoch 1/40 started.
2025-01-25 20:56:28,069 - INFO - Sample 1: input min=0.0, max=0.0, target min=0.0, max=0.0
2025-01-25 20:57:46,129 - INFO - Appending Batch Losses: Mask: 0.428483247756958, Hybrid: 0.9656786918640137, Combined: 0.6970809698104858
2025-01-25 20:57:55,689 - INFO - [Batch] Epoch: 1/40, Batch: 13/40, Combined Loss: 0.697081, Mask Loss: 0.428483, Hybrid Loss: 0.965679
2025-01-25 20:57:56,572 - INFO - Appending Batch Losses: Mask: 0.3110480308532715, Hybrid: 0.7170407176017761, Combined: 0.5140444040298462
2025-01-25 20:57:58,380 - INFO - [Batch] Epoch: 1/40, Batch: 14/40, Combined Loss: 0.514044, Mask Loss: 0.311048, Hybrid Loss: 0.717041
2025-01-25 20:57:58,637 - INFO - Appending Batch Losses: Mask: 0.31079190969467163, Hybrid: 0.6286494731903076, Combined: 0.4697206914424896
2025-01-25 20:57:59,107 - INFO - [Batch] Epoch: 1/40, Batch: 15/40, Combined Loss: 0.469721, Mask Loss: 0.310792, Hybrid Loss: 0.628649
2025-01-25 20:57:59,376 - INFO - Appending Batch Losses: Mask: 0.2667638659477234, Hybrid: 0.8630185723304749, Combined: 0.5648912191390991
2025-01-25 20:57:59,903 - INFO - [Batch] Epoch: 1/40, Batch: 16/40, Combined Loss: 0.564891, Mask Loss: 0.266764, Hybrid Loss: 0.863019
2025-01-25 20:58:00,181 - INFO - Appending Batch Losses: Mask: 0.32935014367103577, Hybrid: 0.6711199283599854, Combined: 0.5002350211143494
2025-01-25 20:58:00,655 - INFO - [Batch] Epoch: 1/40, Batch: 17/40, Combined Loss: 0.500235, Mask Loss: 0.329350, Hybrid Loss: 0.671120
2025-01-25 20:58:00,949 - INFO - Appending Batch Losses: Mask: 0.3772728443145752, Hybrid: 0.798810601234436, Combined: 0.5880417227745056
2025-01-25 20:58:01,421 - INFO - [Batch] Epoch: 1/40, Batch: 19/40, Combined Loss: 0.588042, Mask Loss: 0.377273, Hybrid Loss: 0.798811
2025-01-25 20:58:03,764 - INFO - Appending Batch Losses: Mask: 0.2695983052253723, Hybrid: 0.42589420080184937, Combined: 0.34774625301361084
2025-01-25 20:58:08,044 - INFO - [Batch] Epoch: 1/40, Batch: 20/40, Combined Loss: 0.347746, Mask Loss: 0.269598, Hybrid Loss: 0.425894
2025-01-25 20:58:08,380 - INFO - Appending Batch Losses: Mask: 0.4025009870529175, Hybrid: 0.8798143863677979, Combined: 0.6411576867103577
2025-01-25 20:58:10,365 - INFO - [Batch] Epoch: 1/40, Batch: 21/40, Combined Loss: 0.641158, Mask Loss: 0.402501, Hybrid Loss: 0.879814
2025-01-25 20:58:10,651 - INFO - Appending Batch Losses: Mask: 0.7878037095069885, Hybrid: 1.8039371967315674, Combined: 1.2958704233169556
2025-01-25 20:58:11,148 - INFO - [Batch] Epoch: 1/40, Batch: 22/40, Combined Loss: 1.295870, Mask Loss: 0.787804, Hybrid Loss: 1.803937
2025-01-25 20:58:11,423 - INFO - Appending Batch Losses: Mask: 0.2802734375, Hybrid: 0.49143803119659424, Combined: 0.3858557343482971
2025-01-25 20:58:11,889 - INFO - [Batch] Epoch: 1/40, Batch: 23/40, Combined Loss: 0.385856, Mask Loss: 0.280273, Hybrid Loss: 0.491438
2025-01-25 20:58:12,177 - INFO - Appending Batch Losses: Mask: 0.2524551451206207, Hybrid: 0.598547637462616, Combined: 0.42550140619277954
2025-01-25 20:58:12,654 - INFO - [Batch] Epoch: 1/40, Batch: 24/40, Combined Loss: 0.425501, Mask Loss: 0.252455, Hybrid Loss: 0.598548
2025-01-25 20:58:13,392 - INFO - Appending Batch Losses: Mask: 0.3303831219673157, Hybrid: 0.586266279220581, Combined: 0.45832470059394836
2025-01-25 20:58:14,064 - INFO - [Batch] Epoch: 1/40, Batch: 25/40, Combined Loss: 0.458325, Mask Loss: 0.330383, Hybrid Loss: 0.586266
2025-01-25 20:58:24,885 - INFO - Appending Batch Losses: Mask: 0.7419685125350952, Hybrid: 1.8869974613189697, Combined: 1.3144829273223877
2025-01-25 20:58:25,361 - INFO - [Batch] Epoch: 1/40, Batch: 26/40, Combined Loss: 1.314483, Mask Loss: 0.741969, Hybrid Loss: 1.886997
2025-01-25 20:58:25,644 - INFO - Appending Batch Losses: Mask: 0.43842336535453796, Hybrid: 0.6822956800460815, Combined: 0.560359537601471
2025-01-25 20:58:26,128 - INFO - [Batch] Epoch: 1/40, Batch: 27/40, Combined Loss: 0.560360, Mask Loss: 0.438423, Hybrid Loss: 0.682296
2025-01-25 20:58:43,565 - INFO - Appending Batch Losses: Mask: 0.35972556471824646, Hybrid: 0.7554639577865601, Combined: 0.5575947761535645
2025-01-25 20:58:44,043 - INFO - [Batch] Epoch: 1/40, Batch: 28/40, Combined Loss: 0.557595, Mask Loss: 0.359726, Hybrid Loss: 0.755464
2025-01-25 20:58:44,316 - INFO - Appending Batch Losses: Mask: 0.33449307084083557, Hybrid: 0.5958207249641418, Combined: 0.4651569128036499
2025-01-25 20:58:45,028 - INFO - [Batch] Epoch: 1/40, Batch: 29/40, Combined Loss: 0.465157, Mask Loss: 0.334493, Hybrid Loss: 0.595821
2025-01-25 20:58:45,336 - INFO - Appending Batch Losses: Mask: 0.3834867775440216, Hybrid: 0.7188864946365356, Combined: 0.5511866211891174
2025-01-25 20:58:45,900 - INFO - [Batch] Epoch: 1/40, Batch: 30/40, Combined Loss: 0.551187, Mask Loss: 0.383487, Hybrid Loss: 0.718886
2025-01-25 20:58:46,249 - INFO - Appending Batch Losses: Mask: 0.32885128259658813, Hybrid: 0.6970046758651733, Combined: 0.5129280090332031
2025-01-25 20:58:46,719 - INFO - [Batch] Epoch: 1/40, Batch: 31/40, Combined Loss: 0.512928, Mask Loss: 0.328851, Hybrid Loss: 0.697005
2025-01-25 20:58:54,204 - INFO - Appending Batch Losses: Mask: 0.36805829405784607, Hybrid: 0.7274905443191528, Combined: 0.5477744340896606
2025-01-25 20:58:54,744 - INFO - [Batch] Epoch: 1/40, Batch: 32/40, Combined Loss: 0.547774, Mask Loss: 0.368058, Hybrid Loss: 0.727491
2025-01-25 20:58:55,166 - INFO - Appending Batch Losses: Mask: 0.3921547532081604, Hybrid: 0.7189763784408569, Combined: 0.555565595626831
2025-01-25 20:58:55,645 - INFO - [Batch] Epoch: 1/40, Batch: 33/40, Combined Loss: 0.555566, Mask Loss: 0.392155, Hybrid Loss: 0.718976
2025-01-25 20:59:06,047 - INFO - Appending Batch Losses: Mask: 0.36056312918663025, Hybrid: 0.8597565293312073, Combined: 0.6101598143577576
2025-01-25 20:59:06,592 - INFO - [Batch] Epoch: 1/40, Batch: 34/40, Combined Loss: 0.610160, Mask Loss: 0.360563, Hybrid Loss: 0.859757
2025-01-25 20:59:06,948 - INFO - Appending Batch Losses: Mask: 1.1289489269256592, Hybrid: 2.3819901943206787, Combined: 1.755469560623169
2025-01-25 20:59:07,490 - INFO - [Batch] Epoch: 1/40, Batch: 35/40, Combined Loss: 1.755470, Mask Loss: 1.128949, Hybrid Loss: 2.381990
2025-01-25 20:59:08,162 - INFO - Appending Batch Losses: Mask: 0.33249950408935547, Hybrid: 0.621863603591919, Combined: 0.4771815538406372
2025-01-25 20:59:08,704 - INFO - [Batch] Epoch: 1/40, Batch: 36/40, Combined Loss: 0.477182, Mask Loss: 0.332500, Hybrid Loss: 0.621864
2025-01-25 20:59:17,118 - INFO - Appending Batch Losses: Mask: 0.35767096281051636, Hybrid: 1.0215871334075928, Combined: 0.689629077911377
2025-01-25 20:59:17,846 - INFO - [Batch] Epoch: 1/40, Batch: 37/40, Combined Loss: 0.689629, Mask Loss: 0.357671, Hybrid Loss: 1.021587
2025-01-25 20:59:27,798 - INFO - Appending Batch Losses: Mask: 0.8812298774719238, Hybrid: 1.87237548828125, Combined: 1.376802682876587
2025-01-25 20:59:28,333 - INFO - [Batch] Epoch: 1/40, Batch: 38/40, Combined Loss: 1.376803, Mask Loss: 0.881230, Hybrid Loss: 1.872375
2025-01-25 20:59:28,864 - INFO - Appending Batch Losses: Mask: 0.25071531534194946, Hybrid: 0.42966437339782715, Combined: 0.3401898443698883
2025-01-25 20:59:29,565 - INFO - [Batch] Epoch: 1/40, Batch: 40/40, Combined Loss: 0.340190, Mask Loss: 0.250715, Hybrid Loss: 0.429664
2025-01-25 20:59:30,959 - INFO - Current Learning Rate 0.0007782475802159091
2025-01-25 20:59:30,960 - INFO - [Epoch 1] Average Training Loss: 0.430074
2025-01-25 20:59:30,960 - INFO - [Epoch Improvement] Epoch 1: Loss improved by 0.00% from previous epoch.
2025-01-25 20:59:30,960 - INFO - Avg_epoch_loss=0.43007378950715064, bestloss= 0.43007378950715064
2025-01-25 20:59:32,469 - INFO - [Epoch 1] Saving checkpoint to /mnt/c/Users/didri/Desktop/UNet-Models/Unet_model_Audio_Seperation/Model_Weights/CheckPoints/checkpoint_epochsss_1 with loss 0.430074
2025-01-25 20:59:32,470 - INFO - Starting Evaluation.
2025-01-25 21:01:54,390 - ERROR - [Validation] Error during validation: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)
2025-01-25 21:03:10,386 - ERROR - [Train] Error during training: only integer tensors of a single element can be converted to an index
