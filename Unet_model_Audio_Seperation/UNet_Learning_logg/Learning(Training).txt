###############################################################TRAIN################################################################


""""""""HYPER-PARAMETERS"""""""
learning_rate: Determines the step size at each iteration while moving toward the minimum of the loss function
*Smaller values => more precise but slower convergance
*Larger values => Faster training but risk of overshooting or not converging
*common default => 1e-3 or 1e-4




batch_size: Number of samples processed before the model updates weights.
*Smaller batch sizes => use less memory but may result in noisy gradient updates.
*Larger batch sizes => Faster convergence but require more memory.
*common default => 16, 32, 64





epochs: Number of complete passes through the dataset during training.
*higher epochs => often result in better learning but risk overfitting if too high 
*Depends on the dataset and model complexity







"""""""""DATASET PARAMETERS"""""
sr: (Sampling Rate): Defines the number of samples per second for the audio signal
*Standard value: 44100 Hz (CD-quality audio)
*Higher sr retains more audio details but increases memory usage.





n_fft: (Number of FFT components): Number of points for the Fast Fourier Transform (FFT)
*Larger values => result in higher frequency resolution but lower time resolution
*Typical values =>  512, 1024, 2048





hop_length: Number of samples between successive frames in the short-time Fourier Transform (STFT)
*Smaller values => better time resolution but larger output
*Typical => n_fft // 4





max_length: maximum length of the audio (in samples or spectogram frames) to process.
* helps standardize input sizes, avoiding memory issues 





max_files: Number of files to load for training/testing
*Useful for limiting dataset size during development or testing.





""""""DATALOADER PARAMETERS"""""""
dataset: specifies the dataset object (in this project MUSDB18StemDataset)
*It handles loading,preprocessing and augmenting data.





batch_size:same as in hyperparameters (Controls number of samples per batch)






Shuffle:Improves generalization by reducing overfitting
Shuffle=True =>  Randomize the order of the data during training.
Shuffle=False => keeps a consistent order (useful for debugging)





num_workers:Number of subproceses used for data loading
*Higher values speed up loading but setting it too high may overload your cpu
* On windows, setting this to 0 avoids multiprocessing issues.



pin_memory: if True, data is transferred to GPU memory faster
*recommended when using CUDA-enabled GPU's





""""""KEY CONSIDERATIONS""""""

#MEMORY USAGE
Adjust batch_size, n_fft and hop_length to fit GPU/CPU memory.
*Monitor GPU/CPU memory during training to avoid crashes.


#LEARNING EFFICIENCY
expiment with learning_rate and epochs for optimal performance
*use validation set to avoid overfitting


#DATASET STANDARDIZATION
*Ensure consistent sampling rates and spectrogram sizes to match model input.





""""""""""""RUNNING THE FILE"""""""""""""""
Device cuda
Loaded model from C:\Users\didri\Desktop\AI AudioEnchancer\UNet_Model\unet_vocal_isolation.pth
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Epoch [1/3], Loss: 3.4725
Epoch [1/3], Validation Loss: 3.3963
Checkpoint saved for epoch 1 at C:\Users\didri\Desktop\AI AudioEnchancer\UNet_Model\CheckPoints\unet_checkpoint_epoch1.pth
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Epoch [2/3], Loss: 2.6523
Epoch [2/3], Validation Loss: 2.1109
Checkpoint saved for epoch 2 at C:\Users\didri\Desktop\AI AudioEnchancer\UNet_Model\CheckPoints\unet_checkpoint_epoch2.pth
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Input shape: torch.Size([2, 1, 513, 10000]), Target shape: torch.Size([2, 1, 513, 10000])
Epoch [3/3], Loss: 2.2240
Epoch [3/3], Validation Loss: 1.8147
Checkpoint saved for epoch 3 at C:\Users\didri\Desktop\AI AudioEnchancer\UNet_Model\CheckPoints\unet_checkpoint_epoch3.pth
Training complete. Model saved.
Evaluating the model on the test set...
Test Loss: 1.8147
Testing complete.
PS C:\Users\didri\Desktop\AI AudioEnchancer> 


TrainingResult
epoch 1: Loss = 3.4725
epoch 2: Loss = 2.1100
epoch 3: Loss = 2.2240

ValidationLoss epoch 3: validation Loss = 1.8147

""""""Summary of training"""""
training loss decrease at every epoch and that tells us that the model is learning and adapting the trainingdata
ValidationLoss on dataset is low compared too the training loss and that is a good sign that the model is not overfitting
Evaluation: test loss 1.8147  this number represent how good the model is doing on the dataset that it has not seen earlier, lower loss indicates better model



"""""Analyse""""""
Loss decreses = model is learning well
not too low validation loss = if the validationloss was alot higher then the trainingloss would indicate a overfitting
the difference between validation loss & training loss = a little different is normal and healthy, if they are to alike that could tell us that the vaidation dataset is not so different from the train dataset






"""""Future adjustments""""""""
1. Higher epochs

2, various and big enough datasets

3. learning rate scheduling => reduce the learning rate as the model has trained alot

4.evaluate performance with metric, because loss alone is not always good enough too evaluate performance, ue metric for exsample (Signal-to-distortion-ratio) SDR for music isolation




####Different Learning rate sheduler and parameter optimizer######
#optimizer = optim.RMSprop( model.parameters(),  lr=learning_rate, alpha=0.99,  eps=1e-8,  weight_decay=1e-5)
#optimizer = optim.SGD(  model.parameters(), lr=learning_rate, momentum=0.9,  weight_decay=1e-5)
#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True) ## Needs a evaluation/validation function that calculates loss.
#scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)
#scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(musdb18_Train_Dataloader), epochs=epochs)

